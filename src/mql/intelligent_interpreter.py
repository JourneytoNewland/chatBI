"""æ™ºèƒ½è§£è¯»æ¨¡å— - åŸºäºæŸ¥è¯¢ç»“æœç”Ÿæˆåˆ†ææŠ¥å‘Š."""

import json
import logging
import statistics
from typing import Any, Dict, List, Optional

from .models import InterpretationResult, DataAnalysisResult

logger = logging.getLogger(__name__)


class IntelligentInterpreter:
    """æ™ºèƒ½è§£è¯»å™¨.

    åŠŸèƒ½:
    1. æ•°æ®åˆ†æï¼ˆè¶‹åŠ¿ã€æ³¢åŠ¨ã€å¼‚å¸¸æ£€æµ‹ï¼‰
    2. LLMè§£è¯»ç”Ÿæˆï¼ˆæ€»ç»“ã€å‘ç°ã€æ´å¯Ÿã€å»ºè®®ï¼‰
    3. é™çº§æœºåˆ¶ï¼ˆLLMå¤±è´¥æ—¶ä½¿ç”¨æ¨¡æ¿ï¼‰

    Attributes:
        llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
    """

    def __init__(self, llm_model: str = "glm-4-flash") -> None:
        """åˆå§‹åŒ–æ™ºèƒ½è§£è¯»å™¨.

        Args:
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨glm-4-flashï¼‰
        """
        self.llm_model = llm_model

    def interpret(
        self,
        query: str,
        mql_result: Dict[str, Any],
        metric_def: Dict[str, Any]
    ) -> InterpretationResult:
        """æ™ºèƒ½è§£è¯»æŸ¥è¯¢ç»“æœ.

        Args:
            query: ç”¨æˆ·åŸå§‹æŸ¥è¯¢
            mql_result: MQLæ‰§è¡Œç»“æœ
            metric_def: æŒ‡æ ‡å®šä¹‰

        Returns:
            InterpretationResult: æ™ºèƒ½è§£è¯»ç»“æœ
        """
        # 1. æ•°æ®åˆ†æ
        data_analysis = self._analyze_data(mql_result["result"])

        # 2. å°è¯•LLMè§£è¯»
        try:
            # æ„å»ºæç¤ºè¯ï¼ˆä¿å­˜ç”¨äºå±•ç¤ºï¼‰
            prompt = self._build_llm_prompt(query, data_analysis, metric_def, mql_result)

            # ä¿å­˜æç¤ºè¯åˆ°data_analysisä¸­ï¼Œä¾›å‰ç«¯å±•ç¤º
            data_analysis["_prompt"] = prompt

            interpretation = self._generate_llm_interpretation(
                query,
                data_analysis,
                metric_def,
                mql_result
            )

            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(data_analysis, interpretation)

            return InterpretationResult(
                summary=interpretation.get("summary", self._generate_default_summary(data_analysis, metric_def)),
                trend=data_analysis["trend"],
                key_findings=interpretation.get("key_findings", self._generate_default_findings(data_analysis)),
                insights=interpretation.get("insights", self._generate_default_insights(data_analysis, metric_def)),
                suggestions=interpretation.get("suggestions", self._generate_default_suggestions(data_analysis)),
                confidence=confidence,
                data_analysis=data_analysis
            )

        except Exception as e:
            logger.warning(f"LLMè§£è¯»å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ: {e}")

            # 3. é™çº§åˆ°æ¨¡æ¿ç”Ÿæˆ
            return self._generate_template_interpretation(
                query,
                data_analysis,
                metric_def,
                mql_result
            )

    def _analyze_data(self, data: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææ•°æ®ç‰¹å¾.

        Args:
            data: æŸ¥è¯¢ç»“æœæ•°æ®

        Returns:
            æ•°æ®åˆ†æç»“æœå­—å…¸
        """
        if len(data) < 2:
            return {
                "trend": "stable",
                "change_rate": 0,
                "volatility": 0,
                "anomalies": [],
                "min": data[0]["value"] if data else 0,
                "max": data[0]["value"] if data else 0,
                "avg": data[0]["value"] if data else 0,
                "std": 0
            }

        values = [row["value"] for row in data]

        # è®¡ç®—å˜åŒ–ç‡
        first_val = values[0]
        last_val = values[-1]
        change_rate = (last_val - first_val) / first_val * 100 if first_val != 0 else 0

        # åˆ¤æ–­è¶‹åŠ¿
        if change_rate > 10:
            trend = "upward"
        elif change_rate < -10:
            trend = "downward"
        elif abs(change_rate) < 5:
            trend = "stable"
        else:
            trend = "fluctuating"

        # è®¡ç®—ç»Ÿè®¡é‡
        mean_val = statistics.mean(values)
        std_val = statistics.stdev(values) if len(values) > 1 else 0
        volatility = (std_val / mean_val * 100) if mean_val != 0 else 0

        # è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆè¶…è¿‡2ä¸ªæ ‡å‡†å·®ï¼‰
        anomalies = []
        if len(values) > 3:
            for i, v in enumerate(values):
                if abs(v - mean_val) > 2 * std_val:
                    anomalies.append(i)

        return {
            "trend": trend,
            "change_rate": round(change_rate, 2),
            "volatility": round(volatility, 2),
            "anomalies": anomalies,
            "min": round(min(values), 2),
            "max": round(max(values), 2),
            "avg": round(mean_val, 2),
            "std": round(std_val, 2)
        }

    def _generate_llm_interpretation(
        self,
        query: str,
        data_analysis: Dict,
        metric_def: Dict,
        mql_result: Dict
    ) -> Dict[str, Any]:
        """åŸºäºLLMç”Ÿæˆæ™ºèƒ½è§£è¯».

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            data_analysis: æ•°æ®åˆ†æç»“æœ
            metric_def: æŒ‡æ ‡å®šä¹‰
            mql_result: MQLæ‰§è¡Œç»“æœ

        Returns:
            LLMç”Ÿæˆçš„è§£è¯»å­—å…¸

        Raises:
            RuntimeError: LLMè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
        """
        try:
            from ..inference.zhipu_intent import ZhipuIntentRecognizer

            # æ„å»ºæç¤ºè¯
            prompt = self._build_llm_prompt(query, data_analysis, metric_def, mql_result)

            # è°ƒç”¨ZhipuAI
            llm = ZhipuIntentRecognizer(model=self.llm_model)
            response = llm._call_api(prompt)

            # è§£æJSONå“åº”
            interpretation = json.loads(response)
            return interpretation

        except ImportError:
            raise RuntimeError("ZhipuAI SDKæœªå®‰è£…")
        except json.JSONDecodeError as e:
            raise RuntimeError(f"LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆJSON: {e}")
        except Exception as e:
            raise RuntimeError(f"LLMè°ƒç”¨å¤±è´¥: {e}")

    def _build_llm_prompt(
        self,
        query: str,
        data_analysis: Dict,
        metric_def: Dict,
        mql_result: Dict
    ) -> str:
        """æ„å»ºLLMæç¤ºè¯.

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            data_analysis: æ•°æ®åˆ†æç»“æœ
            metric_def: æŒ‡æ ‡å®šä¹‰
            mql_result: MQLæ‰§è¡Œç»“æœ

        Returns:
            LLMæç¤ºè¯å­—ç¬¦ä¸²
        """
        trend_label = {
            "upward": "ä¸Šå‡ â†—",
            "downward": "ä¸‹é™ â†˜",
            "fluctuating": "æ³¢åŠ¨ ã€°",
            "stable": "ç¨³å®š â†’"
        }.get(data_analysis["trend"], "æœªçŸ¥")

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æŸ¥è¯¢ç»“æœç”Ÿæˆæ™ºèƒ½è§£è¯»ï¼š

## ç”¨æˆ·æŸ¥è¯¢
{query}

## æŒ‡æ ‡ä¿¡æ¯
- æŒ‡æ ‡åç§°ï¼š{metric_def.get('name', 'æœªçŸ¥')}
- æŒ‡æ ‡å®šä¹‰ï¼š{metric_def.get('description', 'æ— æè¿°')}
- å•ä½ï¼š{metric_def.get('unit', '')}

## æ•°æ®åˆ†æç»“æœ
- è¶‹åŠ¿ï¼š{trend_label}
- å˜åŒ–ç‡ï¼š{data_analysis['change_rate']:.2f}%
- æ³¢åŠ¨æ€§ï¼š{data_analysis['volatility']:.2f}%
- æœ€å°å€¼ï¼š{data_analysis['min']}
- æœ€å¤§å€¼ï¼š{data_analysis['max']}
- å¹³å‡å€¼ï¼š{data_analysis['avg']:.2f}

## æŸ¥è¯¢ç»“æœï¼ˆå‰5æ¡ï¼‰
{self._format_results(mql_result['result'][:5])}

è¯·ç”Ÿæˆï¼š
1. **summary**ï¼ˆæ€»ç»“ï¼Œ2-3å¥è¯ï¼‰ï¼šæ¦‚æ‹¬ä¸»è¦å‘ç°
2. **key_findings**ï¼ˆå…³é”®å‘ç°ï¼Œ3-5ç‚¹ï¼‰ï¼šæ•°æ®ä¸­çš„é‡è¦ç‰¹å¾
3. **insights**ï¼ˆæ·±å…¥æ´å¯Ÿï¼Œ2-3ç‚¹ï¼‰ï¼šèƒŒåçš„åŸå› åˆ†æ
4. **suggestions**ï¼ˆè¡ŒåŠ¨å»ºè®®ï¼Œ2-3ç‚¹ï¼‰ï¼šåŸºäºæ•°æ®çš„å»ºè®®

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ˆä¸è¦ä½¿ç”¨markdownä»£ç å—ï¼Œç›´æ¥è¿”å›JSONï¼‰ï¼š
{{
    "summary": "...",
    "key_findings": ["...", "...", "..."],
    "insights": ["...", "..."],
    "suggestions": ["...", "..."]
}}
"""

    def _format_results(self, results: List[Dict]) -> str:
        """æ ¼å¼åŒ–æŸ¥è¯¢ç»“æœç”¨äºæç¤ºè¯.

        Args:
            results: æŸ¥è¯¢ç»“æœåˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        if not results:
            return "æ— æ•°æ®"

        lines = []
        for i, row in enumerate(results, 1):
            date = row.get("date", "æœªçŸ¥æ—¥æœŸ")
            value = row.get("value", 0)
            lines.append(f"{i}. {date}: {value}")

        return "\n".join(lines)

    def _calculate_confidence(
        self,
        data_analysis: Dict,
        interpretation: Dict
    ) -> float:
        """è®¡ç®—è§£è¯»ç½®ä¿¡åº¦.

        Args:
            data_analysis: æ•°æ®åˆ†æç»“æœ
            interpretation: LLMè§£è¯»ç»“æœ

        Returns:
            ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
        """
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦

        # 1. æ•°æ®é‡å½±å“ç½®ä¿¡åº¦
        if "data_count" in data_analysis:
            if data_analysis["data_count"] >= 7:
                confidence += 0.2
            elif data_analysis["data_count"] >= 3:
                confidence += 0.1

        # 2. æ³¢åŠ¨æ€§å½±å“ç½®ä¿¡åº¦ï¼ˆæ³¢åŠ¨è¿‡å¤§é™ä½ç½®ä¿¡åº¦ï¼‰
        if data_analysis.get("volatility", 0) < 20:
            confidence += 0.15
        elif data_analysis.get("volatility", 0) > 50:
            confidence -= 0.15

        # 3. è¶‹åŠ¿æ˜ç¡®æ€§å½±å“ç½®ä¿¡åº¦
        if data_analysis.get("trend") in ["upward", "downward"]:
            confidence += 0.1

        # 4. è§£è¯»å®Œæ•´æ€§å½±å“ç½®ä¿¡åº¦
        if interpretation.get("summary"):
            confidence += 0.05
        if interpretation.get("key_findings") and len(interpretation["key_findings"]) >= 3:
            confidence += 0.1
        if interpretation.get("insights") and len(interpretation["insights"]) >= 2:
            confidence += 0.1
        if interpretation.get("suggestions") and len(interpretation["suggestions"]) >= 2:
            confidence += 0.1

        # ç¡®ä¿ç½®ä¿¡åº¦åœ¨0-1ä¹‹é—´
        return max(0.0, min(1.0, confidence))

    def _generate_template_interpretation(
        self,
        query: str,
        data_analysis: Dict,
        metric_def: Dict,
        mql_result: Dict
    ) -> InterpretationResult:
        """ç”Ÿæˆæ¨¡æ¿è§£è¯»ï¼ˆé™çº§æ–¹æ¡ˆï¼‰.

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            data_analysis: æ•°æ®åˆ†æç»“æœ
            metric_def: æŒ‡æ ‡å®šä¹‰
            mql_result: MQLæ‰§è¡Œç»“æœ

        Returns:
            InterpretationResult: æ¨¡æ¿è§£è¯»ç»“æœ
        """
        return InterpretationResult(
            summary=self._generate_default_summary(data_analysis, metric_def),
            trend=data_analysis["trend"],
            key_findings=self._generate_default_findings(data_analysis),
            insights=self._generate_default_insights(data_analysis, metric_def),
            suggestions=self._generate_default_suggestions(data_analysis),
            confidence=0.6,  # æ¨¡æ¿è§£è¯»ç½®ä¿¡åº¦è¾ƒä½
            data_analysis=data_analysis
        )

    def _generate_default_summary(self, data_analysis: Dict, metric_def: Dict) -> str:
        """ç”Ÿæˆé»˜è®¤æ€»ç»“.

        Args:
            data_analysis: æ•°æ®åˆ†æç»“æœ
            metric_def: æŒ‡æ ‡å®šä¹‰

        Returns:
            æ€»ç»“å­—ç¬¦ä¸²
        """
        metric_name = metric_def.get("name", "æŒ‡æ ‡")
        change_rate = data_analysis["change_rate"]

        trend_desc = {
            "upward": "å‘ˆä¸Šå‡è¶‹åŠ¿",
            "downward": "å‘ˆä¸‹é™è¶‹åŠ¿",
            "fluctuating": "å‘ˆæ³¢åŠ¨çŠ¶æ€",
            "stable": "ä¿æŒç¨³å®š"
        }.get(data_analysis["trend"], "å˜åŒ–")

        if abs(change_rate) > 0:
            return f"{metric_name}{trend_desc}ï¼Œå˜åŒ–ç‡ä¸º{change_rate:.2f}%ã€‚"
        else:
            return f"{metric_name}{trend_desc}ã€‚"

    def _generate_default_findings(self, data_analysis: Dict) -> List[str]:
        """ç”Ÿæˆé»˜è®¤å…³é”®å‘ç°.

        Args:
            data_analysis: æ•°æ®åˆ†æç»“æœ

        Returns:
            å…³é”®å‘ç°åˆ—è¡¨
        """
        findings = []

        # è¶‹åŠ¿å‘ç°
        trend_desc = {
            "upward": "æ•°æ®å‘ˆä¸Šå‡è¶‹åŠ¿",
            "downward": "æ•°æ®å‘ˆä¸‹é™è¶‹åŠ¿",
            "fluctuating": "æ•°æ®æ³¢åŠ¨è¾ƒå¤§",
            "stable": "æ•°æ®ä¿æŒç¨³å®š"
        }.get(data_analysis["trend"], "æ•°æ®å˜åŒ–")

        findings.append(trend_desc)

        # å˜åŒ–ç‡å‘ç°
        if abs(data_analysis["change_rate"]) > 10:
            findings.append(f"æ€»ä½“å˜åŒ–ç‡è¾¾åˆ°{data_analysis['change_rate']:.2f}%")

        # æ³¢åŠ¨æ€§å‘ç°
        if data_analysis["volatility"] < 10:
            findings.append("æ³¢åŠ¨æ€§è¾ƒä½ï¼Œæ•°æ®ç¨³å®š")
        elif data_analysis["volatility"] > 30:
            findings.append(f"æ³¢åŠ¨æ€§è¾ƒé«˜ï¼ˆ{data_analysis['volatility']:.2f}%ï¼‰ï¼Œéœ€å…³æ³¨å¼‚å¸¸")

        # æå€¼å‘ç°
        findings.append(f"æœ€å°å€¼{data_analysis['min']}ï¼Œæœ€å¤§å€¼{data_analysis['max']}")

        # å¼‚å¸¸å€¼å‘ç°
        if data_analysis.get("anomalies"):
            findings.append(f"æ£€æµ‹åˆ°{len(data_analysis['anomalies'])}ä¸ªå¼‚å¸¸å€¼ç‚¹")

        return findings[:5]  # æœ€å¤šè¿”å›5æ¡

    def _generate_default_insights(
        self,
        data_analysis: Dict,
        metric_def: Dict
    ) -> List[str]:
        """ç”Ÿæˆé»˜è®¤æ·±å…¥æ´å¯Ÿ.

        Args:
            data_analysis: æ•°æ®åˆ†æç»“æœ
            metric_def: æŒ‡æ ‡å®šä¹‰

        Returns:
            æ·±å…¥æ´å¯Ÿåˆ—è¡¨
        """
        insights = []

        # åŸºäºè¶‹åŠ¿çš„æ´å¯Ÿ
        if data_analysis["trend"] == "upward":
            insights.append("æŒç»­å¢é•¿å¯èƒ½åæ˜ å‡ºä¸šåŠ¡æ‰©å¼ æˆ–å­£èŠ‚æ€§éœ€æ±‚å¢åŠ ")
        elif data_analysis["trend"] == "downward":
            insights.append("ä¸‹é™è¶‹åŠ¿å¯èƒ½ä¸å¸‚åœºç«äº‰åŠ å‰§æˆ–éœ€æ±‚å‡å°‘æœ‰å…³")
        elif data_analysis["trend"] == "fluctuating":
            insights.append("æ³¢åŠ¨å¯èƒ½å—å‘¨æœŸæ€§å› ç´ æˆ–ä¿ƒé”€æ´»åŠ¨å½±å“")

        # åŸºäºæ³¢åŠ¨æ€§çš„æ´å¯Ÿ
        if data_analysis["volatility"] > 30:
            insights.append("é«˜æ³¢åŠ¨æ€§è¡¨æ˜å­˜åœ¨ä¸ç¨³å®šå› ç´ ï¼Œå»ºè®®æ·±å…¥åˆ†æåŸå› ")

        # åŸºäºå¼‚å¸¸å€¼çš„æ´å¯Ÿ
        if data_analysis.get("anomalies"):
            insights.append("å¼‚å¸¸å€¼å¯èƒ½ä»£è¡¨ç‰¹æ®Šäº‹ä»¶æˆ–æ•°æ®è´¨é‡é—®é¢˜ï¼Œéœ€è¿›ä¸€æ­¥æ ¸å®")

        return insights[:3]  # æœ€å¤šè¿”å›3æ¡

    def _generate_default_suggestions(self, data_analysis: Dict) -> List[str]:
        """ç”Ÿæˆé»˜è®¤è¡ŒåŠ¨å»ºè®®.

        Args:
            data_analysis: æ•°æ®åˆ†æç»“æœ

        Returns:
            è¡ŒåŠ¨å»ºè®®åˆ—è¡¨
        """
        suggestions = []

        # åŸºäºè¶‹åŠ¿çš„å»ºè®®
        if data_analysis["trend"] == "upward":
            suggestions.append("å»ºè®®ä¿æŒå½“å‰ç­–ç•¥ï¼ŒåŒæ—¶ç›‘æ§å¢é•¿å¯æŒç»­æ€§")
        elif data_analysis["trend"] == "downward":
            suggestions.append("å»ºè®®åˆ†æä¸‹é™åŸå› ï¼Œè€ƒè™‘è°ƒæ•´ç­–ç•¥æˆ–é‡‡å–æ”¹è¿›æªæ–½")
        elif data_analysis["trend"] == "fluctuating":
            suggestions.append("å»ºè®®åŠ å¼ºæ•°æ®åˆ†æï¼Œè¯†åˆ«å¹¶æ¶ˆé™¤æ³¢åŠ¨å› ç´ ")

        # åŸºäºå¼‚å¸¸å€¼çš„å»ºè®®
        if data_analysis.get("anomalies"):
            suggestions.append("å»ºè®®è°ƒæŸ¥å¼‚å¸¸å€¼åŸå› ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§")

        # åŸºäºæ³¢åŠ¨æ€§çš„å»ºè®®
        if data_analysis["volatility"] > 30:
            suggestions.append("å»ºè®®å®æ–½é£é™©æ§åˆ¶æªæ–½ï¼Œé™ä½æ³¢åŠ¨æ€§")

        return suggestions[:3]  # æœ€å¤šè¿”å›3æ¡


# æµ‹è¯•
if __name__ == "__main__":
    print("\nğŸ§ª æµ‹è¯•æ™ºèƒ½è§£è¯»å™¨")
    print("=" * 60)

    interpreter = IntelligentInterpreter()

    # æ¨¡æ‹ŸæŸ¥è¯¢ç»“æœ
    from datetime import datetime, timedelta

    mock_data = []
    for i in range(7):
        mock_data.append({
            "date": (datetime.now() - timedelta(days=6-i)).strftime("%Y-%m-%d"),
            "value": 500000 + i * 20000 + (i % 2) * 10000,  # æ¨¡æ‹Ÿä¸Šå‡è¶‹åŠ¿
            "metric": "GMV",
            "unit": "å…ƒ"
        })

    metric_def = {
        "name": "GMV",
        "description": "å•†å“äº¤æ˜“æ€»é¢",
        "unit": "å…ƒ"
    }

    mql_result = {
        "result": mock_data,
        "row_count": len(mock_data)
    }

    # æµ‹è¯•è§£è¯»
    print("\næµ‹è¯•æŸ¥è¯¢: æœ€è¿‘7å¤©GMV")
    print("-" * 60)

    interpretation = interpreter.interpret(
        query="æœ€è¿‘7å¤©GMV",
        mql_result=mql_result,
        metric_def=metric_def
    )

    print(f"\nâœ… è§£è¯»ç»“æœ:")
    print(f"æ€»ç»“: {interpretation.summary}")
    print(f"è¶‹åŠ¿: {interpretation.trend}")
    print(f"ç½®ä¿¡åº¦: {interpretation.confidence:.2f}")
    print(f"\nå…³é”®å‘ç°:")
    for finding in interpretation.key_findings:
        print(f"  - {finding}")
    print(f"\næ·±å…¥æ´å¯Ÿ:")
    for insight in interpretation.insights:
        print(f"  - {insight}")
    print(f"\nè¡ŒåŠ¨å»ºè®®:")
    for suggestion in interpretation.suggestions:
        print(f"  - {suggestion}")

    print("\n" + "=" * 60)
