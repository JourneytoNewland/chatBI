"""å¢å¼ºç‰ˆä¸‰å±‚æ··åˆæ„å›¾è¯†åˆ«æ¶æ„ï¼ˆé›†æˆæ™ºè°±AIï¼‰."""

import asyncio
import os
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional

from .intent import IntentRecognizer, QueryIntent, TimeGranularity, AggregationType
from .llm_intent import LLMIntentRecognizer, LocalLLMIntentRecognizer
from .zhipu_intent import ZhipuIntentRecognizer
from ..recall.semantic_recall import SemanticRecall, FallbackSemanticRecall
from ..recall.dual_recall import DualRecall
from ..recall.vector.qdrant_store import QdrantVectorStore
from ..recall.vector.vectorizer import MetricVectorizer
from ..recall.graph.neo4j_client import Neo4jClient
from ..rerank.ranker import RuleBasedRanker
from ..rerank.models import Candidate, QueryContext


@dataclass
class LayerResult:
    """å•å±‚è¯†åˆ«ç»“æœ."""

    layer_name: str
    success: bool
    intent: Optional[QueryIntent]
    confidence: float
    duration: float
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class HybridIntentResult:
    """æ··åˆæ¶æ„è¯†åˆ«ç»“æœ."""

    query: str
    final_intent: QueryIntent
    source_layer: str
    all_layers: list[LayerResult]
    total_duration: float
    candidates: list[Any] = field(default_factory=list)  # å€™é€‰æŒ‡æ ‡


class EnhancedHybridIntentRecognizer:
    """å¢å¼ºç‰ˆä¸‰å±‚æ··åˆæ„å›¾è¯†åˆ«å™¨.

    æ¶æ„:
    L1: è§„åˆ™åŒ¹é… (å¿«é€Ÿ, <10ms, å¤„ç†10%å¸¸è§æŸ¥è¯¢)
    L2: è¯­ä¹‰å‘é‡ (ä¸­ç­‰, ~50ms, å¤„ç†60%æŸ¥è¯¢)
    L3: LLMæ¨ç† (å‡†ç¡®, ~500ms, å¤„ç†30%å¤æ‚æŸ¥è¯¢)

    LLMé€‰é¡¹:
    - æ™ºè°±AI GLM (æ¨è, å›½äº§, Â¥1/1M tokens)
    - OpenAI GPT-4o (å¤‡é€‰, $0.005/1K tokens)
    - æœ¬åœ°Ollama (å…è´¹, éœ€è¦GPU)
    """

    def __init__(
        self,
        llm_provider: str = "zhipu",  # zhipu/openai/local
        enable_semantic: bool = True,
        enable_dual_recall: bool = True,  # æ–°å¢ï¼šæ˜¯å¦å¯ç”¨åŒè·¯å¬å›
        enable_rerank: bool = True,  # æ–°å¢ï¼šæ˜¯å¦å¯ç”¨ç²¾æ’
        confidence_thresholds: dict[str, float] = None
    ):
        """åˆå§‹åŒ–æ··åˆè¯†åˆ«å™¨.

        Args:
            llm_provider: LLMæä¾›å•† (zhipu/openai/local)
            enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰å‘é‡æ£€ç´¢
            enable_dual_recall: æ˜¯å¦å¯ç”¨åŒè·¯å¬å›ï¼ˆå‘é‡+å›¾è°±ï¼‰
            enable_rerank: æ˜¯å¦å¯ç”¨11ç»´ç‰¹å¾ç²¾æ’
            confidence_thresholds: å„å±‚ç½®ä¿¡åº¦é˜ˆå€¼
        """
        # L1: è§„åˆ™è¯†åˆ«å™¨
        self.rule_recognizer = IntentRecognizer()

        # L2: è¯­ä¹‰å¬å›ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
        self.enable_semantic = enable_semantic
        if enable_semantic:
            try:
                self.semantic_recall = SemanticRecall()
            except Exception as e:
                print(f"âš ï¸  è¯­ä¹‰å¬å›åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨å…œåº•æ–¹æ¡ˆ")
                self.semantic_recall = FallbackSemanticRecall()
        else:
            self.semantic_recall = FallbackSemanticRecall()

        # L2å¢å¼º: åŒè·¯å¬å›èåˆ
        self.enable_dual_recall = enable_dual_recall
        self.dual_recall = None
        if enable_dual_recall:
            try:
                # ä½¿ç”¨ä¸ç³»ç»Ÿç›¸åŒçš„å‘é‡æ¨¡å‹
                import os
                model_name = os.getenv('VECTORIZER_MODEL_NAME', 'sentence-transformers/all-MiniLM-L6-v2')
                vectorizer = MetricVectorizer(model_name=model_name)
                vector_store = QdrantVectorStore()
                neo4j_client = Neo4jClient()
                self.dual_recall = DualRecall(vectorizer, vector_store, neo4j_client)
                print("âœ… åŒè·¯å¬å›åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  åŒè·¯å¬å›åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨å•ä¸€è¯­ä¹‰å¬å›")
                self.dual_recall = None

        # L2å¢å¼º: èåˆç²¾æ’
        self.enable_rerank = enable_rerank
        self.ranker = None
        if enable_rerank:
            try:
                self.ranker = RuleBasedRanker()
                print("âœ… èåˆç²¾æ’å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸  èåˆç²¾æ’å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.ranker = None

        # L3: LLMè¯†åˆ«å™¨
        self.llm_provider = llm_provider
        if llm_provider == "zhipu":
            # ä¼˜å…ˆä½¿ç”¨æ™ºè°±AIï¼ˆå›½äº§ï¼Œä»·æ ¼ä¼˜æƒ ï¼‰
            self.llm_recognizer = ZhipuIntentRecognizer(model="glm-4-flash")
        elif llm_provider == "openai":
            self.llm_recognizer = LLMIntentRecognizer(model="gpt-4o-mini")
        elif llm_provider == "local":
            self.llm_recognizer = LocalLLMIntentRecognizer(model="qwen2.5:7b")
        else:
            self.llm_recognizer = None

        # ç½®ä¿¡åº¦é˜ˆå€¼
        self.thresholds = confidence_thresholds or {
            "rule": 0.90,  # è§„åˆ™åŒ¹é…éœ€è¦é«˜ç½®ä¿¡åº¦
            "semantic": 0.75,  # è¯­ä¹‰åŒ¹é…é˜ˆå€¼
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "l1_hits": 0,
            "l2_hits": 0,
            "l3_hits": 0,
            "failures": 0,
        }

    def recognize(self, query: str, top_k: int = 10) -> HybridIntentResult:
        """ä½¿ç”¨ä¸‰å±‚æ¶æ„è¯†åˆ«æŸ¥è¯¢æ„å›¾.

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å€™é€‰æ•°é‡

        Returns:
            æ··åˆè¯†åˆ«ç»“æœ
        """
        self.stats["total_queries"] += 1

        start = time.time()
        all_layers = []
        candidates = []

        # L1: è§„åˆ™åŒ¹é…
        l1_result = self._layer1_rule_match(query)
        all_layers.append(l1_result)

        if l1_result.success and l1_result.confidence >= self.thresholds["rule"]:
            self.stats["l1_hits"] += 1
            return HybridIntentResult(
                query=query,
                final_intent=l1_result.intent,
                source_layer="L1_Rule",
                all_layers=all_layers,
                total_duration=time.time() - start,
                candidates=candidates
            )

        # L2: è¯­ä¹‰å‘é‡åŒ¹é…
        l2_result = self._layer2_semantic_match(query, top_k)
        all_layers.append(l2_result)

        # è·å–L2çš„å€™é€‰æŒ‡æ ‡ï¼Œä¼ é€’ç»™L3
        l2_candidates = l2_result.metadata.get("candidates", [])

        if l2_result.success and l2_result.confidence >= self.thresholds["semantic"]:
            self.stats["l2_hits"] += 1
            return HybridIntentResult(
                query=query,
                final_intent=l2_result.intent,
                source_layer="L2_Semantic",
                all_layers=all_layers,
                total_duration=time.time() - start,
                candidates=l2_candidates
            )

        # L3: LLMæ·±åº¦æ¨ç†ï¼ˆä¼ é€’L2çš„å€™é€‰æŒ‡æ ‡ï¼‰
        l3_result = self._layer3_llm_inference(query, l2_candidates)
        all_layers.append(l3_result)

        if l3_result.success:
            self.stats["l3_hits"] += 1
            return HybridIntentResult(
                query=query,
                final_intent=l3_result.intent,
                source_layer="L3_LLM",
                all_layers=all_layers,
                total_duration=time.time() - start,
                candidates=l3_result.metadata.get("candidates", [])
            )

        # å…¨éƒ¨å¤±è´¥ï¼Œä½¿ç”¨æœ€ä½³é™çº§ç»“æœ
        self.stats["failures"] += 1
        best_result = max(
            [r for r in all_layers if r.intent],
            key=lambda x: x.confidence,
            default=l1_result
        )

        return HybridIntentResult(
            query=query,
            final_intent=best_result.intent or QueryIntent(
                query=query,
                core_query=query,
                time_range=None,
                time_granularity=None,
                aggregation_type=None,
                dimensions=[],
                comparison_type=None,
                filters={}
            ),
            source_layer="Fallback",
            all_layers=all_layers,
            total_duration=time.time() - start,
            candidates=[]
        )

    def _layer1_rule_match(self, query: str) -> LayerResult:
        """L1å±‚ï¼šåŸºäºè§„åˆ™çš„å¿«é€ŸåŒ¹é…."""
        start = time.time()

        try:
            intent = self.rule_recognizer.recognize(query)
            confidence = self._calculate_rule_confidence(query, intent)

            return LayerResult(
                layer_name="L1_Rule",
                success=True,
                intent=intent,
                confidence=confidence,
                duration=time.time() - start,
                metadata={
                    "method": "regex_patterns",
                    "time_detected": intent.time_range is not None,
                    "aggregation_detected": intent.aggregation_type is not None,
                    "dimensions_detected": len(intent.dimensions) > 0
                }
            )

        except Exception as e:
            return LayerResult(
                layer_name="L1_Rule",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": str(e)}
            )

    def _layer2_semantic_match(self, query: str, top_k: int) -> LayerResult:
        """L2å±‚ï¼šè¯­ä¹‰å‘é‡åŒ¹é…ï¼ˆå¢å¼ºç‰ˆï¼šåŒè·¯å¬å› + èåˆç²¾æ’ï¼‰."""
        start = time.time()

        try:
            # å¦‚æœå¯ç”¨äº†åŒè·¯å¬å›ï¼Œä½¿ç”¨ DualRecall
            if self.dual_recall is not None:
                return self._dual_recall_with_rerank(query, top_k, start)

            # å¦åˆ™ä½¿ç”¨åŸæœ‰çš„å•ä¸€è¯­ä¹‰å¬å›ï¼ˆå‘åå…¼å®¹ï¼‰
            recall_result = self.semantic_recall.recall(query, top_k=top_k)

            if not recall_result:
                raise Exception("è¯­ä¹‰å¬å›å¤±è´¥")

            # ä»å¬å›ç»“æœæå–æ„å›¾
            intent = self.rule_recognizer.recognize(query)

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºå¬å›ç»“æœï¼‰
            confidence = 0.8  # åŸºç¡€åˆ†
            if recall_result.total > 0:
                top_score = recall_result.candidates[0].score
                confidence = max(confidence, top_score)

            return LayerResult(
                layer_name="L2_Semantic",
                success=True,
                intent=intent,
                confidence=confidence,
                duration=time.time() - start,
                metadata={
                    "method": recall_result.search_method,
                    "recall_type": "semantic_only",  # æ ‡è¯†ä¸ºå•ä¸€è¯­ä¹‰å¬å›
                    "candidates_found": recall_result.total,
                    "top_score": recall_result.candidates[0].score if recall_result.candidates else 0,
                    "embedding_dim": recall_result.embedding_dim,
                    "candidates": [
                        {
                            "name": c.name,
                            "score": c.score,
                            "reason": c.match_reason
                        }
                        for c in recall_result.candidates[:3]
                    ]
                }
            )

        except Exception as e:
            return LayerResult(
                layer_name="L2_Semantic",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": str(e)}
            )

    def _dual_recall_with_rerank(self, query: str, top_k: int, start_time: float) -> LayerResult:
        """ä½¿ç”¨åŒè·¯å¬å›å’Œèåˆç²¾æ’çš„L2å±‚å®ç°.

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›æ•°é‡
            start_time: å¼€å§‹æ—¶é—´ï¼ˆç”¨äºè®¡ç®—è€—æ—¶ï¼‰

        Returns:
            LayerResult åŒ…å«è¯¦ç»†çš„å¬å›å’Œç²¾æ’ä¿¡æ¯
        """
        try:
            # Step 1: åŒè·¯å¬å›ï¼ˆä½¿ç”¨çº¿ç¨‹æ± è¿è¡Œå¼‚æ­¥ä»£ç ï¼‰
            from concurrent.futures import ThreadPoolExecutor
            import threading

            result_container = []
            exception_container = []

            def run_in_thread():
                """åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥ä»£ç """
                try:
                    # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
                    new_loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(new_loop)

                    # è¿è¡Œå¼‚æ­¥åŒè·¯å¬å›
                    recall_results = new_loop.run_until_complete(
                        self.dual_recall.dual_recall(
                            query=query,
                            vector_top_k=50,
                            graph_top_k=30,
                            final_top_k=top_k * 2,  # å…ˆå¬å›æ›´å¤šï¼Œä¾›ç²¾æ’ä½¿ç”¨
                            timeout=1.0
                        )
                    )

                    result_container.append(recall_results)
                    new_loop.close()
                except Exception as e:
                    exception_container.append(e)
                finally:
                    # æ¸…ç†äº‹ä»¶å¾ªç¯
                    try:
                        new_loop = asyncio.get_event_loop()
                        if new_loop and not new_loop.is_closed():
                            new_loop.close()
                    except:
                        pass

            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
            with ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_in_thread)
                future.result(timeout=5)  # 5ç§’è¶…æ—¶

            # æ£€æŸ¥å¼‚å¸¸
            if exception_container:
                print(f"âŒ åŒè·¯å¬å›å¼‚å¸¸: {exception_container[0]}")
                import traceback
                traceback.print_exc()
                raise exception_container[0]

            # è·å–ç»“æœ
            recall_results = result_container[0] if result_container else None

            if not recall_results:
                print(f"âš ï¸  åŒè·¯å¬å›è¿”å›ç©ºç»“æœï¼Œresult_container={result_container}")
                raise Exception("åŒè·¯å¬å›æœªè¿”å›ç»“æœ")

            # Step 2: è½¬æ¢ä¸º Candidate å¯¹è±¡ï¼ˆç”¨äºç²¾æ’ï¼‰
            candidates = []
            for result in recall_results:
                candidate = Candidate(
                    metric_id=result.metric_id,
                    name=result.name,
                    code=result.code,
                    description=result.description,
                    domain=result.domain,
                    synonyms=[],  # å¯ä»¥åç»­å¡«å……
                    importance=0.5,  # é»˜è®¤é‡è¦æ€§
                    formula=None,
                    vector_score=result.vector_score or 0.0,
                    graph_score=result.graph_score or 0.0,
                    source=result.source
                )
                candidates.append(candidate)

            # Step 3: èåˆç²¾æ’
            query_context = QueryContext.from_text(query)
            reranked_results = []

            if self.ranker is not None:
                # ä½¿ç”¨ç²¾æ’å™¨
                reranked_results = self.ranker.rerank(candidates, query_context, top_k=top_k)
            else:
                # é™çº§ï¼šæŒ‰åŸå§‹åˆ†æ•°æ’åº
                reranked_results = [
                    (c, c.vector_score, {})
                    for c in sorted(candidates, key=lambda x: x.vector_score, reverse=True)
                ][:top_k]

            # Step 4: æå–æœ€ç»ˆæ„å›¾ï¼ˆä½¿ç”¨æ’åç¬¬ä¸€çš„ç»“æœï¼‰
            top_candidate = reranked_results[0][0] if reranked_results else candidates[0]

            # ä½¿ç”¨è§„åˆ™è¯†åˆ«å™¨ä»å€™é€‰æŒ‡æ ‡ä¸­æå–å®Œæ•´æ„å›¾
            intent = self.rule_recognizer.recognize(query)

            # Step 5: è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºç²¾æ’åçš„åˆ†æ•°ï¼‰
            top_score = reranked_results[0][1] if reranked_results else 0.0
            confidence = max(0.75, min(0.95, top_score))

            # Step 6: æ„å»ºè¯¦ç»†çš„å…ƒæ•°æ®
            metadata = {
                "method": "dual_recall_with_rerank",
                "recall_type": "dual_recall",  # æ ‡è¯†ä¸ºåŒè·¯å¬å›
                "vector_top_k": 50,
                "graph_top_k": 30,
                "final_top_k": top_k,
                "candidates_found": len(recall_results),
                "reranked": self.ranker is not None,
                "fusion_stats": self._calculate_fusion_stats(recall_results),
            }

            # æ·»åŠ å¬å›æ¥æºç»Ÿè®¡
            source_counts = {"vector": 0, "graph": 0, "both": 0}
            for r in recall_results:
                source_counts[r.source] = source_counts.get(r.source, 0) + 1
            metadata["source_distribution"] = source_counts

            # æ·»åŠ  Top-3 å€™é€‰çš„è¯¦ç»†ä¿¡æ¯
            metadata["candidates"] = []
            for i, (candidate, score, details) in enumerate(reranked_results[:3]):
                candidate_info = {
                    "rank": i + 1,
                    "name": candidate.name,
                    "code": candidate.code,
                    "domain": candidate.domain,
                    "final_score": score,
                    "vector_score": candidate.vector_score,
                    "graph_score": candidate.graph_score,
                    "source": candidate.source,
                }

                # å¦‚æœæœ‰ç²¾æ’è¯¦æƒ…ï¼Œæ·»åŠ ç‰¹å¾åˆ†æ•°
                if details:
                    candidate_info["feature_scores"] = details

                metadata["candidates"].append(candidate_info)

            # å¦‚æœå¯ç”¨äº†ç²¾æ’ï¼Œæ·»åŠ ç‰¹å¾æƒé‡ä¿¡æ¯
            if self.ranker is not None:
                metadata["feature_weights"] = self.ranker.weights

            return LayerResult(
                layer_name="L2_Semantic_Enhanced",
                success=True,
                intent=intent,
                confidence=confidence,
                duration=time.time() - start_time,
                metadata=metadata
            )

        except Exception as e:
            import traceback
            error_details = str(e)
            error_traceback = traceback.format_exc()

            print(f"âŒ åŒè·¯å¬å›+ç²¾æ’å¤±è´¥: {error_details}")
            print(f"Traceback: {error_traceback}")

            # é™çº§åˆ°å•ä¸€è¯­ä¹‰å¬å›
            print("âš ï¸  é™çº§åˆ°å•ä¸€è¯­ä¹‰å¬å›")
            return self._layer2_semantic_match_fallback(query, top_k, start_time)

    def _layer2_semantic_match_fallback(self, query: str, top_k: int, start_time: float) -> LayerResult:
        """é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨å•ä¸€è¯­ä¹‰å¬å›."""
        try:
            recall_result = self.semantic_recall.recall(query, top_k=top_k)

            if not recall_result:
                raise Exception("è¯­ä¹‰å¬å›é™çº§ä¹Ÿå¤±è´¥")

            intent = self.rule_recognizer.recognize(query)

            confidence = 0.75  # é™çº§åç½®ä¿¡åº¦é™ä½
            if recall_result.total > 0:
                top_score = recall_result.candidates[0].score
                confidence = max(confidence, top_score * 0.9)

            return LayerResult(
                layer_name="L2_Semantic_Fallback",
                success=True,
                intent=intent,
                confidence=confidence,
                duration=time.time() - start_time,
                metadata={
                    "method": recall_result.search_method,
                    "recall_type": "semantic_fallback",  # æ ‡è¯†ä¸ºé™çº§æ–¹æ¡ˆ
                    "fallback_reason": "dual_recall_failed",
                    "candidates_found": recall_result.total,
                    "top_score": recall_result.candidates[0].score if recall_result.candidates else 0,
                    "candidates": [
                        {
                            "name": c.name,
                            "score": c.score,
                            "reason": c.match_reason
                        }
                        for c in recall_result.candidates[:3]
                    ]
                }
            )
        except Exception as e:
            return LayerResult(
                layer_name="L2_Semantic",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start_time,
                metadata={"error": str(e), "fallback_failed": True}
            )

    def _calculate_fusion_stats(self, recall_results: list) -> dict:
        """è®¡ç®—å¬å›èåˆçš„ç»Ÿè®¡ä¿¡æ¯.

        Args:
            recall_results: å¬å›ç»“æœåˆ—è¡¨

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not recall_results:
            return {}

        vector_scores = [r.vector_score for r in recall_results if r.vector_score is not None]
        graph_scores = [r.graph_score for r in recall_results if r.graph_score is not None]

        stats = {
            "total_candidates": len(recall_results),
            "vector_avg_score": sum(vector_scores) / len(vector_scores) if vector_scores else 0,
            "graph_avg_score": sum(graph_scores) / len(graph_scores) if graph_scores else 0,
            "vector_max_score": max(vector_scores) if vector_scores else 0,
            "graph_max_score": max(graph_scores) if graph_scores else 0,
        }

        return stats

    def _layer3_llm_inference(self, query: str, candidates: list = None) -> LayerResult:
        """L3å±‚ï¼šLLMæ·±åº¦æ¨ç†.

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            candidates: ä»L2å±‚è·å–çš„å€™é€‰æŒ‡æ ‡åˆ—è¡¨
        """
        start = time.time()

        if not self.llm_recognizer:
            return LayerResult(
                layer_name="L3_LLM",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": "LLM not configured"}
            )

        try:
            # è°ƒç”¨LLMï¼Œä¼ é€’candidatesä»¥å¸®åŠ©æ­£ç¡®è¯†åˆ«æŒ‡æ ‡
            if self.llm_provider == "zhipu":
                llm_result = self.llm_recognizer.recognize(query, candidates)
            else:
                llm_result = self.llm_recognizer.recognize(query, candidates)

            if not llm_result:
                raise Exception("LLM returned None")

            # è½¬æ¢ä¸ºQueryIntent
            intent = QueryIntent(
                query=query,
                core_query=llm_result.core_query,
                time_range=self._parse_time_range(llm_result.time_range),
                time_granularity=self._parse_granularity(llm_result.time_granularity),
                aggregation_type=self._parse_aggregation(llm_result.aggregation_type),
                dimensions=llm_result.dimensions,
                comparison_type=llm_result.comparison_type,
                filters=llm_result.filters
            )

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "model": llm_result.model,
                "reasoning": llm_result.reasoning,
                "confidence": llm_result.confidence
            }

            if hasattr(llm_result, 'tokens_used'):
                metadata["tokens_used"] = llm_result.tokens_used

            if self.llm_provider == "zhipu":
                metadata["cost"] = self._estimate_zhipu_cost(llm_result.tokens_used)

            return LayerResult(
                layer_name=f"L3_LLM_{self.llm_provider.capitalize()}",
                success=True,
                intent=intent,
                confidence=llm_result.confidence,
                duration=llm_result.latency,
                metadata=metadata
            )

        except Exception as e:
            return LayerResult(
                layer_name="L3_LLM",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": str(e)}
            )

    def _calculate_rule_confidence(self, query: str, intent: QueryIntent) -> float:
        """è®¡ç®—è§„åˆ™åŒ¹é…çš„ç½®ä¿¡åº¦."""
        score = 0.5

        if intent.core_query.lower() in query.lower():
            score += 0.2

        if intent.time_range:
            score += 0.15

        if intent.aggregation_type:
            score += 0.1

        if intent.dimensions:
            score += 0.1

        if len(intent.core_query) < len(query) * 0.5:
            score += 0.1

        return min(score, 1.0)

    def _parse_time_range(self, time_range: Optional[dict]) -> Optional[tuple]:
        """è§£æLLMè¿”å›çš„æ—¶é—´èŒƒå›´."""
        if not time_range:
            return None
        # TODO: å®ç°æ—¶é—´èŒƒå›´è§£æ
        return None

    def _parse_granularity(self, granularity: Optional[str]) -> Optional[TimeGranularity]:
        """è§£ææ—¶é—´ç²’åº¦."""
        if not granularity:
            return None
        try:
            return TimeGranularity(granularity)
        except ValueError:
            return None

    def _parse_aggregation(self, aggregation: Optional[str]) -> Optional[AggregationType]:
        """è§£æèšåˆç±»å‹."""
        if not aggregation:
            return None
        try:
            return AggregationType(aggregation)
        except ValueError:
            return None

    def _estimate_zhipu_cost(self, tokens_used: dict[str, int]) -> float:
        """ä¼°ç®—æ™ºè°±AIæˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰."""
        # æ™ºè°±AIä»·æ ¼: Â¥1/1M tokens (glm-4-flashå…è´¹)
        total_tokens = tokens_used.get("total_tokens", 0)
        return total_tokens / 1_000_000 * 1.0  # Â¥1/1M

    def get_statistics(self) -> dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯."""
        total = self.stats["total_queries"]

        if total == 0:
            return {"message": "æš‚æ— ç»Ÿè®¡æ•°æ®"}

        return {
            "total_queries": total,
            "layer_distribution": {
                "L1_Rule": f"{self.stats['l1_hits']/total*100:.1f}%",
                "L2_Semantic": f"{self.stats['l2_hits']/total*100:.1f}%",
                f"L3_LLM_{self.llm_provider.capitalize()}": f"{self.stats['l3_hits']/total*100:.1f}%",
            },
            "failure_rate": f"{self.stats['failures']/total*100:.1f}%",
            "llm_provider": self.llm_provider,
            "semantic_enabled": self.enable_semantic
        }


# æµ‹è¯•å‡½æ•°
def test_enhanced_hybrid():
    """æµ‹è¯•å¢å¼ºç‰ˆæ··åˆæ¶æ„."""
    print("\nğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆä¸‰å±‚æ··åˆæ¶æ„")
    print("=" * 50)

    # åˆå§‹åŒ–ï¼ˆä½¿ç”¨æ™ºè°±AIï¼‰
    recognizer = EnhancedHybridIntentRecognizer(
        llm_provider="zhipu",
        enable_semantic=True
    )

    test_queries = [
        "GMVæ˜¯ä»€ä¹ˆ",
        "æœ€è¿‘7å¤©çš„æˆäº¤é‡‘é¢",
        "æœ¬æœˆè¥æ”¶æ€»å’Œ"
    ]

    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("-" * 50)

        result = recognizer.recognize(query)

        print(f"âœ… è¯†åˆ«æˆåŠŸ")
        print(f"   æ¥æºå±‚: {result.source_layer}")
        print(f"   æ ¸å¿ƒæŸ¥è¯¢: {result.final_intent.core_query}")
        print(f"   æ€»è€—æ—¶: {result.total_duration*1000:.2f}ms")

        print(f"\n   å„å±‚ç»“æœ:")
        for layer in result.all_layers:
            if layer.success:
                print(f"      {layer.layer_name}: âœ“ ({layer.confidence:.2f}, {layer.duration*1000:.2f}ms)")
            else:
                print(f"      {layer.layer_name}: âœ—")

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(recognizer.get_statistics())

    print("\n" + "=" * 50)


if __name__ == "__main__":
    test_enhanced_hybrid()
