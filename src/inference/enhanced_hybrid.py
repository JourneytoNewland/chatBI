"""å¢å¼ºç‰ˆä¸‰å±‚æ··åˆæ„å›¾è¯†åˆ«æ¶æ„ï¼ˆé›†æˆæ™ºè°±AIï¼‰."""

import os
import time
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Optional

from .intent import IntentRecognizer, QueryIntent, TimeGranularity, AggregationType
from .llm_intent import LLMIntentRecognizer, LocalLLMIntentRecognizer
from .zhipu_intent import ZhipuIntentRecognizer
from ..recall.semantic_recall import SemanticRecall, FallbackSemanticRecall


@dataclass
class LayerResult:
    """å•å±‚è¯†åˆ«ç»“æœ."""

    layer_name: str
    success: bool
    intent: Optional[QueryIntent]
    confidence: float
    duration: float
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class HybridIntentResult:
    """æ··åˆæ¶æ„è¯†åˆ«ç»“æœ."""

    query: str
    final_intent: QueryIntent
    source_layer: str
    all_layers: list[LayerResult]
    total_duration: float
    candidates: list[Any] = field(default_factory=list)  # å€™é€‰æŒ‡æ ‡


class EnhancedHybridIntentRecognizer:
    """å¢å¼ºç‰ˆä¸‰å±‚æ··åˆæ„å›¾è¯†åˆ«å™¨.

    æ¶æ„:
    L1: è§„åˆ™åŒ¹é… (å¿«é€Ÿ, <10ms, å¤„ç†10%å¸¸è§æŸ¥è¯¢)
    L2: è¯­ä¹‰å‘é‡ (ä¸­ç­‰, ~50ms, å¤„ç†60%æŸ¥è¯¢)
    L3: LLMæ¨ç† (å‡†ç¡®, ~500ms, å¤„ç†30%å¤æ‚æŸ¥è¯¢)

    LLMé€‰é¡¹:
    - æ™ºè°±AI GLM (æ¨è, å›½äº§, Â¥1/1M tokens)
    - OpenAI GPT-4o (å¤‡é€‰, $0.005/1K tokens)
    - æœ¬åœ°Ollama (å…è´¹, éœ€è¦GPU)
    """

    def __init__(
        self,
        llm_provider: str = "zhipu",  # zhipu/openai/local
        enable_semantic: bool = True,
        confidence_thresholds: dict[str, float] = None
    ):
        """åˆå§‹åŒ–æ··åˆè¯†åˆ«å™¨.

        Args:
            llm_provider: LLMæä¾›å•† (zhipu/openai/local)
            enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰å‘é‡æ£€ç´¢
            confidence_thresholds: å„å±‚ç½®ä¿¡åº¦é˜ˆå€¼
        """
        # L1: è§„åˆ™è¯†åˆ«å™¨
        self.rule_recognizer = IntentRecognizer()

        # L2: è¯­ä¹‰å¬å›
        self.enable_semantic = enable_semantic
        if enable_semantic:
            try:
                self.semantic_recall = SemanticRecall()
            except Exception as e:
                print(f"âš ï¸  è¯­ä¹‰å¬å›åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨å…œåº•æ–¹æ¡ˆ")
                self.semantic_recall = FallbackSemanticRecall()
        else:
            self.semantic_recall = FallbackSemanticRecall()

        # L3: LLMè¯†åˆ«å™¨
        self.llm_provider = llm_provider
        if llm_provider == "zhipu":
            # ä¼˜å…ˆä½¿ç”¨æ™ºè°±AIï¼ˆå›½äº§ï¼Œä»·æ ¼ä¼˜æƒ ï¼‰
            self.llm_recognizer = ZhipuIntentRecognizer(model="glm-4-flash")
        elif llm_provider == "openai":
            self.llm_recognizer = LLMIntentRecognizer(model="gpt-4o-mini")
        elif llm_provider == "local":
            self.llm_recognizer = LocalLLMIntentRecognizer(model="qwen2.5:7b")
        else:
            self.llm_recognizer = None

        # ç½®ä¿¡åº¦é˜ˆå€¼
        self.thresholds = confidence_thresholds or {
            "rule": 0.90,  # è§„åˆ™åŒ¹é…éœ€è¦é«˜ç½®ä¿¡åº¦
            "semantic": 0.75,  # è¯­ä¹‰åŒ¹é…é˜ˆå€¼
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queries": 0,
            "l1_hits": 0,
            "l2_hits": 0,
            "l3_hits": 0,
            "failures": 0,
        }

    def recognize(self, query: str, top_k: int = 10) -> HybridIntentResult:
        """ä½¿ç”¨ä¸‰å±‚æ¶æ„è¯†åˆ«æŸ¥è¯¢æ„å›¾.

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›å€™é€‰æ•°é‡

        Returns:
            æ··åˆè¯†åˆ«ç»“æœ
        """
        self.stats["total_queries"] += 1

        start = time.time()
        all_layers = []
        candidates = []

        # L1: è§„åˆ™åŒ¹é…
        l1_result = self._layer1_rule_match(query)
        all_layers.append(l1_result)

        if l1_result.success and l1_result.confidence >= self.thresholds["rule"]:
            self.stats["l1_hits"] += 1
            return HybridIntentResult(
                query=query,
                final_intent=l1_result.intent,
                source_layer="L1_Rule",
                all_layers=all_layers,
                total_duration=time.time() - start,
                candidates=candidates
            )

        # L2: è¯­ä¹‰å‘é‡åŒ¹é…
        l2_result = self._layer2_semantic_match(query, top_k)
        all_layers.append(l2_result)

        # è·å–L2çš„å€™é€‰æŒ‡æ ‡ï¼Œä¼ é€’ç»™L3
        l2_candidates = l2_result.metadata.get("candidates", [])

        if l2_result.success and l2_result.confidence >= self.thresholds["semantic"]:
            self.stats["l2_hits"] += 1
            return HybridIntentResult(
                query=query,
                final_intent=l2_result.intent,
                source_layer="L2_Semantic",
                all_layers=all_layers,
                total_duration=time.time() - start,
                candidates=l2_candidates
            )

        # L3: LLMæ·±åº¦æ¨ç†ï¼ˆä¼ é€’L2çš„å€™é€‰æŒ‡æ ‡ï¼‰
        l3_result = self._layer3_llm_inference(query, l2_candidates)
        all_layers.append(l3_result)

        if l3_result.success:
            self.stats["l3_hits"] += 1
            return HybridIntentResult(
                query=query,
                final_intent=l3_result.intent,
                source_layer="L3_LLM",
                all_layers=all_layers,
                total_duration=time.time() - start,
                candidates=l3_result.metadata.get("candidates", [])
            )

        # å…¨éƒ¨å¤±è´¥ï¼Œä½¿ç”¨æœ€ä½³é™çº§ç»“æœ
        self.stats["failures"] += 1
        best_result = max(
            [r for r in all_layers if r.intent],
            key=lambda x: x.confidence,
            default=l1_result
        )

        return HybridIntentResult(
            query=query,
            final_intent=best_result.intent or QueryIntent(
                query=query,
                core_query=query,
                time_range=None,
                time_granularity=None,
                aggregation_type=None,
                dimensions=[],
                comparison_type=None,
                filters={}
            ),
            source_layer="Fallback",
            all_layers=all_layers,
            total_duration=time.time() - start,
            candidates=[]
        )

    def _layer1_rule_match(self, query: str) -> LayerResult:
        """L1å±‚ï¼šåŸºäºè§„åˆ™çš„å¿«é€ŸåŒ¹é…."""
        start = time.time()

        try:
            intent = self.rule_recognizer.recognize(query)
            confidence = self._calculate_rule_confidence(query, intent)

            return LayerResult(
                layer_name="L1_Rule",
                success=True,
                intent=intent,
                confidence=confidence,
                duration=time.time() - start,
                metadata={
                    "method": "regex_patterns",
                    "time_detected": intent.time_range is not None,
                    "aggregation_detected": intent.aggregation_type is not None,
                    "dimensions_detected": len(intent.dimensions) > 0
                }
            )

        except Exception as e:
            return LayerResult(
                layer_name="L1_Rule",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": str(e)}
            )

    def _layer2_semantic_match(self, query: str, top_k: int) -> LayerResult:
        """L2å±‚ï¼šè¯­ä¹‰å‘é‡åŒ¹é…."""
        start = time.time()

        try:
            # è¯­ä¹‰å¬å›
            recall_result = self.semantic_recall.recall(query, top_k=top_k)

            if not recall_result:
                raise Exception("è¯­ä¹‰å¬å›å¤±è´¥")

            # ä»å¬å›ç»“æœæå–æ„å›¾
            intent = self.rule_recognizer.recognize(query)

            # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºå¬å›ç»“æœï¼‰
            confidence = 0.8  # åŸºç¡€åˆ†
            if recall_result.total > 0:
                top_score = recall_result.candidates[0].score
                confidence = max(confidence, top_score)

            return LayerResult(
                layer_name="L2_Semantic",
                success=True,
                intent=intent,
                confidence=confidence,
                duration=time.time() - start,
                metadata={
                    "method": recall_result.search_method,
                    "candidates_found": recall_result.total,
                    "top_score": recall_result.candidates[0].score if recall_result.candidates else 0,
                    "embedding_dim": recall_result.embedding_dim,
                    "candidates": [
                        {
                            "name": c.name,
                            "score": c.score,
                            "reason": c.match_reason
                        }
                        for c in recall_result.candidates[:3]
                    ]
                }
            )

        except Exception as e:
            return LayerResult(
                layer_name="L2_Semantic",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": str(e)}
            )

    def _layer3_llm_inference(self, query: str, candidates: list = None) -> LayerResult:
        """L3å±‚ï¼šLLMæ·±åº¦æ¨ç†.

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
            candidates: ä»L2å±‚è·å–çš„å€™é€‰æŒ‡æ ‡åˆ—è¡¨
        """
        start = time.time()

        if not self.llm_recognizer:
            return LayerResult(
                layer_name="L3_LLM",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": "LLM not configured"}
            )

        try:
            # è°ƒç”¨LLMï¼Œä¼ é€’candidatesä»¥å¸®åŠ©æ­£ç¡®è¯†åˆ«æŒ‡æ ‡
            if self.llm_provider == "zhipu":
                llm_result = self.llm_recognizer.recognize(query, candidates)
            else:
                llm_result = self.llm_recognizer.recognize(query, candidates)

            if not llm_result:
                raise Exception("LLM returned None")

            # è½¬æ¢ä¸ºQueryIntent
            intent = QueryIntent(
                query=query,
                core_query=llm_result.core_query,
                time_range=self._parse_time_range(llm_result.time_range),
                time_granularity=self._parse_granularity(llm_result.time_granularity),
                aggregation_type=self._parse_aggregation(llm_result.aggregation_type),
                dimensions=llm_result.dimensions,
                comparison_type=llm_result.comparison_type,
                filters=llm_result.filters
            )

            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "model": llm_result.model,
                "reasoning": llm_result.reasoning,
                "confidence": llm_result.confidence
            }

            if hasattr(llm_result, 'tokens_used'):
                metadata["tokens_used"] = llm_result.tokens_used

            if self.llm_provider == "zhipu":
                metadata["cost"] = self._estimate_zhipu_cost(llm_result.tokens_used)

            return LayerResult(
                layer_name=f"L3_LLM_{self.llm_provider.capitalize()}",
                success=True,
                intent=intent,
                confidence=llm_result.confidence,
                duration=llm_result.latency,
                metadata=metadata
            )

        except Exception as e:
            return LayerResult(
                layer_name="L3_LLM",
                success=False,
                intent=None,
                confidence=0.0,
                duration=time.time() - start,
                metadata={"error": str(e)}
            )

    def _calculate_rule_confidence(self, query: str, intent: QueryIntent) -> float:
        """è®¡ç®—è§„åˆ™åŒ¹é…çš„ç½®ä¿¡åº¦."""
        score = 0.5

        if intent.core_query.lower() in query.lower():
            score += 0.2

        if intent.time_range:
            score += 0.15

        if intent.aggregation_type:
            score += 0.1

        if intent.dimensions:
            score += 0.1

        if len(intent.core_query) < len(query) * 0.5:
            score += 0.1

        return min(score, 1.0)

    def _parse_time_range(self, time_range: Optional[dict]) -> Optional[tuple]:
        """è§£æLLMè¿”å›çš„æ—¶é—´èŒƒå›´."""
        if not time_range:
            return None
        # TODO: å®ç°æ—¶é—´èŒƒå›´è§£æ
        return None

    def _parse_granularity(self, granularity: Optional[str]) -> Optional[TimeGranularity]:
        """è§£ææ—¶é—´ç²’åº¦."""
        if not granularity:
            return None
        try:
            return TimeGranularity(granularity)
        except ValueError:
            return None

    def _parse_aggregation(self, aggregation: Optional[str]) -> Optional[AggregationType]:
        """è§£æèšåˆç±»å‹."""
        if not aggregation:
            return None
        try:
            return AggregationType(aggregation)
        except ValueError:
            return None

    def _estimate_zhipu_cost(self, tokens_used: dict[str, int]) -> float:
        """ä¼°ç®—æ™ºè°±AIæˆæœ¬ï¼ˆäººæ°‘å¸ï¼‰."""
        # æ™ºè°±AIä»·æ ¼: Â¥1/1M tokens (glm-4-flashå…è´¹)
        total_tokens = tokens_used.get("total_tokens", 0)
        return total_tokens / 1_000_000 * 1.0  # Â¥1/1M

    def get_statistics(self) -> dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯."""
        total = self.stats["total_queries"]

        if total == 0:
            return {"message": "æš‚æ— ç»Ÿè®¡æ•°æ®"}

        return {
            "total_queries": total,
            "layer_distribution": {
                "L1_Rule": f"{self.stats['l1_hits']/total*100:.1f}%",
                "L2_Semantic": f"{self.stats['l2_hits']/total*100:.1f}%",
                f"L3_LLM_{self.llm_provider.capitalize()}": f"{self.stats['l3_hits']/total*100:.1f}%",
            },
            "failure_rate": f"{self.stats['failures']/total*100:.1f}%",
            "llm_provider": self.llm_provider,
            "semantic_enabled": self.enable_semantic
        }


# æµ‹è¯•å‡½æ•°
def test_enhanced_hybrid():
    """æµ‹è¯•å¢å¼ºç‰ˆæ··åˆæ¶æ„."""
    print("\nğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆä¸‰å±‚æ··åˆæ¶æ„")
    print("=" * 50)

    # åˆå§‹åŒ–ï¼ˆä½¿ç”¨æ™ºè°±AIï¼‰
    recognizer = EnhancedHybridIntentRecognizer(
        llm_provider="zhipu",
        enable_semantic=True
    )

    test_queries = [
        "GMVæ˜¯ä»€ä¹ˆ",
        "æœ€è¿‘7å¤©çš„æˆäº¤é‡‘é¢",
        "æœ¬æœˆè¥æ”¶æ€»å’Œ"
    ]

    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("-" * 50)

        result = recognizer.recognize(query)

        print(f"âœ… è¯†åˆ«æˆåŠŸ")
        print(f"   æ¥æºå±‚: {result.source_layer}")
        print(f"   æ ¸å¿ƒæŸ¥è¯¢: {result.final_intent.core_query}")
        print(f"   æ€»è€—æ—¶: {result.total_duration*1000:.2f}ms")

        print(f"\n   å„å±‚ç»“æœ:")
        for layer in result.all_layers:
            if layer.success:
                print(f"      {layer.layer_name}: âœ“ ({layer.confidence:.2f}, {layer.duration*1000:.2f}ms)")
            else:
                print(f"      {layer.layer_name}: âœ—")

    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(recognizer.get_statistics())

    print("\n" + "=" * 50)


if __name__ == "__main__":
    test_enhanced_hybrid()
