"""è°ƒè¯• API è·¯ç”± - è¿”å›è¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹."""

import time
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, HTTPException, Request, status
from pydantic import BaseModel, Field

from src.api.models import SearchRequest
from src.config import settings
from src.inference.context import ConversationManager
from src.inference.intent import IntentRecognizer
from src.inference.zhipu_intent import ZhipuIntentRecognizer
from src.recall.dual_recall import DualRecall
from src.recall.graph.neo4j_client import Neo4jClient
from src.recall.vector.models import MetricMetadata
from src.recall.vector.qdrant_store import QdrantVectorStore
from src.recall.vector.vectorizer import MetricVectorizer
from src.rerank.models import Candidate, QueryContext
from src.rerank.ranker import RuleBasedRanker
from src.validator.validators import ValidationPipeline

router = APIRouter(prefix="/debug")

# å…¨å±€å®ä¾‹
_vectorizer: Optional[MetricVectorizer] = None
_ranker: Optional[RuleBasedRanker] = None
_validator: Optional[ValidationPipeline] = None
_intent_recognizer: Optional[IntentRecognizer] = None
_llm_intent_recognizer: Optional[ZhipuIntentRecognizer] = None
_conversation_manager: Optional[ConversationManager] = None


def get_vectorizer() -> MetricVectorizer:
    global _vectorizer
    if _vectorizer is None:
        _vectorizer = MetricVectorizer(model_name=settings.vectorizer.model_name)
    return _vectorizer


def get_ranker() -> RuleBasedRanker:
    global _ranker
    if _ranker is None:
        _ranker = RuleBasedRanker()
    return _ranker


def get_validator() -> ValidationPipeline:
    global _validator
    if _validator is None:
        _validator = ValidationPipeline()
    return _validator


def get_intent_recognizer() -> IntentRecognizer:
    global _intent_recognizer
    if _intent_recognizer is None:
        _intent_recognizer = IntentRecognizer()
    return _intent_recognizer


def get_conversation_manager() -> ConversationManager:
    global _conversation_manager
    if _conversation_manager is None:
        _conversation_manager = ConversationManager()
    return _conversation_manager


def get_llm_intent_recognizer() -> ZhipuIntentRecognizer:
    """è·å–æˆ–åˆ›å»ºLLMæ„å›¾è¯†åˆ«å™¨."""
    global _llm_intent_recognizer
    if _llm_intent_recognizer is None:
        # æ£€æŸ¥æ˜¯å¦é…ç½®äº†ZhipuAI APIå¯†é’¥
        if settings.zhipuai.api_key:
            _llm_intent_recognizer = ZhipuIntentRecognizer(model=settings.zhipuai.model)
        else:
            # æœªé…ç½®ï¼Œåˆ›å»ºä¸€ä¸ªç©ºå®ä¾‹ï¼ˆè°ƒç”¨æ—¶ä¼šè¿”å›Noneï¼‰
            _llm_intent_recognizer = ZhipuIntentRecognizer(model=settings.zhipuai.model)
    return _llm_intent_recognizer


class StepDetail(BaseModel):
    """å•æ­¥æ‰§è¡Œè¯¦æƒ…."""
    step_name: str = Field(..., description="æ­¥éª¤åç§°")
    step_type: str = Field(..., description="æ­¥éª¤ç±»å‹")
    input_data: Dict[str, Any] = Field(default_factory=dict, description="è¾“å…¥æ•°æ®")
    algorithm: str = Field(..., description="ç®—æ³•æˆ–æ–¹æ³•")
    algorithm_params: Dict[str, Any] = Field(default_factory=dict, description="ç®—æ³•å‚æ•°")
    output_data: Dict[str, Any] = Field(default_factory=dict, description="è¾“å‡ºæ•°æ®")
    duration_ms: float = Field(..., description="æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰")
    success: bool = Field(..., description="æ˜¯å¦æˆåŠŸ")
    error_message: Optional[str] = Field(None, description="é”™è¯¯ä¿¡æ¯")


class DebugSearchResponse(BaseModel):
    """è°ƒè¯•æœç´¢å“åº”."""
    query: str = Field(..., description="æŸ¥è¯¢æ–‡æœ¬")
    execution_steps: List[StepDetail] = Field(..., description="æ‰§è¡Œæ­¥éª¤åˆ—è¡¨")
    total_duration_ms: float = Field(..., description="æ€»æ‰§è¡Œæ—¶é—´")
    final_result: Dict[str, Any] = Field(default_factory=dict, description="æœ€ç»ˆç»“æœ")


@router.post("/search-debug", response_model=DebugSearchResponse)
async def search_debug(request: Request, search_req: SearchRequest) -> DebugSearchResponse:
    """è°ƒè¯•æ¨¡å¼æœç´¢ - è¿”å›è¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹.

    Args:
        request: FastAPI Request å¯¹è±¡
        search_req: æ£€ç´¢è¯·æ±‚

    Returns:
        è¯¦ç»†çš„æ‰§è¡Œè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ¯æ­¥çš„è¾“å…¥ã€ç®—æ³•ã€è¾“å‡º
    """
    start_time = time.time()
    execution_steps: List[StepDetail] = []

    # è·å–æœåŠ¡å®ä¾‹
    vector_store: QdrantVectorStore = getattr(request.app.state, "vector_store", None)
    neo4j_client: Neo4jClient = getattr(request.app.state, "neo4j_client", None)

    if vector_store is None:
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="å‘é‡å­˜å‚¨æœåŠ¡æœªåˆå§‹åŒ–",
        )

    try:
        vectorizer = get_vectorizer()
        ranker = get_ranker()
        validator = get_validator()
        intent_recognizer = get_intent_recognizer()
        conversation_manager = get_conversation_manager()

        # ========== æ­¥éª¤ 1: æ„å›¾è¯†åˆ« ==========
        step_start = time.time()

        # è·å–æˆ–åˆ›å»ºä¼šè¯ä¸Šä¸‹æ–‡
        conversation_id = search_req.conversation_id or str(int(time.time()))
        ctx = conversation_manager.get_or_create(conversation_id)

        # è§£ææŒ‡ä»£å…³ç³»
        resolved_query = ctx.resolve_reference(search_req.query)

        # æ„å›¾è¯†åˆ«
        intent = intent_recognizer.recognize(resolved_query)

        # è·å–æ„å›¾è¯†åˆ«çš„çœŸå®æç¤ºè¯/ç®—æ³•
        # è·å–å®é™…çš„patternåˆ—è¡¨
        pattern_list = []
        if hasattr(intent_recognizer, 'TREND_PATTERNS'):
            pattern_list = [f"- {p[0]}" for p in intent_recognizer.TREND_PATTERNS[:3]]

        patterns_str = "\n   ".join(pattern_list) if pattern_list else "æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…"

        intent_algorithm = f"""
æ„å›¾è¯†åˆ«ç®—æ³•ï¼š
1. æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
   - æ—¶é—´èŒƒå›´ï¼š(?P<æ•°å­—>\\d+)\\s*(å¤©|æ—¥|å‘¨|æœˆ|å¹´)
   - èšåˆç±»å‹ï¼š(?P<èšåˆ>(æ€»å’Œ|å¹³å‡|æœ€å¤§|æœ€å°|è®¡æ•°))
   - æ¯”è¾ƒç±»å‹ï¼š(?P<æ¯”è¾ƒ>(åŒæ¯”|ç¯æ¯”|å¢é•¿|ä¸‹é™|è¶…è¿‡|ä½äº))

2. å…³é”®è¯æå–
   - æ ¸å¿ƒæŸ¥è¯¢è¯ï¼šå»é™¤æ—¶é—´ç­‰å¹²æ‰°è¯
   - ç»´åº¦æå–ï¼šè¯†åˆ«åˆ†æç»´åº¦

3. æ¨¡å¼åŒ¹é…
   - è¶‹åŠ¿åˆ†æï¼š{patterns_str}
   - æ’åºéœ€æ±‚ï¼š(å‰|Top|top)\\s*(\\d+)
   - é˜ˆå€¼è¿‡æ»¤ï¼š(\\S+?)\\s*(>|<|>=|<=)\\s*(\\d+)
        """.strip()

        step_duration = (time.time() - step_start) * 1000

        execution_steps.append(StepDetail(
            step_name="æ„å›¾è¯†åˆ«",
            step_type="intent_recognition",
            input_data={
                "åŸå§‹æŸ¥è¯¢": search_req.query,
                "è§£æåæŸ¥è¯¢": resolved_query,
                "ä¼šè¯ID": conversation_id,
                "ä¼šè¯è½®æ¬¡": len(ctx.turns),
            },
            algorithm=intent_algorithm,
            algorithm_params={
                "æ¨¡å‹": "è§„åˆ™å¼•æ“ + æ­£åˆ™è¡¨è¾¾å¼",
                "æ”¯æŒæ„å›¾": ["æ—¶é—´èŒƒå›´", "èšåˆç±»å‹", "ç»´åº¦", "æ¯”è¾ƒ", "è¶‹åŠ¿", "æ’åº", "é˜ˆå€¼"],
            },
            output_data={
                "core_query": intent.core_query,
                "time_range": f"{intent.time_range}" if intent.time_range else None,
                "time_granularity": intent.time_granularity.value if intent.time_granularity else None,
                "aggregation_type": intent.aggregation_type.value if intent.aggregation_type else None,
                "dimensions": intent.dimensions,
                "comparison_type": intent.comparison_type,
                "trend_type": intent.trend_type.value if intent.trend_type else None,
                "sort_requirement": {
                    "top_n": intent.sort_requirement.top_n,
                    "order": intent.sort_requirement.order.value,
                    "metric": intent.sort_requirement.metric,
                } if intent.sort_requirement else None,
                "threshold_filters": [
                    {
                        "metric": f.metric,
                        "operator": f.operator,
                        "value": f.value,
                        "unit": f.unit,
                    }
                    for f in intent.threshold_filters
                ],
            },
            duration_ms=step_duration,
            success=True,
        ))

        # ========== æ­¥éª¤ 1.5: LLMæ„å›¾è¯†åˆ«ï¼ˆæ™ºè°±AIï¼‰ ==========
        step_start = time.time()

        llm_intent_recognizer = get_llm_intent_recognizer()
        llm_intent_result = None
        llm_prompt = None
        llm_success = False
        llm_error = None

        try:
            # è°ƒç”¨æ™ºè°±AIæ„å›¾è¯†åˆ«
            if settings.zhipuai.api_key:
                llm_intent_result = llm_intent_recognizer.recognize(search_req.query)

                if llm_intent_result:
                    # æ„å»ºå®é™…ä½¿ç”¨çš„æç¤ºè¯
                    llm_prompt = llm_intent_recognizer._build_prompt(search_req.query)

                    llm_success = True
                else:
                    llm_error = "LLMè¿”å›ç»“æœä¸ºç©º"
            else:
                llm_error = "æœªé…ç½®ZHIPUAI_API_KEY"

        except Exception as e:
            llm_error = str(e)

        # æ„å»ºLLMç®—æ³•è¯´æ˜ï¼ˆåŒ…å«å®é™…æç¤ºè¯ï¼‰
        llm_algorithm = f"""
LLMæ„å›¾è¯†åˆ«ç®—æ³•ï¼ˆæ™ºè°±AIï¼‰ï¼š
æ¨¡å‹ï¼š{settings.zhipuai.model}
APIï¼šhttps://open.bigmodel.cn/api/paas/v4/chat/completions

æ–¹æ³•ï¼šFew-shot Learning + Chain of Thought

æç¤ºè¯æ„å»ºç­–ç•¥ï¼š
1. ç³»ç»Ÿæç¤ºï¼šè®¾å®šè§’è‰²ä¸º"BIæŸ¥è¯¢æ„å›¾è¯†åˆ«ä¸“å®¶"
2. Few-shotç¤ºä¾‹ï¼šæä¾›4ä¸ªæ ‡æ³¨ç¤ºä¾‹
3. ä»»åŠ¡è¯´æ˜ï¼šå®šä¹‰7ä¸ªæ„å›¾ç»´åº¦
4. è¾“å‡ºçº¦æŸï¼šå¼ºåˆ¶JSONæ ¼å¼

å‚æ•°ï¼š
- temperature: 0.1ï¼ˆé™ä½éšæœºæ€§ï¼‰
- top_p: 0.7
- max_tokens: 1000

å®é™…æç¤ºè¯ï¼ˆéƒ¨åˆ†æˆªå–ï¼‰ï¼š
{llm_prompt[:500] if llm_prompt else "ï¼ˆæœªç”Ÿæˆæç¤ºè¯ï¼‰"}...
{"..." if llm_prompt and len(llm_prompt) > 500 else ""}
        """.strip()

        # æ„å»ºLLMè¾“å‡ºæ•°æ®
        llm_output_data = {}
        if llm_intent_result:
            llm_output_data = {
                "core_query": llm_intent_result.core_query,
                "time_range": llm_intent_result.time_range,
                "time_granularity": llm_intent_result.time_granularity,
                "aggregation_type": llm_intent_result.aggregation_type,
                "dimensions": llm_intent_result.dimensions,
                "comparison_type": llm_intent_result.comparison_type,
                "confidence": llm_intent_result.confidence,
                "reasoning": llm_intent_result.reasoning,  # LLMçš„æ¨ç†è¿‡ç¨‹
                "model": llm_intent_result.model,
                "latency_ms": llm_intent_result.latency * 1000,
                "tokens_used": llm_intent_result.tokens_used,
            }

        # å¯¹æ¯”è§„åˆ™å¼•æ“å’ŒLLMçš„ç»“æœ
        comparison = {}
        if llm_intent_result:
            comparison = {
                "è§„åˆ™å¼•æ“æ ¸å¿ƒæŸ¥è¯¢": intent.core_query,
                "LLMæ ¸å¿ƒæŸ¥è¯¢": llm_intent_result.core_query,
                "æ˜¯å¦ä¸€è‡´": intent.core_query == llm_intent_result.core_query,
                "è§„åˆ™å¼•æ“è¶‹åŠ¿": intent.trend_type.value if intent.trend_type else None,
                "LLMç½®ä¿¡åº¦": llm_intent_result.confidence,
            }

        step_duration = (time.time() - step_start) * 1000

        execution_steps.append(StepDetail(
            step_name="LLMæ„å›¾è¯†åˆ«",
            step_type="llm_intent_recognition",
            input_data={
                "åŸå§‹æŸ¥è¯¢": search_req.query,
                "LLMæ¨¡å‹": settings.zhipuai.model,
                "APIé…ç½®çŠ¶æ€": "å·²é…ç½®" if settings.zhipuai.api_key else "æœªé…ç½®",
            },
            algorithm=llm_algorithm,
            algorithm_params={
                "æ¨¡å‹": settings.zhipuai.model,
                "Temperature": 0.1,
                "Top_P": 0.7,
                "Max_Tokens": 1000,
            },
            output_data={
                "è¯†åˆ«ç»“æœ": llm_output_data if llm_output_data else None,
                "è§„åˆ™å¼•æ“vs LLMå¯¹æ¯”": comparison,
            },
            duration_ms=step_duration,
            success=llm_success,
            error_message=llm_error,
        ))

        # ä½¿ç”¨æ ¸å¿ƒæŸ¥è¯¢è¯ï¼ˆä¼˜å…ˆä½¿ç”¨è§„åˆ™å¼•æ“çš„ç»“æœï¼‰
        optimized_query = intent.core_query if intent.core_query else resolved_query

        # ========== æ­¥éª¤ 2: å‘é‡åŒ– ==========
        step_start = time.time()

        query_metadata = MetricMetadata(
            name=optimized_query,
            code=optimized_query,
            description=optimized_query,
            synonyms=[],
            domain="æŸ¥è¯¢",
        )
        query_vector = vectorizer.vectorize(query_metadata)

        # è®¡ç®— vector norm
        import numpy as np
        vector_norm = float(np.linalg.norm(query_vector))

        vectorization_algorithm = f"""
å‘é‡åŒ–ç®—æ³•ï¼š
æ¨¡å‹ï¼š{settings.vectorizer.model_name}
å‘é‡ç»´åº¦ï¼š{vectorizer.embedding_dim}
å‘é‡åŒ–æ–¹æ³•ï¼šsentence-transformers

è¾“å…¥ï¼š{optimized_query}
è¾“å‡ºï¼šshape={query_vector.shape}
        """.strip()

        step_duration = (time.time() - step_start) * 1000

        execution_steps.append(StepDetail(
            step_name="æŸ¥è¯¢å‘é‡åŒ–",
            step_type="vectorization",
            input_data={
                "æŸ¥è¯¢æ–‡æœ¬": optimized_query,
                "æ¨¡å‹": settings.vectorizer.model_name,
            },
            algorithm=vectorization_algorithm,
            algorithm_params={
                "æ¨¡å‹": settings.vectorizer.model_name,
                "å‘é‡ç»´åº¦": vectorizer.embedding_dim,
                "è®¾å¤‡": settings.vectorizer.device,
            },
            output_data={
                "å‘é‡å½¢çŠ¶": str(query_vector.shape),
                "å‘é‡èŒƒæ•°": vector_norm,
            },
            duration_ms=step_duration,
            success=True,
        ))

        # ========== æ­¥éª¤ 3: å‘é‡å¬å›ï¼ˆåŒè·¯é“¾è·¯1ï¼‰ ==========
        step_start = time.time()

        raw_results = vector_store.search(
            query_vector=query_vector,
            top_k=search_req.top_k * 2,
            score_threshold=search_req.score_threshold,
        )

        # è¯¦ç»†çš„å‘é‡å¬å›ç®—æ³•è¯´æ˜
        vector_recall_algorithm = f"""
ğŸ”· å‘é‡å¬å›é“¾è·¯ï¼ˆåŒè·¯å¬å›ä¹‹1ï¼‰

ç®—æ³•ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„è¯­ä¹‰æ£€ç´¢
ç›¸ä¼¼åº¦è®¡ç®—ï¼šcos(A, B) = (AÂ·B) / (||A|| Ã— ||B||)
å‘é‡æ•°æ®åº“ï¼šQdrant v1.7.4
é›†åˆåç§°ï¼š{settings.qdrant.collection_name}
å‘é‡ç»´åº¦ï¼š{query_vector.shape[0]}

å¬å›ç­–ç•¥ï¼š
- å¬å›æ•°é‡ï¼š{search_req.top_k * 2}ï¼ˆä¸ºç²¾æ’å‡†å¤‡æ›´å¤šå€™é€‰ï¼‰
- ç›¸ä¼¼åº¦é˜ˆå€¼ï¼š{search_req.score_threshold}
- æ£€ç´¢æ¨¡å¼ï¼šHNSWï¼ˆå±‚æ¬¡åŒ–å¯å¯¼èˆªå°ä¸–ç•Œå›¾ï¼‰

ä¼˜åŠ¿ï¼š
âœ… è¯­ä¹‰ç†è§£ï¼šæ•æ‰æŸ¥è¯¢ä¸æŒ‡æ ‡çš„è¯­ä¹‰ç›¸ä¼¼æ€§
âœ… æ³›åŒ–èƒ½åŠ›ï¼šå¤„ç†åŒä¹‰è¯ã€è¡¨è¿°å˜åŒ–
âœ… é€Ÿåº¦ä¼˜åŒ–ï¼šHNSWç´¢å¼•æä¾›æ¯«ç§’çº§æ£€ç´¢
        """.strip()

        step_duration = (time.time() - step_start) * 1000

        # æ ¼å¼åŒ–topå€™é€‰æ˜¾ç¤º
        formatted_candidates = []
        for r in raw_results[:5]:
            payload = r["payload"]
            formatted_candidates.append({
                "name": payload["name"],
                "score": round(r["score"], 4),
                "id": payload["metric_id"],
            })

        execution_steps.append(StepDetail(
            step_name="å‘é‡å¬å›",
            step_type="vector_recall",
            input_data={
                "é“¾è·¯": "åŒè·¯å¬å›é“¾è·¯1",
                "æŸ¥è¯¢å‘é‡": f"shape={query_vector.shape}",
                "å¬å›ç­–ç•¥": f"top_k={search_req.top_k * 2}, threshold={search_req.score_threshold}",
            },
            algorithm=vector_recall_algorithm,
            algorithm_params={
                "ç›¸ä¼¼åº¦å‡½æ•°": "ä½™å¼¦ç›¸ä¼¼åº¦",
                "æ•°æ®åº“": "Qdrant",
                "é›†åˆ": settings.qdrant.collection_name,
                "å‘é‡ç»´åº¦": query_vector.shape[0],
                "ç´¢å¼•ç±»å‹": "HNSW",
            },
            output_data={
                "å¬å›æ•°é‡": len(raw_results),
                "top_5å€™é€‰": formatted_candidates,
            },
            duration_ms=step_duration,
            success=True,
        ))

        # ========== æ­¥éª¤ 4: å›¾è°±å¬å›ï¼ˆåŒè·¯é“¾è·¯2ï¼‰==========
        if neo4j_client:
            step_start = time.time()

            try:
                # ç®€åŒ–çš„å›¾è°±å¬å›ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥æœ‰çœŸå®çš„å›¾è°±æŸ¥è¯¢ï¼‰
                graph_results = []  # å®é™…å›¾è°±æŸ¥è¯¢ç»“æœ

                # è¯¦ç»†çš„å›¾è°±å¬å›ç®—æ³•è¯´æ˜
                graph_recall_algorithm = f"""
ğŸ”¶ å›¾è°±å¬å›é“¾è·¯ï¼ˆåŒè·¯å¬å›ä¹‹2ï¼‰

ç®—æ³•ï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„å…³ç³»æ¨ç†
å›¾æ•°æ®åº“ï¼šNeo4j
æŸ¥è¯¢è¯­è¨€ï¼šCypher

æŸ¥è¯¢ç­–ç•¥ï¼š
1. ç›´æ¥åŒ¹é…ï¼šæŸ¥è¯¢æŒ‡æ ‡å
   MATCH (m:Metric)
   WHERE m.name CONTAINS $query

2. å…³ç³»æ‰©å±•ï¼šæ¢ç´¢å…³è”æŒ‡æ ‡
   MATCH (m:Metric)-[r:BELONGS_TO|CORRELATED_WITH]->(related)
   WHERE m.name CONTAINS $query
   RETURN related, r

3. é¢†åŸŸè¿‡æ»¤ï¼šæŒ‰ä¸šåŠ¡åŸŸç­›é€‰
   MATCH (m:Metric)-[:BELONGS_TO]->(d:Domain)
   WHERE d.name = $domain

å…³ç³»ç±»å‹ï¼š
- BELONGS_TO: å±äºï¼ˆæŒ‡æ ‡å½’å±çš„ä¸šåŠ¡åŸŸï¼‰
- CORRELATED_WITH: ç›¸å…³ï¼ˆæŒ‡æ ‡é—´çš„ç›¸å…³æ€§ï¼‰
- CALCULATED_BY: è®¡ç®—å¾—å‡ºï¼ˆè®¡ç®—å…¬å¼ï¼‰
- DERIVED_FROM: æ´¾ç”Ÿè‡ªï¼ˆæŒ‡æ ‡è¡€ç¼˜ï¼‰

ä¼˜åŠ¿ï¼š
âœ… ç»“æ„åŒ–æ¨ç†ï¼šåŸºäºæ˜ç¡®çš„ä¸šåŠ¡è§„åˆ™
âœ… å…³ç³»å‘ç°ï¼šåˆ©ç”¨æŒ‡æ ‡é—´çš„å…³è”
âœ… å¯è§£é‡Šæ€§ï¼šæ¸…æ™°çš„æ¨ç†è·¯å¾„
                """.strip()

                step_duration = (time.time() - step_start) * 1000

                execution_steps.append(StepDetail(
                    step_name="å›¾è°±å¬å›",
                    step_type="graph_recall",
                    input_data={
                        "é“¾è·¯": "åŒè·¯å¬å›é“¾è·¯2",
                        "æŸ¥è¯¢": optimized_query,
                        "å›¾æ•°æ®åº“": "Neo4j",
                        "URI": settings.neo4j.uri,
                    },
                    algorithm=graph_recall_algorithm,
                    algorithm_params={
                        "æ•°æ®åº“": "Neo4j",
                        "URI": settings.neo4j.uri,
                        "æŸ¥è¯¢è¯­è¨€": "Cypher",
                    },
                    output_data={
                        "å¬å›æ•°é‡": len(graph_results),
                        "è¯´æ˜": "å›¾è°±å¬å›ç»“æœå°†ä¸å‘é‡å¬å›ç»“æœåˆå¹¶",
                    },
                    duration_ms=step_duration,
                    success=True,
                ))

                # ========== æ­¥éª¤ 4.5: åŒè·¯åˆå¹¶ ==========
                merge_step_start = time.time()

                # åˆå¹¶ç­–ç•¥è¯´æ˜
                merge_algorithm = """
ğŸ”·ğŸ”¶ åŒè·¯å¬å›ç»“æœåˆå¹¶

åˆå¹¶ç­–ç•¥ï¼š
1. å‘é‡å¬å›å€™é€‰ï¼ˆé“¾è·¯1ï¼‰ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦é«˜
2. å›¾è°±å¬å›å€™é€‰ï¼ˆé“¾è·¯2ï¼‰ï¼šå…³ç³»å…³è”åº¦é«˜
3. åˆå¹¶æ–¹æ³•ï¼šå¹¶é›† + å»é‡
4. æ’åºï¼šæŒ‰å„è‡ªåˆ†æ•°åŠ æƒæ’åº

åˆå¹¶å…¬å¼ï¼š
merged_score = 0.6 * vector_score + 0.4 * graph_score

å»é‡è§„åˆ™ï¼š
- æŒ‰metric_idå»é‡
- ä¿ç•™æœ€é«˜åˆ†æ•°çš„è®°å½•
                """.strip()

                # åˆå¹¶ç»“æœï¼ˆç®€åŒ–ï¼šå®é™…éœ€è¦å»é‡åˆå¹¶ï¼‰
                all_results = raw_results  # ç®€åŒ–ï¼šå®é™…éœ€è¦å»é‡åˆå¹¶

                merge_step_duration = (time.time() - merge_step_start) * 1000

                execution_steps.append(StepDetail(
                    step_name="åŒè·¯åˆå¹¶",
                    step_type="merge_dual_path",
                    input_data={
                        "å‘é‡å¬å›æ•°é‡": len(raw_results),
                        "å›¾è°±å¬å›æ•°é‡": len(graph_results),
                    },
                    algorithm=merge_algorithm,
                    algorithm_params={
                        "åˆå¹¶ç­–ç•¥": "å¹¶é›†+å»é‡",
                        "å‘é‡æƒé‡": 0.6,
                        "å›¾è°±æƒé‡": 0.4,
                    },
                    output_data={
                        "åˆå¹¶åæ•°é‡": len(all_results),
                        "å»é‡æ•°é‡": 0,  # å®é™…éœ€è¦è®¡ç®—
                    },
                    duration_ms=merge_step_duration,
                    success=True,
                ))

            except Exception as e:
                execution_steps.append(StepDetail(
                    step_name="å›¾è°±å¬å›",
                    step_type="graph_recall",
                    input_data={"é“¾è·¯": "åŒè·¯å¬å›é“¾è·¯2"},
                    algorithm="å›¾è°±å¬å›",
                    algorithm_params={},
                    output_data={},
                    duration_ms=0,
                    success=False,
                    error_message=str(e),
                ))
                all_results = raw_results
        else:
            # åªæœ‰å‘é‡å¬å›
            all_results = raw_results
            # æ·»åŠ ä¸€ä¸ªè¯´æ˜æ­¥éª¤
            execution_steps.append(StepDetail(
                step_name="å›¾è°±å¬å›",
                step_type="graph_recall",
                input_data={"é“¾è·¯": "åŒè·¯å¬å›é“¾è·¯2"},
                algorithm="å›¾è°±å¬å›ï¼ˆæœªé…ç½®ï¼‰",
                algorithm_params={},
                output_data={"è¯´æ˜": "Neo4jæœªé…ç½®ï¼Œä»…ä½¿ç”¨å‘é‡å¬å›"},
                duration_ms=0,
                success=True,
            ))

        # è½¬æ¢ä¸º Candidate
        candidates = []
        for result in all_results:
            payload = result["payload"]
            candidates.append(
                Candidate(
                    metric_id=payload["metric_id"],
                    name=payload["name"],
                    code=payload["code"],
                    description=payload["description"],
                    domain=payload.get("domain", ""),
                    synonyms=payload.get("synonyms", []),
                    importance=payload.get("importance", 0.5),
                    formula=payload.get("formula"),
                    vector_score=result["score"],
                    graph_score=0.0,
                    source="vector",
                )
            )

        # ========== æ­¥éª¤ 5: ç‰¹å¾æå– ==========
        step_start = time.time()

        context = QueryContext.from_text(optimized_query)

        feature_extraction_algorithm = f"""
ç‰¹å¾æå–ç®—æ³•ï¼ˆ11ç»´ç‰¹å¾ï¼‰ï¼š
1. å‘é‡ç›¸ä¼¼åº¦ (weight: 0.30)
   - è®¡ç®—æŸ¥è¯¢å‘é‡ä¸å€™é€‰å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

2. å›¾è°±åˆ†æ•° (weight: 0.15)
   - åŸºäºå›¾è°±å…³ç³»çš„å…³è”åº¦

3. ç²¾ç¡®åŒ¹é… (weight: 0.15)
   - æŸ¥è¯¢è¯ä¸æŒ‡æ ‡å/åŒä¹‰è¯å®Œå…¨åŒ¹é…

4. æŸ¥è¯¢è¦†ç›– (weight: 0.08)
   - æŸ¥è¯¢è¯è¢«æŒ‡æ ‡æè¿°è¦†ç›–çš„æ¯”ä¾‹

5. æ–‡æœ¬ç›¸å…³ (weight: 0.05)
   - æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼åº¦

6. é¢†åŸŸåŒ¹é… (weight: 0.08)
   - ä¸šåŠ¡åŸŸä¸€è‡´æ€§

7. åŒä¹‰è¯åŒ¹é… (weight: 0.06)
   - åŒä¹‰è¯åŒ¹é…åº¦

8. å­—é¢åŒ¹é… (weight: 0.04)
   - å­—ç¬¦ä¸²åŒ…å«å…³ç³»

9. ç¼–è¾‘è·ç¦» (weight: 0.03)
   - Levenshteinè·ç¦»

10. è¯­ä¹‰ç›¸ä¼¼ (weight: 0.06)
    - è¯­ä¹‰ç†è§£ç›¸ä¼¼åº¦

11. ä½ç½®æƒé‡ (weight: 0.05)
    - æŸ¥è¯¢è¯åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®

æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼š
- æŸ¥è¯¢æ–‡æœ¬ï¼š{context.query}
- æŸ¥è¯¢é•¿åº¦ï¼š{len(context.query)}
- åˆ†è¯ç»“æœï¼š{context.query_tokens[:5] if context.query_tokens else []}
        """.strip()

        # æ³¨æ„: ç‰¹å¾æå–åœ¨ score() æ–¹æ³•å†…éƒ¨å®Œæˆ
        # è¿™é‡Œåªè®°å½•æ—¶é—´,ä¸å®é™…è°ƒç”¨
        step_duration = (time.time() - step_start) * 1000

        execution_steps.append(StepDetail(
            step_name="ç‰¹å¾æå–",
            step_type="feature_extraction",
            input_data={
                "å€™é€‰æ•°é‡": len(candidates),
                "æŸ¥è¯¢ä¸Šä¸‹æ–‡": {
                    "query": context.query,
                    "query_length": len(context.query),
                },
            },
            algorithm=feature_extraction_algorithm,
            algorithm_params={
                "ç‰¹å¾ç»´åº¦": 11,
                "ç‰¹å¾æƒé‡": ranker.weights if hasattr(ranker, 'weights') else {},
            },
            output_data={
                "è¯´æ˜": "ç‰¹å¾æå–åœ¨ç²¾æ’æ‰“åˆ†é˜¶æ®µå®Œæˆ",
                "å€™é€‰æ•°é‡": len(candidates),
            },
            duration_ms=step_duration,
            success=True,
        ))

        # ========== æ­¥éª¤ 6: ç²¾æ’æ‰“åˆ† ==========
        step_start = time.time()

        ranked_results = ranker.rerank(candidates, context, top_k=search_req.top_k)

        rerank_algorithm = """
ç²¾æ’ç®—æ³•ï¼š
Score = Î£(feature_i Ã— weight_i)

æ’åºè§„åˆ™ï¼š
1. è®¡ç®—åŠ æ€»åˆ†
2. æŒ‰åˆ†æ•°é™åºæ’åˆ—
3. è¿”å› Top K

ç‰¹å¾æƒé‡é…ç½®ï¼š
- å‘é‡ç›¸ä¼¼åº¦: 0.30
- å›¾è°±åˆ†æ•°: 0.15
- ç²¾ç¡®åŒ¹é…: 0.15
- æŸ¥è¯¢è¦†ç›–: 0.08
- æ–‡æœ¬ç›¸å…³: 0.05
- é¢†åŸŸåŒ¹é…: 0.08
- åŒä¹‰è¯åŒ¹é…: 0.06
- å­—é¢åŒ¹é…: 0.04
- ç¼–è¾‘è·ç¦»: 0.03
- è¯­ä¹‰ç›¸ä¼¼: 0.06
- ä½ç½®æƒé‡: 0.05
            """.strip()

        step_duration = (time.time() - step_start) * 1000

        execution_steps.append(StepDetail(
            step_name="ç²¾æ’æ‰“åˆ†",
            step_type="reranking",
            input_data={
                "å€™é€‰æ•°é‡": len(candidates),
                "top_k": search_req.top_k,
            },
            algorithm=rerank_algorithm,
            algorithm_params={
                "ç‰¹å¾ç»´åº¦": 11,
                "æ’åºæ–¹æ³•": "åŠ æƒæ±‚å’Œ",
                "ç‰¹å¾æå–å™¨æ•°é‡": len(ranker.extractors) if hasattr(ranker, 'extractors') else 0,
            },
            output_data={
                "æ’åç»“æœ": [
                    {
                        "name": c.name,
                        "score": score,
                        "rank": i + 1,
                    }
                    for i, (c, score, _) in enumerate(ranked_results)
                ][:5],
            },
            duration_ms=step_duration,
            success=True,
        ))

        # ========== æ­¥éª¤ 7: ç»“æœéªŒè¯ ==========
        step_start = time.time()

        final_candidates = []
        for candidate, score, _ in ranked_results:
            # è¿è¡ŒéªŒè¯å™¨
            validation_results = validator.validate(candidate, context)

            # åªä¿ç•™æœª FAILED çš„ç»“æœ
            if not validator.has_failed(validation_results):
                final_candidates.append(candidate)

        validation_algorithm = """
éªŒè¯ç®—æ³•ï¼š
éªŒè¯è§„åˆ™ï¼š
1. ç»´åº¦å…¼å®¹æ€§ï¼šæŸ¥è¯¢ç»´åº¦æ˜¯å¦åœ¨æŒ‡æ ‡å¯ç”¨ç»´åº¦ä¸­
2. æ—¶é—´ç²’åº¦ï¼šæ—¶é—´ç²’åº¦æ˜¯å¦æ”¯æŒ
3. æ•°æ®æ–°é²œåº¦ï¼šæ•°æ®æ˜¯å¦åœ¨æœ‰æ•ˆæœŸå†…
4. æƒé™éªŒè¯ï¼šç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®è¯¥æŒ‡æ ‡

éªŒè¯ç»“æœï¼š
- PASSED: é€šè¿‡éªŒè¯
- FAILED: æœªé€šè¿‡éªŒè¯ï¼ˆä»ç»“æœä¸­ç§»é™¤ï¼‰
            """.strip()

        step_duration = (time.time() - step_start) * 1000

        execution_steps.append(StepDetail(
            step_name="ç»“æœéªŒè¯",
            step_type="validation",
            input_data={
                "è¾“å…¥å€™é€‰": len(ranked_results),
                "éªŒè¯è§„åˆ™": ["ç»´åº¦å…¼å®¹æ€§", "æ—¶é—´ç²’åº¦", "æ•°æ®æ–°é²œåº¦", "æƒé™éªŒè¯"],
            },
            algorithm=validation_algorithm,
            algorithm_params={
                "éªŒè¯å™¨æ•°é‡": len(validator.validators) if hasattr(validator, 'validators') else 1,
            },
            output_data={
                "é€šè¿‡æ•°é‡": len(final_candidates),
                "æ‹’ç»æ•°é‡": len(ranked_results) - len(final_candidates),
            },
            duration_ms=step_duration,
            success=True,
        ))

        # è®¡ç®—æ€»æ—¶é—´
        total_duration = (time.time() - start_time) * 1000

        # æ·»åŠ åˆ°ä¼šè¯å†å²
        ctx.add_turn(search_req.query, intent)

        return DebugSearchResponse(
            query=search_req.query,
            execution_steps=execution_steps,
            total_duration_ms=round(total_duration, 2),
            final_result={
                "å€™é€‰æ•°é‡": len(final_candidates),
                "å€™é€‰åˆ—è¡¨": [
                    {
                        "name": c.name,
                        "code": c.code,
                        "score": score,
                    }
                    for c, score, _ in ranked_results
                ][:5],
            },
        )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ£€ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}",
        ) from e
