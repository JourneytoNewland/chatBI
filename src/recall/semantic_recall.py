"""L2å±‚è¯­ä¹‰å‘é‡å¬å›æ¨¡å—."""

import time
from dataclasses import dataclass
from typing import Any, Optional

from ..embedding.bge_embedding import BGEEmbeddingModel, get_bge_model
from ..recall.vector.qdrant_store import QdrantVectorStore


@dataclass
class SemanticSearchResult:
    """è¯­ä¹‰æœç´¢ç»“æœ."""

    metric_id: str
    name: str
    score: float
    metadata: dict[str, Any]
    match_reason: str


@dataclass
class SemanticRecallResult:
    """è¯­ä¹‰å¬å›ç»“æœ."""

    query: str
    core_query: str
    candidates: list[SemanticSearchResult]
    total: int
    search_method: str
    latency: float
    embedding_dim: int


class SemanticRecall:
    """è¯­ä¹‰å‘é‡å¬å›å™¨ï¼ˆL2å±‚ï¼‰.

    åŠŸèƒ½:
    - ä½¿ç”¨BGE-M3ç¼–ç æŸ¥è¯¢
    - Qdrantå‘é‡æ£€ç´¢
    - Top-Kç›¸ä¼¼åº¦æ’åº

    æ€§èƒ½:
    - å»¶è¿Ÿ: ~50ms
    - å¬å›ç‡: ~85%
    - æˆæœ¬: æœ¬åœ°å…è´¹
    """

    def __init__(
        self,
        embedding_model: Optional[BGEEmbeddingModel] = None,
        qdrant_store: Optional[QdrantVectorStore] = None
    ):
        """åˆå§‹åŒ–è¯­ä¹‰å¬å›å™¨.

        Args:
            embedding_model: åµŒå…¥æ¨¡å‹
            qdrant_store: Qdrantå­˜å‚¨
        """
        self.embedding_model = embedding_model or get_bge_model()
        self.qdrant_store = qdrant_store or QdrantVectorStore()

        # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
        self.available = self.embedding_model.is_available()

        if not self.available:
            print("âš ï¸  BGEæ¨¡å‹ä¸å¯ç”¨ï¼Œè¯­ä¹‰å¬å›åŠŸèƒ½å°†è¢«ç¦ç”¨")

    def recall(
        self,
        query: str,
        top_k: int = 10,
        score_threshold: float = 0.6
    ) -> Optional[SemanticRecallResult]:
        """æ‰§è¡Œè¯­ä¹‰å¬å›.

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            å¬å›ç»“æœ
        """
        if not self.available:
            return None

        start = time.time()

        try:
            # 1. ç¼–ç æŸ¥è¯¢
            query_vector = self.embedding_model.encode_query(query)
            embedding_time = time.time() - start

            # 2. å‘é‡æ£€ç´¢
            search_start = time.time()
            qdrant_results = self.qdrant_store.search(
                query_vector=query_vector,
                top_k=top_k,
                score_threshold=score_threshold
            )
            search_time = time.time() - search_start

            # 3. æ ¼å¼åŒ–ç»“æœ
            candidates = []
            for result in qdrant_results:
                candidates.append(SemanticSearchResult(
                    metric_id=result["payload"].get("metric_id", result["id"]),
                    name=result["payload"].get("name", ""),
                    score=result["score"],
                    metadata=result["payload"],
                    match_reason=f"è¯­ä¹‰ç›¸ä¼¼åº¦ {result['score']:.3f}"
                ))

            return SemanticRecallResult(
                query=query,
                core_query=query,
                candidates=candidates,
                total=len(candidates),
                search_method="BGE-M3 + Qdrant",
                latency=time.time() - start,
                embedding_dim=len(query_vector)
            )

        except Exception as e:
            print(f"âŒ è¯­ä¹‰å¬å›å¤±è´¥: {e}")
            return None

    def batch_encode_metrics(self, metrics: list[dict[str, Any]]) -> int:
        """æ‰¹é‡ç¼–ç æŒ‡æ ‡å¹¶å¯¼å…¥Qdrant.

        Args:
            metrics: æŒ‡æ ‡åˆ—è¡¨

        Returns:
            æˆåŠŸå¯¼å…¥çš„æ•°é‡
        """
        if not self.available:
            print("âŒ BGEæ¨¡å‹ä¸å¯ç”¨")
            return 0

        print(f"ğŸ“¦ æ‰¹é‡ç¼–ç  {len(metrics)} ä¸ªæŒ‡æ ‡...")

        # 1. ç¼–ç 
        texts = [
            f"{m['name']} {m.get('description', '')} {' '.join(m.get('synonyms', []))}"
            for m in metrics
        ]

        embeddings = self.embedding_model.encode(texts, show_progress=True)

        # 2. å¯¼å…¥Qdrant
        ids = [m["metric_id"] for m in metrics]
        payloads = metrics

        count = self.qdrant_store.upsert(
            ids=ids,
            vectors=embeddings,
            payloads=payloads,
            batch_size=64
        )

        print(f"âœ… æˆåŠŸå¯¼å…¥ {count} ä¸ªæŒ‡æ ‡å‘é‡")

        return count


# å¸¦å…œåº•çš„è¯­ä¹‰å¬å›ï¼ˆä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼‰
class FallbackSemanticRecall:
    """å…œåº•è¯­ä¹‰å¬å›å™¨ï¼ˆåŸºäºåŒä¹‰è¯åŒ¹é…ï¼‰.

    åœ¨Qdrantä¸å¯ç”¨æ—¶ä½¿ç”¨
    """

    def __init__(self, mock_metrics: list[dict] = None):
        """åˆå§‹åŒ–å…œåº•å¬å›å™¨.

        Args:
            mock_metrics: æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®
        """
        from .llm_intent import MOCK_METRICS
        self.metrics = mock_metrics or MOCK_METRICS

    def recall(
        self,
        query: str,
        top_k: int = 10,
        score_threshold: float = 0.5
    ) -> SemanticRecallResult:
        """æ‰§è¡Œå…œåº•å¬å›."""
        import re

        start = time.time()

        # æ¸…ç†æŸ¥è¯¢
        query_clean = re.sub(r'^[çš„çš„ä¹‹ä¹‹]+', '', query.lower().strip())
        query_clean = re.sub(r'[çš„çš„ä¹‹ä¹‹]+$', '', query_clean)

        # ç®€å•åŒ¹é…ç®—æ³•
        candidates = []
        for metric in self.metrics:
            score = 0.0

            # ç²¾ç¡®åŒ¹é…åç§°
            if query_clean == metric["name"].lower():
                score = 1.0
            # ç²¾ç¡®åŒ¹é…åŒä¹‰è¯
            elif any(query_clean == syn.lower() for syn in metric["synonyms"]):
                score = 0.98
            # åŒ…å«åŒ¹é…
            elif query_clean in metric["name"].lower():
                score = 0.85
            elif query_clean in metric["description"].lower():
                score = 0.75
            # åŒä¹‰è¯åŒ…å«
            elif any(query_clean in syn.lower() for syn in metric["synonyms"]):
                score = 0.80

            if score >= score_threshold:
                candidates.append(SemanticSearchResult(
                    metric_id=metric["metric_id"],
                    name=metric["name"],
                    score=score,
                    metadata=metric,
                    match_reason=f"è§„åˆ™åŒ¹é… {score:.2f}"
                ))

        # æ’åº
        candidates.sort(key=lambda x: x.score, reverse=True)
        candidates = candidates[:top_k]

        return SemanticRecallResult(
            query=query,
            core_query=query_clean,
            candidates=candidates,
            total=len(candidates),
            search_method="è§„åˆ™å…œåº•ï¼ˆåŒä¹‰è¯åŒ¹é…ï¼‰",
            latency=time.time() - start,
            embedding_dim=0
        )


# æµ‹è¯•å‡½æ•°
def test_semantic_recall():
    """æµ‹è¯•è¯­ä¹‰å¬å›."""
    print("\nğŸ§ª æµ‹è¯•è¯­ä¹‰å‘é‡å¬å›")
    print("=" * 50)

    # æµ‹è¯•å…œåº•å¬å›å™¨
    fallback = FallbackSemanticRecall()

    test_queries = [
        "GMV",
        "æˆäº¤é‡‘é¢",
        "æ—¥æ´»ç”¨æˆ·",
        "æœ¬æœˆè¥æ”¶"
    ]

    for query in test_queries:
        print(f"\næŸ¥è¯¢: {query}")
        print("-" * 50)

        result = fallback.recall(query, top_k=3)

        print(f"âœ… å¬å›æˆåŠŸ: {result.total} ä¸ªç»“æœ")
        print(f"   è€—æ—¶: {result.latency*1000:.2f}ms")
        print(f"   æ–¹æ³•: {result.search_method}")

        for i, cand in enumerate(result.candidates, 1):
            print(f"   {i}. {cand.name} (åˆ†æ•°: {cand.score:.3f})")

    print("\n" + "=" * 50)


if __name__ == "__main__":
    test_semantic_recall()
