"""ç”Ÿäº§çº§æ„å›¾è¯†åˆ«æœåŠ¡å™¨ï¼ˆé›†æˆæ™ºè°±AIï¼‰."""

import os
import sys
sys.path.insert(0, "/Users/wangzheng/Downloads/playDemo/AntigravityDemo/chatBI")

from datetime import datetime
from typing import Any, Optional, List
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# å¯¼å…¥å¢å¼ºç‰ˆæ··åˆè¯†åˆ«å™¨
from src.inference.enhanced_hybrid import EnhancedHybridIntentRecognizer
from src.inference.intent import QueryIntent
from src.recall.semantic_recall import FallbackSemanticRecall

app = FastAPI(
    title="æ™ºèƒ½é—®æ•°ç³»ç»Ÿ - ç”Ÿäº§ç‰ˆ",
    version="2.0",
    description="åŸºäºæ™ºè°±AI + BGE-M3çš„ä¼ä¸šçº§æ„å›¾è¯†åˆ«ç³»ç»Ÿ"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åˆå§‹åŒ–æ··åˆè¯†åˆ«å™¨ï¼ˆä½¿ç”¨æ™ºè°±AIï¼‰
print("\nğŸš€ åˆå§‹åŒ–æ„å›¾è¯†åˆ«ç³»ç»Ÿ...")
print("=" * 60)

recognizer = EnhancedHybridIntentRecognizer(
    llm_provider="zhipu",  # ä½¿ç”¨æ™ºè°±AI
    enable_semantic=True   # å¯ç”¨è¯­ä¹‰å‘é‡æ£€ç´¢
)

print(f"âœ… æ··åˆè¯†åˆ«å™¨åˆå§‹åŒ–å®Œæˆ")
print(f"   LLMæä¾›å•†: æ™ºè°±AI (GLM-4-Flash)")
print(f"   è¯­ä¹‰æ£€ç´¢: å¯ç”¨")
print(f"   æ¶æ„: ä¸‰å±‚æ··åˆ (è§„åˆ™ â†’ è¯­ä¹‰ â†’ LLM)")
print("=" * 60 + "\n")

# æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®
MOCK_METRICS = [
    {
        "metric_id": "m001",
        "name": "GMV",
        "code": "gmv",
        "description": "æˆäº¤æ€»é¢ï¼ˆGross Merchandise Volumeï¼‰",
        "domain": "ç”µå•†",
        "synonyms": ["æˆäº¤é‡‘é¢", "äº¤æ˜“é¢", "æˆäº¤æ€»é¢", "é”€å”®é¢"],
        "formula": "SUM(order_amount)",
    },
    {
        "metric_id": "m002",
        "name": "DAU",
        "code": "dau",
        "description": "æ—¥æ´»è·ƒç”¨æˆ·æ•°ï¼ˆDaily Active Usersï¼‰",
        "domain": "ç”¨æˆ·",
        "synonyms": ["æ—¥æ´»", "æ—¥æ´»è·ƒç”¨æˆ·", "æ¯æ—¥æ´»è·ƒç”¨æˆ·"],
        "formula": "COUNT(active_users WHERE date = current_date)",
    },
    {
        "metric_id": "m003",
        "name": "MAU",
        "code": "mau",
        "description": "æœˆæ´»è·ƒç”¨æˆ·æ•°ï¼ˆMonthly Active Usersï¼‰",
        "domain": "ç”¨æˆ·",
        "synonyms": ["æœˆæ´»", "æœˆæ´»è·ƒç”¨æˆ·", "æ¯æœˆæ´»è·ƒç”¨æˆ·"],
        "formula": "COUNT(active_users WHERE month = current_month)",
    },
    {
        "metric_id": "m004",
        "name": "ARPU",
        "code": "arpu",
        "description": "å¹³å‡æ¯ç”¨æˆ·æ”¶å…¥ï¼ˆAverage Revenue Per Userï¼‰",
        "domain": "è¥æ”¶",
        "synonyms": ["äººå‡æ”¶å…¥", "æ¯ç”¨æˆ·å¹³å‡æ”¶å…¥"],
        "formula": "SUM(revenue) / COUNT(users)",
    },
    {
        "metric_id": "m005",
        "name": "è½¬åŒ–ç‡",
        "code": "conversion_rate",
        "description": "è®¿å®¢è½¬åŒ–ä¸ºç”¨æˆ·çš„æ¯”ä¾‹",
        "domain": "è¥é”€",
        "synonyms": ["è½¬åŒ–æ¯”ç‡", "è®¿é—®è½¬åŒ–ç‡"],
        "formula": "COUNT(conversions) / COUNT(visitors)",
    },
]

# è¯·æ±‚/å“åº”æ¨¡å‹
class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500, description="æŸ¥è¯¢æ–‡æœ¬")
    top_k: int = Field(default=10, ge=1, le=100, description="è¿”å›ç»“æœæ•°é‡")


class MetricCandidate(BaseModel):
    metric_id: str
    name: str
    code: str
    description: str
    domain: str
    score: float
    synonyms: List[str]
    formula: Optional[str]


class SearchResponse(BaseModel):
    query: str
    intent: dict
    candidates: List[MetricCandidate]
    total: int
    source_layer: str
    latency_ms: float


@app.get("/")
async def root():
    """æ ¹è·¯å¾„."""
    return {
        "service": "æ™ºèƒ½é—®æ•°ç³»ç»Ÿ",
        "version": "2.0 (Production)",
        "features": {
            "llm_provider": "æ™ºè°±AI GLM-4-Flash",
            "architecture": "ä¸‰å±‚æ··åˆæ¶æ„",
            "semantic_search": "BGE-M3 + Qdrant",
            "intent_dimensions": "7ç»´è¯†åˆ«"
        },
        "docs": "/docs",
        "visualization": "æ‰“å¼€ frontend/intent-visualization.html"
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥."""
    return {
        "status": "healthy",
        "service": "intent-recognition-production",
        "llm_provider": "zhipu",
        "model": "glm-4-flash"
    }


@app.post("/api/v1/search", response_model=SearchResponse)
async def search_metrics(request: SearchRequest) -> SearchResponse:
    """æ™ºèƒ½æ£€ç´¢æŒ‡æ ‡ï¼ˆä¸‰å±‚æ··åˆæ¶æ„ï¼‰."""

    import time
    start = time.time()

    # 1. æ„å›¾è¯†åˆ«ï¼ˆä¸‰å±‚æ··åˆï¼‰
    result = recognizer.recognize(request.query, top_k=request.top_k)

    # 2. ä½¿ç”¨æ ¸å¿ƒæŸ¥è¯¢è¿›è¡ŒåŒ¹é…
    core_query = result.final_intent.core_query or request.query
    core_query = core_query.strip()

    # 3. æŒ‡æ ‡åŒ¹é…
    candidates = []
    for metric in MOCK_METRICS:
        score = calculate_similarity(core_query, metric)
        if score > 0:
            candidates.append({**metric, "score": score})

    candidates.sort(key=lambda x: x["score"], reverse=True)
    candidates = candidates[:request.top_k]

    # 4. æ ¼å¼åŒ–ç»“æœ
    formatted_candidates = [
        MetricCandidate(
            metric_id=c["metric_id"],
            name=c["name"],
            code=c["code"],
            description=c["description"],
            domain=c["domain"],
            score=c["score"],
            synonyms=c["synonyms"],
            formula=c.get("formula")
        )
        for c in candidates
    ]

    # 5. æ ¼å¼åŒ–æ„å›¾
    intent = {
        "core_query": result.final_intent.core_query,
        "time_range": format_time_range(result.final_intent.time_range),
        "time_granularity": format_enum(result.final_intent.time_granularity),
        "aggregation_type": format_enum(result.final_intent.aggregation_type),
        "dimensions": result.final_intent.dimensions,
        "comparison_type": result.final_intent.comparison_type,
        "filters": result.final_intent.filters
    }

    latency = time.time() - start

    return SearchResponse(
        query=request.query,
        intent=intent,
        candidates=formatted_candidates,
        total=len(formatted_candidates),
        source_layer=result.source_layer,
        latency_ms=round(latency * 1000, 2)
    )


@app.post("/api/v1/debug/intent-visualization")
async def debug_intent_visualization(request: SearchRequest):
    """æ„å›¾è¯†åˆ«å¯è§†åŒ–è°ƒè¯•æ¥å£."""

    import time
    start = time.time()

    # æ‰§è¡Œæ··åˆè¯†åˆ«
    result = recognizer.recognize(request.query, top_k=request.top_k)

    # æ„å»ºå¯è§†åŒ–æ•°æ®
    return {
        "query_info": {
            "original_query": request.query,
            "query_length": len(request.query),
            "core_query": result.final_intent.core_query
        },

        "recognition_timeline": [
            {
                "layer": layer.layer_name,
                "success": layer.success,
                "confidence": layer.confidence,
                "duration_ms": round(layer.duration * 1000, 2),
                "metadata": layer.metadata
            }
            for layer in result.all_layers
        ],

        "final_intent": {
            "core_query": result.final_intent.core_query,
            "time_range": format_time_range(result.final_intent.time_range),
            "time_granularity": format_enum(result.final_intent.time_granularity),
            "aggregation_type": format_enum(result.final_intent.aggregation_type),
            "dimensions": result.final_intent.dimensions,
            "comparison_type": result.final_intent.comparison_type,
            "filters": result.final_intent.filters
        },

        "performance": {
            "total_duration_ms": round(result.total_duration * 1000, 2),
            "source_layer": result.source_layer,
            "layer_breakdown": {
                layer.layer_name: round(layer.duration * 1000, 2)
                for layer in result.all_layers
            }
        },

        "confidence_heatmap": [
            {
                "layer": layer.layer_name,
                "confidence": layer.confidence,
                "status": "âœ“" if layer.success else "âœ—"
            }
            for layer in result.all_layers
        ],

        "llm_reasoning": extract_llm_reasoning(result)
    }


@app.get("/api/v1/statistics")
async def get_statistics():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯."""
    return recognizer.get_statistics()


# è¾…åŠ©å‡½æ•°
def calculate_similarity(query: str, metric: dict) -> float:
    """è®¡ç®—æŸ¥è¯¢ä¸æŒ‡æ ‡çš„ç›¸ä¼¼åº¦."""
    import re
    query_clean = re.sub(r'^[çš„çš„ä¹‹ä¹‹]+', '', query.lower().strip())
    query_clean = re.sub(r'[çš„çš„ä¹‹ä¹‹]+$', '', query_clean)

    if query_clean == metric["name"].lower():
        return 1.0
    elif any(query_clean == syn.lower() for syn in metric["synonyms"]):
        return 0.98
    elif query_clean in metric["name"].lower():
        return 0.85
    elif query_clean in metric["description"].lower():
        return 0.75
    elif any(query_clean in syn.lower() for syn in metric["synonyms"]):
        return 0.80
    return 0.0


def format_time_range(time_range: Optional[tuple]) -> Optional[dict]:
    """æ ¼å¼åŒ–æ—¶é—´èŒƒå›´."""
    if not time_range:
        return None
    start, end = time_range
    return {
        "start": start.strftime("%Y-%m-%d") if hasattr(start, "strftime") else str(start),
        "end": end.strftime("%Y-%m-%d") if hasattr(end, "strftime") else str(end)
    }


def format_enum(value) -> Optional[str]:
    """æ ¼å¼åŒ–æšä¸¾å€¼."""
    if value is None:
        return None
    return value.value if hasattr(value, "value") else str(value)


def extract_llm_reasoning(result) -> Optional[dict]:
    """æå–LLMæ¨ç†è¿‡ç¨‹."""
    for layer in result.all_layers:
        if "LLM" in layer.layer_name and layer.metadata.get("reasoning"):
            return {
                "model": layer.metadata.get("model"),
                "reasoning": layer.metadata.get("reasoning"),
                "tokens_used": layer.metadata.get("tokens_used"),
                "cost": layer.metadata.get("cost")
            }
    return None


if __name__ == "__main__":
    print("\nğŸ¯ æ™ºèƒ½é—®æ•°ç³»ç»Ÿ v2.0 - ç”Ÿäº§ç‰ˆ")
    print("=" * 60)
    print("æœåŠ¡åœ°å€: http://localhost:8000")
    print("API æ–‡æ¡£: http://localhost:8000/docs")
    print("å¯è§†åŒ–ç•Œé¢: æ‰“å¼€ frontend/intent-visualization.html")
    print("=" * 60)
    print("\næ ¸å¿ƒç‰¹æ€§:")
    print("  âœ… æ™ºè°±AI GLM-4 Flash (Â¥1/1M tokens)")
    print("  âœ… ä¸‰å±‚æ··åˆæ¶æ„ (è§„åˆ™ â†’ è¯­ä¹‰ â†’ LLM)")
    print("  âœ… 7ç»´æ„å›¾è¯†åˆ« (æ—¶é—´/èšåˆ/ç»´åº¦/æ¯”è¾ƒ/è¿‡æ»¤)")
    print("  âœ… å®æ—¶å¯è§†åŒ–è°ƒè¯•")
    print("  âœ… 10+ æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡\n")

    uvicorn.run(app, host="0.0.0.0", port=8000)
