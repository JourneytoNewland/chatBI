# ã€æ–¹æ¡ˆä¸‰éš¾ç‚¹ã€‘ç»“åˆè¯­ä¹‰å±‚çš„æ™ºèƒ½é—®æ•°æ··åˆæ–¹æ¡ˆæ ¸å¿ƒæŠ€æœ¯æ·±åº¦è®¾è®¡ä¸MVPå®ç°


> **æ–‡æ¡£ç›®æ ‡**ï¼š æ·±å…¥è®¾è®¡æ··åˆæ–¹æ¡ˆä¸­æœ€éš¾ã€æœ€æœ‰æ·±åº¦ã€æœ€éœ€è¦èŠ±ç²¾åŠ›çš„æ ¸å¿ƒæŠ€æœ¯ç‚¹ï¼Œå¹¶æä¾›å¯æµ‹è¯•çš„ MVP ä»£ç å®ç°

---





## ä¸€ã€æ ¸å¿ƒæŠ€æœ¯éš¾ç‚¹è¯†åˆ«

ç»è¿‡æ·±åº¦åˆ†æï¼Œæ··åˆæ–¹æ¡ˆï¼ˆå‘é‡åº“+å›¾è°±ï¼‰ä¸­**æœ€å…·æŒ‘æˆ˜æ€§çš„ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯**æ˜¯ï¼š

### 1.1 æŠ€æœ¯éš¾ç‚¹æ’åº

| æ’å    | æŠ€æœ¯æ¨¡å—             | éš¾åº¦  | æ ¸å¿ƒæŒ‘æˆ˜                               | ä¸šåŠ¡ä»·å€¼             |
| ------- | -------------------- | ----- | -------------------------------------- | -------------------- |
| ğŸ¥‡ **1** | **è¯­ä¹‰èåˆä¸ç²¾æ’å±‚** | â­â­â­â­â­ | å¼‚æ„æ•°æ®èåˆã€å¤šç‰¹å¾å»ºæ¨¡ã€æ’åºæ¨¡å‹è®­ç»ƒ | ç›´æ¥å†³å®šæœ€ç»ˆç»“æœè´¨é‡ |
| ğŸ¥ˆ **2** | **è¯­ä¹‰æ¨ç†å¼•æ“**     | â­â­â­â­â­ | å›¾è°±æ¨ç†ç®—æ³•ã€å› æœé“¾åˆ†æã€æ¨ç†è·¯å¾„è§£é‡Š | æ”¯æ’‘é«˜çº§åˆ†æèƒ½åŠ›     |
| ğŸ¥‰ **3** | **æœ¬ä½“å›¾è°±éªŒè¯å™¨**   | â­â­â­â­  | ä¸šåŠ¡è§„åˆ™å¼•æ“ã€çº¦æŸéªŒè¯ã€å†²çªæ£€æµ‹       | ä¿è¯ç»“æœåˆæ³•æ€§       |

### 1.2 ä¸ºä»€ä¹ˆè¿™ä¸‰ä¸ªæ¨¡å—æœ€éš¾ï¼Ÿ

**è¯­ä¹‰èåˆä¸ç²¾æ’å±‚**ï¼š

- âŒ **å¼‚æ„æ•°æ®èåˆ**ï¼š å‘é‡ç›¸ä¼¼åº¦ï¼ˆ0-1 è¿ç»­å€¼ï¼‰ vs å›¾è°±åŒ¹é…ç±»å‹ï¼ˆç¦»æ•£æšä¸¾ï¼‰

- âŒ **ç‰¹å¾å·¥ç¨‹**ï¼š éœ€è¦è®¾è®¡ 10+ä¸ªç‰¹å¾ï¼Œæƒé‡å¦‚ä½•åˆ†é…ï¼Ÿ

- âŒ **å†·å¯åŠ¨é—®é¢˜**ï¼š åˆæœŸæ²¡æœ‰æ ‡æ³¨æ•°æ®ï¼Œå¦‚ä½•è®­ç»ƒæ’åºæ¨¡å‹ï¼Ÿ

- âŒ **å®æ—¶æ€§è¦æ±‚**ï¼š èåˆæ’åºå¿…é¡»åœ¨ 15ms å†…å®Œæˆ

**è¯­ä¹‰æ¨ç†å¼•æ“**ï¼š

- âŒ **æ¨ç†ç®—æ³•å¤æ‚**ï¼š ä¼ é€’æ€§æ¨ç†ã€å› æœé“¾åˆ†æã€å¤šè·³å…³ç³»éå†

- âŒ **æ¨ç†è·¯å¾„çˆ†ç‚¸**ï¼š å›¾è°±ä¸­å¯èƒ½å­˜åœ¨å¤§é‡æ¨ç†è·¯å¾„ï¼Œå¦‚ä½•å‰ªæï¼Ÿ

- âŒ **å¯è§£é‡Šæ€§**ï¼š æ¨ç†ç»“æœå¿…é¡»å¯è¿½æº¯ã€å¯è§£é‡Š

- âŒ **æ€§èƒ½æŒ‘æˆ˜**ï¼š å¤æ‚æ¨ç†å¯èƒ½è€—æ—¶ 100ms+

**æœ¬ä½“å›¾è°±éªŒè¯å™¨**ï¼š

- âŒ **ä¸šåŠ¡è§„åˆ™å¤æ‚**ï¼š ç»´åº¦å…¼å®¹æ€§ã€æ—¶é—´ç²’åº¦ã€æ•°æ®æƒé™ã€æŸ¥è¯¢åˆç†æ€§

- âŒ **è§„åˆ™å†²çª**ï¼š å¤šæ¡è§„åˆ™å¯èƒ½äº§ç”Ÿå†²çªï¼Œå¦‚ä½•åè°ƒï¼Ÿ

- âŒ **åŠ¨æ€è§„åˆ™**ï¼š ä¸šåŠ¡è§„åˆ™ä¼šé¢‘ç¹å˜åŒ–ï¼Œå¦‚ä½•æ”¯æŒçƒ­æ›´æ–°ï¼Ÿ

---

## äºŒã€æ ¸å¿ƒæŠ€æœ¯ 1: è¯­ä¹‰èåˆä¸ç²¾æ’å±‚ (Rerank Engine)

### 2.1 æŠ€æœ¯æ¶æ„

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è¯­ä¹‰èåˆä¸ç²¾æ’å±‚ (Rerank Engine)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  è¾“å…¥: å‘é‡å¬å›Top-50 + å›¾è°±å¬å›Top-30                    â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Step 1: å€™é€‰é›†åˆå¹¶ä¸å»é‡                       â”‚    â”‚
â”‚  â”‚  - åˆå¹¶ä¸¤è·¯å¬å›ç»“æœ                             â”‚    â”‚
â”‚  â”‚  - å»é‡(metricIdç›¸åŒè§†ä¸ºé‡å¤)                   â”‚    â”‚
â”‚  â”‚  - è¾“å‡º: åˆå¹¶åçš„å€™é€‰é›†(çº¦60-70ä¸ª)              â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Step 2: å¤šç»´ç‰¹å¾æå–                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ å‘é‡ç‰¹å¾ç»„ (Vector Features)              â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - vector_similarity: ä½™å¼¦ç›¸ä¼¼åº¦           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - query_coverage: æŸ¥è¯¢è¯è¦†ç›–ç‡            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - semantic_distance: è¯­ä¹‰è·ç¦»             â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ å›¾è°±ç‰¹å¾ç»„ (Graph Features)               â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - graph_match_type: åŒ¹é…ç±»å‹å¾—åˆ†          â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - relation_strength: å…³ç³»å¼ºåº¦             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - path_length: å›¾è°±è·¯å¾„é•¿åº¦               â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - centrality_score: èŠ‚ç‚¹ä¸­å¿ƒæ€§            â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ ä¸šåŠ¡ç‰¹å¾ç»„ (Business Features)            â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - domain_match: ä¸šåŠ¡åŸŸåŒ¹é…                â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - metric_importance: æŒ‡æ ‡é‡è¦åº¦           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - usage_frequency: ä½¿ç”¨é¢‘ç‡               â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - data_freshness: æ•°æ®æ–°é²œåº¦              â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ ç”¨æˆ·ç‰¹å¾ç»„ (User Features)                â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - user_preference: ç”¨æˆ·åå¥½å¾—åˆ†           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - click_history: å†å²ç‚¹å‡»ç‡               â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Step 3: èåˆæ‰“åˆ†æ¨¡å‹                           â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ é˜¶æ®µ1: è§„åˆ™æ‰“åˆ† (Rule-based Scoring)      â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - åŸºäºä¸“å®¶ç»éªŒçš„åŠ æƒæ±‚å’Œ                  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - é€‚ç”¨äºå†·å¯åŠ¨é˜¶æ®µ                        â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ é˜¶æ®µ2: æœºå™¨å­¦ä¹ æ’åº (Learning to Rank)    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - LambdaMART / XGBoost                   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ - åŸºäºç”¨æˆ·åé¦ˆæŒç»­ä¼˜åŒ–                    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Step 4: æ’åºä¸æˆªæ–­                             â”‚    â”‚
â”‚  â”‚  - æŒ‰æœ€ç»ˆå¾—åˆ†é™åºæ’åº                           â”‚    â”‚
â”‚  â”‚  - è¿”å›Top-10ç»“æœ                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  è¾“å‡º: ç²¾æ’åçš„Top-10ç»“æœ + å¾—åˆ†æ˜ç»† + æ’åºè§£é‡Š          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 2.2 ç‰¹å¾å·¥ç¨‹è¯¦ç»†è®¾è®¡

#### 2.2.1 å‘é‡ç‰¹å¾ç»„

**ç‰¹å¾ 1: vector_similarity ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰**

```python
def compute_vector_similarity(candidate, query_vector):
    """
    è®¡ç®—æŸ¥è¯¢å‘é‡ä¸å€™é€‰æŒ‡æ ‡å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡å¯¹è±¡
        query_vector: æŸ¥è¯¢å‘é‡ (768ç»´)
    
    Returns:
        float: ç›¸ä¼¼åº¦å¾—åˆ† [0, 1]
    """
    metric_vector = candidate.get('vector')  # ä»å‘é‡åº“è·å–
    
    # ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = cosine_similarity(query_vector, metric_vector)
    
    return float(similarity)


# ç¤ºä¾‹
query = "æœ€è¿‘7å¤©GMV"
query_vector = embedding_model.encode(query)  # [0.123, -0.456, ...]
candidate = {"metricId": "metric_001", "vector": [...]}
score = compute_vector_similarity(candidate, query_vector)
# è¾“å‡º: 0.88
```

**ç‰¹å¾ 2: query_coverage ï¼ˆæŸ¥è¯¢è¯è¦†ç›–ç‡ï¼‰**

```python
def compute_query_coverage(candidate, query_tokens):
    """
    è®¡ç®—å€™é€‰æŒ‡æ ‡å¯¹æŸ¥è¯¢è¯çš„è¦†ç›–ç¨‹åº¦
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡å¯¹è±¡
        query_tokens: æŸ¥è¯¢åˆ†è¯ç»“æœ ["æœ€è¿‘", "7å¤©", "GMV"]
    
    Returns:
        float: è¦†ç›–ç‡ [0, 1]
    """
    # è·å–æŒ‡æ ‡çš„æ‰€æœ‰æ–‡æœ¬å­—æ®µ
    metric_texts = [
        candidate.get('metricName', ''),
        candidate.get('metricCode', ''),
        candidate.get('description', ''),
        ' '.join(candidate.get('synonyms', [])),
    ]
    
    # åˆå¹¶ä¸ºå•ä¸€æ–‡æœ¬
    combined_text = ' '.join(metric_texts).lower()
    
    # è®¡ç®—è¦†ç›–çš„æŸ¥è¯¢è¯æ•°é‡
    covered_tokens = sum(1 for token in query_tokens if token in combined_text)
    
    coverage = covered_tokens / len(query_tokens) if query_tokens else 0
    
    return float(coverage)


# ç¤ºä¾‹
candidate = {
    "metricName": "GMV",
    "metricCode": "GMV",
    "synonyms": ["æˆäº¤æ€»é¢", "äº¤æ˜“é¢"]
}
query_tokens = ["æœ€è¿‘", "7å¤©", "GMV"]
score = compute_query_coverage(candidate, query_tokens)
# è¾“å‡º: 0.33 (åªè¦†ç›–äº†"GMV")
```

**ç‰¹å¾ 3: semantic_distance ï¼ˆè¯­ä¹‰è·ç¦»ï¼‰**

```python
def compute_semantic_distance(candidate, query_embedding, entities):
    """
    è®¡ç®—è¯­ä¹‰ç©ºé—´ä¸­çš„è·ç¦»(è€ƒè™‘å®ä½“ä¿¡æ¯)
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
        query_embedding: æŸ¥è¯¢åµŒå…¥å‘é‡
        entities: è¯†åˆ«å‡ºçš„å®ä½“åˆ—è¡¨
    
    Returns:
        float: å½’ä¸€åŒ–çš„è¯­ä¹‰è·ç¦»å¾—åˆ† [0, 1]
    """
    # åŸºç¡€å‘é‡è·ç¦»
    base_distance = 1 - cosine_similarity(query_embedding, candidate['vector'])
    
    # å®ä½“åŒ¹é…åŠ æˆ
    entity_boost = 0
    for entity in entities:
        if entity['type'] == 'Metric' and entity['value'] in [
            candidate.get('metricName'),
            candidate.get('metricCode')
        ]:
            entity_boost += 0.2
    
    # æœ€ç»ˆè·ç¦»(è·ç¦»è¶Šå°,å¾—åˆ†è¶Šé«˜)
    final_distance = max(0, base_distance - entity_boost)
    score = 1 - final_distance
    
    return float(np.clip(score, 0, 1))
```

---

#### 2.2.2 å›¾è°±ç‰¹å¾ç»„

**ç‰¹å¾ 4: graph_match_type ï¼ˆå›¾è°±åŒ¹é…ç±»å‹å¾—åˆ†ï¼‰**

```python
def compute_graph_match_score(candidate):
    """
    æ ¹æ®å›¾è°±åŒ¹é…ç±»å‹è®¡ç®—å¾—åˆ†
    
    åŒ¹é…ç±»å‹:
    - EXACT: ç²¾ç¡®åŒ¹é…(åç§°/ç¼–ç å®Œå…¨ä¸€è‡´)
    - SYNONYM: åŒä¹‰è¯åŒ¹é…
    - RELATION: å…³ç³»åŒ¹é…(é€šè¿‡å…³ç³»å›¾è°±æ‰¾åˆ°)
    - INFERENCE: æ¨ç†åŒ¹é…(é€šè¿‡æ¨ç†å¾—åˆ°)
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡(åŒ…å«matchTypeå­—æ®µ)
    
    Returns:
        float: åŒ¹é…ç±»å‹å¾—åˆ† [0, 1]
    """
    match_type = candidate.get('matchType', 'UNKNOWN')
    
    score_map = {
        'EXACT': 1.0,      # ç²¾ç¡®åŒ¹é…,æœ€é«˜åˆ†
        'SYNONYM': 0.9,    # åŒä¹‰è¯åŒ¹é…
        'RELATION': 0.7,   # å…³ç³»åŒ¹é…
        'INFERENCE': 0.5,  # æ¨ç†åŒ¹é…
        'UNKNOWN': 0.3     # æœªçŸ¥æ¥æº
    }
    
    return score_map.get(match_type, 0.3)


# ç¤ºä¾‹
candidate_exact = {"matchType": "EXACT"}
candidate_relation = {"matchType": "RELATION"}
print(compute_graph_match_score(candidate_exact))     # 1.0
print(compute_graph_match_score(candidate_relation))  # 0.7
```

**ç‰¹å¾ 5: relation_strength ï¼ˆå…³ç³»å¼ºåº¦ï¼‰**

```python
def compute_relation_strength(candidate, graph_client):
    """
    è®¡ç®—å€™é€‰æŒ‡æ ‡ä¸æŸ¥è¯¢å®ä½“çš„å…³ç³»å¼ºåº¦
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
        graph_client: Neo4jå®¢æˆ·ç«¯
    
    Returns:
        float: å…³ç³»å¼ºåº¦ [0, 1]
    """
    if candidate.get('recallSource') != 'GRAPH':
        return 0.5  # éå›¾è°±å¬å›,è¿”å›ä¸­æ€§å€¼
    
    # è·å–å›¾è°±è·¯å¾„
    match_path = candidate.get('matchPath', [])
    
    if not match_path:
        return 0.5
    
    # è®¡ç®—è·¯å¾„ä¸Šæ‰€æœ‰å…³ç³»çš„å¼ºåº¦ä¹˜ç§¯
    strength = 1.0
    for edge in match_path:
        if 'strength' in edge:
            strength *= edge['strength']
    
    return float(strength)


# ç¤ºä¾‹
candidate = {
    "recallSource": "GRAPH",
    "matchPath": [
        {"relation": "belongsToDomain", "strength": 0.9},
        {"relation": "correlatesWith", "strength": 0.8}
    ]
}
score = compute_relation_strength(candidate, None)
# è¾“å‡º: 0.72 (0.9 * 0.8)
```

**ç‰¹å¾ 6: path_length ï¼ˆè·¯å¾„é•¿åº¦ï¼‰**

```python
def compute_path_length_score(candidate):
    """
    è®¡ç®—å›¾è°±è·¯å¾„é•¿åº¦å¾—åˆ†(è·¯å¾„è¶ŠçŸ­è¶Šå¥½)
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
    
    Returns:
        float: è·¯å¾„é•¿åº¦å¾—åˆ† [0, 1]
    """
    match_path = candidate.get('matchPath', [])
    
    if not match_path:
        return 1.0  # ç›´æ¥åŒ¹é…,è·¯å¾„é•¿åº¦ä¸º0
    
    path_length = len(match_path)
    
    # è·¯å¾„é•¿åº¦æƒ©ç½š: é•¿åº¦è¶Šå¤§,å¾—åˆ†è¶Šä½
    # å…¬å¼: score = 1 / (1 + path_length)
    score = 1.0 / (1.0 + path_length)
    
    return float(score)


# ç¤ºä¾‹
candidate_direct = {"matchPath": []}
candidate_2hop = {"matchPath": [{"rel": "r1"}, {"rel": "r2"}]}
print(compute_path_length_score(candidate_direct))  # 1.0
print(compute_path_length_score(candidate_2hop))    # 0.33
```

**ç‰¹å¾ 7: centrality_score ï¼ˆèŠ‚ç‚¹ä¸­å¿ƒæ€§ï¼‰**

```python
def compute_centrality_score(candidate, graph_client):
    """
    è®¡ç®—å€™é€‰æŒ‡æ ‡åœ¨å›¾è°±ä¸­çš„ä¸­å¿ƒæ€§(é‡è¦åº¦)
    
    ä½¿ç”¨PageRankæˆ–åº¦ä¸­å¿ƒæ€§
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
        graph_client: Neo4jå®¢æˆ·ç«¯
    
    Returns:
        float: ä¸­å¿ƒæ€§å¾—åˆ† [0, 1]
    """
    metric_id = candidate.get('metricId')
    
    # æŸ¥è¯¢èŠ‚ç‚¹çš„PageRankå€¼(éœ€è¦é¢„å…ˆè®¡ç®—)
    result = graph_client.run("""
        MATCH (m:Metric {metricId: $metricId})
        RETURN m.pagerank as pagerank, m.degree as degree
    """, {"metricId": metric_id}).single()
    
    if not result:
        return 0.5
    
    # å½’ä¸€åŒ–PageRankå€¼
    pagerank = result['pagerank'] or 0
    max_pagerank = 1.0  # å‡è®¾æœ€å¤§PageRankä¸º1
    
    score = pagerank / max_pagerank
    
    return float(np.clip(score, 0, 1))
```

---

#### 2.2.3 ä¸šåŠ¡ç‰¹å¾ç»„

**ç‰¹å¾ 8: domain_match ï¼ˆä¸šåŠ¡åŸŸåŒ¹é…ï¼‰**

```python
def compute_domain_match(candidate, query_context):
    """
    è®¡ç®—å€™é€‰æŒ‡æ ‡ä¸æŸ¥è¯¢ä¸Šä¸‹æ–‡çš„ä¸šåŠ¡åŸŸåŒ¹é…åº¦
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
        query_context: æŸ¥è¯¢ä¸Šä¸‹æ–‡(åŒ…å«è¯†åˆ«å‡ºçš„ä¸šåŠ¡åŸŸ)
    
    Returns:
        float: ä¸šåŠ¡åŸŸåŒ¹é…å¾—åˆ† [0, 1]
    """
    candidate_domain = candidate.get('businessDomain', '')
    query_domains = query_context.get('businessDomains', [])
    
    if not query_domains:
        return 0.5  # æ— æ³•åˆ¤æ–­,è¿”å›ä¸­æ€§å€¼
    
    # ç²¾ç¡®åŒ¹é…
    if candidate_domain in query_domains:
        return 1.0
    
    # çˆ¶å­åŸŸåŒ¹é…(ä¾‹å¦‚: "äº¤æ˜“åŸŸ" åŒ…å« "è®¢å•åŸŸ")
    domain_hierarchy = {
        "äº¤æ˜“åŸŸ": ["è®¢å•åŸŸ", "æ”¯ä»˜åŸŸ"],
        "ç”¨æˆ·åŸŸ": ["ä¼šå‘˜åŸŸ", "è¡Œä¸ºåŸŸ"]
    }
    
    for query_domain in query_domains:
        if query_domain in domain_hierarchy:
            if candidate_domain in domain_hierarchy[query_domain]:
                return 0.8
    
    return 0.0


# ç¤ºä¾‹
candidate = {"businessDomain": "è®¢å•åŸŸ"}
context = {"businessDomains": ["äº¤æ˜“åŸŸ"]}
score = compute_domain_match(candidate, context)
# è¾“å‡º: 0.8 (è®¢å•åŸŸæ˜¯äº¤æ˜“åŸŸçš„å­åŸŸ)
```

**ç‰¹å¾ 9: metric_importance ï¼ˆæŒ‡æ ‡é‡è¦åº¦ï¼‰**

```python
def compute_metric_importance(candidate):
    """
    è®¡ç®—æŒ‡æ ‡çš„ä¸šåŠ¡é‡è¦åº¦
    
    é‡è¦åº¦æ¥æº:
    1. äººå·¥æ ‡æ³¨çš„é‡è¦åº¦
    2. ä½¿ç”¨é¢‘ç‡
    3. è¢«å¼•ç”¨æ¬¡æ•°
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
    
    Returns:
        float: é‡è¦åº¦å¾—åˆ† [0, 1]
    """
    # äººå·¥æ ‡æ³¨é‡è¦åº¦(1-5æ˜Ÿ)
    manual_importance = candidate.get('importance', 3) / 5.0
    
    # ä½¿ç”¨é¢‘ç‡(å½’ä¸€åŒ–)
    usage_count = candidate.get('usageCount', 0)
    max_usage = 10000  # å‡è®¾æœ€å¤§ä½¿ç”¨æ¬¡æ•°
    usage_score = min(usage_count / max_usage, 1.0)
    
    # è¢«å¼•ç”¨æ¬¡æ•°(æœ‰å¤šå°‘å…¶ä»–æŒ‡æ ‡ä¾èµ–å®ƒ)
    reference_count = candidate.get('referenceCount', 0)
    max_reference = 100
    reference_score = min(reference_count / max_reference, 1.0)
    
    # åŠ æƒå¹³å‡
    final_score = (
        0.5 * manual_importance +
        0.3 * usage_score +
        0.2 * reference_score
    )
    
    return float(final_score)
```

**ç‰¹å¾ 10: usage_frequency ï¼ˆä½¿ç”¨é¢‘ç‡ï¼‰**

```python
def compute_usage_frequency(candidate, time_window_days=30):
    """
    è®¡ç®—æŒ‡æ ‡åœ¨æœ€è¿‘æ—¶é—´çª—å£å†…çš„ä½¿ç”¨é¢‘ç‡
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
        time_window_days: æ—¶é—´çª—å£(å¤©)
    
    Returns:
        float: ä½¿ç”¨é¢‘ç‡å¾—åˆ† [0, 1]
    """
    recent_usage = candidate.get('recentUsageCount', 0)
    
    # å½’ä¸€åŒ–(å‡è®¾çƒ­é—¨æŒ‡æ ‡æ¯å¤©è¢«ä½¿ç”¨100æ¬¡)
    max_usage = 100 * time_window_days
    score = min(recent_usage / max_usage, 1.0)
    
    return float(score)
```

---

#### 2.2.4 ç”¨æˆ·ç‰¹å¾ç»„

**ç‰¹å¾ 11: user_preference ï¼ˆç”¨æˆ·åå¥½ï¼‰**

```python
def compute_user_preference(candidate, user_profile):
    """
    è®¡ç®—å€™é€‰æŒ‡æ ‡ä¸ç”¨æˆ·åå¥½çš„åŒ¹é…åº¦
    
    åŸºäºç”¨æˆ·å†å²è¡Œä¸ºå»ºç«‹çš„åå¥½æ¨¡å‹
    
    Args:
        candidate: å€™é€‰æŒ‡æ ‡
        user_profile: ç”¨æˆ·ç”»åƒ
    
    Returns:
        float: ç”¨æˆ·åå¥½å¾—åˆ† [0, 1]
    """
    if not user_profile:
        return 0.5
    
    # ç”¨æˆ·åå¥½çš„ä¸šåŠ¡åŸŸ
    preferred_domains = user_profile.get('preferredDomains', {})
    candidate_domain = candidate.get('businessDomain')
    
    domain_score = preferred_domains.get(candidate_domain, 0.5)
    
    # ç”¨æˆ·åå¥½çš„æŒ‡æ ‡ç±»å‹
    preferred_types = user_profile.get('preferredMetricTypes', {})
    candidate_type = candidate.get('metricType')
    
    type_score = preferred_types.get(candidate_type, 0.5)
    
    # ç»¼åˆå¾—åˆ†
    score = 0.6 * domain_score + 0.4 * type_score
    
    return float(score)
```

---

### 2.3 èåˆæ‰“åˆ†æ¨¡å‹

#### 2.3.1 é˜¶æ®µ 1: è§„åˆ™æ‰“åˆ† ï¼ˆå†·å¯åŠ¨æ–¹æ¡ˆï¼‰

```python
class RuleBasedRanker:
    """
    åŸºäºè§„åˆ™çš„æ’åºå™¨
    é€‚ç”¨äºå†·å¯åŠ¨é˜¶æ®µ,æ— éœ€è®­ç»ƒæ•°æ®
    """
    
    def __init__(self):
        # ç‰¹å¾æƒé‡(åŸºäºä¸“å®¶ç»éªŒ)
        self.weights = {
            # å‘é‡ç‰¹å¾ç»„ (æƒé‡å’Œ: 0.30)
            'vector_similarity': 0.20,
            'query_coverage': 0.08,
            'semantic_distance': 0.02,
            
            # å›¾è°±ç‰¹å¾ç»„ (æƒé‡å’Œ: 0.35)
            'graph_match_type': 0.15,
            'relation_strength': 0.10,
            'path_length': 0.05,
            'centrality_score': 0.05,
            
            # ä¸šåŠ¡ç‰¹å¾ç»„ (æƒé‡å’Œ: 0.25)
            'domain_match': 0.10,
            'metric_importance': 0.08,
            'usage_frequency': 0.07,
            
            # ç”¨æˆ·ç‰¹å¾ç»„ (æƒé‡å’Œ: 0.10)
            'user_preference': 0.10,
        }
    
    def rank(self, candidates, features):
        """
        å¯¹å€™é€‰é›†è¿›è¡Œæ’åº
        
        Args:
            candidates: å€™é€‰æŒ‡æ ‡åˆ—è¡¨
            features: ç‰¹å¾çŸ©é˜µ (N x 11)
        
        Returns:
            æ’åºåçš„å€™é€‰åˆ—è¡¨
        """
        scores = []
        
        for i, candidate in enumerate(candidates):
            # åŠ æƒæ±‚å’Œ
            score = sum(
                self.weights[feature_name] * features[i][feature_name]
                for feature_name in self.weights.keys()
            )
            
            scores.append({
                'candidate': candidate,
                'finalScore': score,
                'featureBreakdown': features[i]
            })
        
        # æŒ‰å¾—åˆ†é™åºæ’åº
        ranked = sorted(scores, key=lambda x: x['finalScore'], reverse=True)
        
        return ranked


# ä½¿ç”¨ç¤ºä¾‹
ranker = RuleBasedRanker()

candidates = [...]  # å€™é€‰åˆ—è¡¨
features = [
    {
        'vector_similarity': 0.88,
        'query_coverage': 0.33,
        'graph_match_type': 1.0,
        'relation_strength': 0.9,
        # ... å…¶ä»–ç‰¹å¾
    },
    # ... æ›´å¤šå€™é€‰
]

ranked_results = ranker.rank(candidates, features)
top10 = ranked_results[:10]
```

---

#### 2.3.2 é˜¶æ®µ 2: Learning to Rank ï¼ˆç”Ÿäº§æ–¹æ¡ˆï¼‰

```python
import xgboost as xgb
from sklearn.model_selection import train_test_split

class LearningToRankModel:
    """
    åŸºäºXGBoostçš„Learning to Rankæ¨¡å‹
    ä½¿ç”¨LambdaMARTç®—æ³•
    """
    
    def __init__(self):
        self.model = None
        self.feature_names = [
            'vector_similarity', 'query_coverage', 'semantic_distance',
            'graph_match_type', 'relation_strength', 'path_length', 'centrality_score',
            'domain_match', 'metric_importance', 'usage_frequency',
            'user_preference'
        ]
    
    def train(self, training_data):
        """
        è®­ç»ƒæ’åºæ¨¡å‹
        
        Args:
            training_data: è®­ç»ƒæ•°æ®
                {
                    'queries': [...],  # æŸ¥è¯¢åˆ—è¡¨
                    'candidates': [...],  # æ¯ä¸ªæŸ¥è¯¢çš„å€™é€‰åˆ—è¡¨
                    'features': [...],  # ç‰¹å¾çŸ©é˜µ
                    'labels': [...]  # ç›¸å…³æ€§æ ‡ç­¾ (0-4)
                }
        """
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = []  # ç‰¹å¾çŸ©é˜µ
        y = []  # æ ‡ç­¾
        qids = []  # æŸ¥è¯¢ID(ç”¨äºåˆ†ç»„)
        
        for query_idx, query in enumerate(training_data['queries']):
            candidates = training_data['candidates'][query_idx]
            features = training_data['features'][query_idx]
            labels = training_data['labels'][query_idx]
            
            for i, candidate in enumerate(candidates):
                X.append([features[i][f] for f in self.feature_names])
                y.append(labels[i])
                qids.append(query_idx)
        
        # è½¬æ¢ä¸ºXGBoostæ ¼å¼
        dtrain = xgb.DMatrix(X, label=y)
        dtrain.set_group([len(training_data['candidates'][i]) 
                          for i in range(len(training_data['queries']))])
        
        # è®­ç»ƒå‚æ•°
        params = {
            'objective': 'rank:ndcg',  # LambdaMARTç›®æ ‡
            'eta': 0.1,
            'max_depth': 6,
            'eval_metric': 'ndcg@10'
        }
        
        # è®­ç»ƒæ¨¡å‹
        self.model = xgb.train(
            params,
            dtrain,
            num_boost_round=100,
            verbose_eval=10
        )
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def rank(self, candidates, features):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ’åº
        
        Args:
            candidates: å€™é€‰åˆ—è¡¨
            features: ç‰¹å¾çŸ©é˜µ
        
        Returns:
            æ’åºåçš„ç»“æœ
        """
        if not self.model:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒ,è¯·å…ˆè°ƒç”¨train()")
        
        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        X = [[f[feat] for feat in self.feature_names] for f in features]
        dtest = xgb.DMatrix(X)
        
        # é¢„æµ‹å¾—åˆ†
        scores = self.model.predict(dtest)
        
        # ç»„åˆç»“æœ
        results = []
        for i, candidate in enumerate(candidates):
            results.append({
                'candidate': candidate,
                'finalScore': float(scores[i]),
                'featureBreakdown': features[i]
            })
        
        # æ’åº
        ranked = sorted(results, key=lambda x: x['finalScore'], reverse=True)
        
        return ranked
    
    def save(self, model_path):
        """ä¿å­˜æ¨¡å‹"""
        self.model.save_model(model_path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load(self, model_path):
        """åŠ è½½æ¨¡å‹"""
        self.model = xgb.Booster()
        self.model.load_model(model_path)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {model_path}")


# ä½¿ç”¨ç¤ºä¾‹
ltr_model = LearningToRankModel()

# è®­ç»ƒ(ä½¿ç”¨æ ‡æ³¨æ•°æ®)
training_data = {
    'queries': ["æœ€è¿‘7å¤©GMV", "è®¢å•é‡è¶‹åŠ¿", ...],
    'candidates': [...],
    'features': [...],
    'labels': [
        [4, 3, 2, 1, 0, ...],  # ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç›¸å…³æ€§æ ‡ç­¾
        [4, 4, 3, 2, 1, ...],  # ç¬¬äºŒä¸ªæŸ¥è¯¢çš„ç›¸å…³æ€§æ ‡ç­¾
        # æ ‡ç­¾è¯´æ˜: 4=å®Œå…¨ç›¸å…³, 3=é«˜åº¦ç›¸å…³, 2=ç›¸å…³, 1=å¼±ç›¸å…³, 0=ä¸ç›¸å…³
    ]
}
ltr_model.train(training_data)

# ä¿å­˜æ¨¡å‹
ltr_model.save("models/rerank_model.xgb")

# æ¨ç†
ltr_model.load("models/rerank_model.xgb")
ranked_results = ltr_model.rank(candidates, features)
```

---

### 2.4 å®Œæ•´çš„ Rerank å¼•æ“å®ç°

```python
import numpy as np
from typing import List, Dict, Any
import time

class SemanticFusionRerankEngine:
    """
    è¯­ä¹‰èåˆä¸ç²¾æ’å¼•æ“
    æ•´åˆå‘é‡å¬å›å’Œå›¾è°±å¬å›çš„ç»“æœ,è¿›è¡Œå¤šç‰¹å¾èåˆæ’åº
    """
    
    def __init__(self, mode='rule', model_path=None):
        """
        Args:
            mode: 'rule' æˆ– 'ltr'
            model_path: LTRæ¨¡å‹è·¯å¾„(ä»…å½“mode='ltr'æ—¶éœ€è¦)
        """
        self.mode = mode
        
        if mode == 'rule':
            self.ranker = RuleBasedRanker()
        elif mode == 'ltr':
            self.ranker = LearningToRankModel()
            if model_path:
                self.ranker.load(model_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å¼: {mode}")
    
    def fuse_and_rank(
        self,
        vector_candidates: List[Dict],
        graph_candidates: List[Dict],
        query: str,
        query_vector: np.ndarray,
        query_context: Dict,
        user_profile: Dict = None,
        top_k: int = 10
    ) -> Dict[str, Any]:
        """
        èåˆå¹¶æ’åº
        
        Args:
            vector_candidates: å‘é‡å¬å›çš„å€™é€‰åˆ—è¡¨
            graph_candidates: å›¾è°±å¬å›çš„å€™é€‰åˆ—è¡¨
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            query_vector: æŸ¥è¯¢å‘é‡
            query_context: æŸ¥è¯¢ä¸Šä¸‹æ–‡(æ„å›¾ã€å®ä½“ç­‰)
            user_profile: ç”¨æˆ·ç”»åƒ
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            {
                'rankedResults': [...],
                'executionTime': {...},
                'statistics': {...}
            }
        """
        start_time = time.time()
        
        # Step 1: åˆå¹¶ä¸å»é‡
        merge_start = time.time()
        merged_candidates = self._merge_candidates(
            vector_candidates, 
            graph_candidates
        )
        merge_time = (time.time() - merge_start) * 1000
        
        # Step 2: ç‰¹å¾æå–
        feature_start = time.time()
        features = self._extract_features(
            merged_candidates,
            query,
            query_vector,
            query_context,
            user_profile
        )
        feature_time = (time.time() - feature_start) * 1000
        
        # Step 3: æ’åº
        rank_start = time.time()
        ranked_results = self.ranker.rank(merged_candidates, features)
        rank_time = (time.time() - rank_start) * 1000
        
        # Step 4: æˆªæ–­Top-K
        top_results = ranked_results[:top_k]
        
        total_time = (time.time() - start_time) * 1000
        
        return {
            'rankedResults': top_results,
            'executionTime': {
                'merge': f"{merge_time:.2f}ms",
                'featureExtraction': f"{feature_time:.2f}ms",
                'ranking': f"{rank_time:.2f}ms",
                'total': f"{total_time:.2f}ms"
            },
            'statistics': {
                'vectorCandidates': len(vector_candidates),
                'graphCandidates': len(graph_candidates),
                'mergedCandidates': len(merged_candidates),
                'topK': len(top_results)
            }
        }
    
    def _merge_candidates(
        self, 
        vector_candidates: List[Dict], 
        graph_candidates: List[Dict]
    ) -> List[Dict]:
        """
        åˆå¹¶å¹¶å»é‡å€™é€‰é›†
        """
        # ä½¿ç”¨å­—å…¸å»é‡(ä»¥metricIdä¸ºkey)
        merged = {}
        
        # æ·»åŠ å‘é‡å¬å›ç»“æœ
        for candidate in vector_candidates:
            metric_id = candidate['metricId']
            merged[metric_id] = {
                **candidate,
                'recallSource': 'VECTOR',
                'vectorScore': candidate.get('similarity', 0)
            }
        
        # æ·»åŠ å›¾è°±å¬å›ç»“æœ
        for candidate in graph_candidates:
            metric_id = candidate['metricId']
            if metric_id in merged:
                # å·²å­˜åœ¨,æ ‡è®°ä¸ºåŒè·¯å¬å›
                merged[metric_id]['recallSource'] = 'BOTH'
                merged[metric_id]['graphScore'] = candidate.get('confidence', 0)
                merged[metric_id]['matchType'] = candidate.get('matchType')
                merged[metric_id]['matchPath'] = candidate.get('matchPath', [])
            else:
                # ä»…å›¾è°±å¬å›
                merged[metric_id] = {
                    **candidate,
                    'recallSource': 'GRAPH',
                    'graphScore': candidate.get('confidence', 0)
                }
        
        return list(merged.values())
    
    def _extract_features(
        self,
        candidates: List[Dict],
        query: str,
        query_vector: np.ndarray,
        query_context: Dict,
        user_profile: Dict
    ) -> List[Dict]:
        """
        ä¸ºæ¯ä¸ªå€™é€‰æå–ç‰¹å¾
        """
        query_tokens = query.split()  # ç®€åŒ–ç‰ˆåˆ†è¯
        entities = query_context.get('entities', [])
        
        features = []
        
        for candidate in candidates:
            feature = {
                # å‘é‡ç‰¹å¾
                'vector_similarity': compute_vector_similarity(candidate, query_vector),
                'query_coverage': compute_query_coverage(candidate, query_tokens),
                'semantic_distance': compute_semantic_distance(candidate, query_vector, entities),
                
                # å›¾è°±ç‰¹å¾
                'graph_match_type': compute_graph_match_score(candidate),
                'relation_strength': compute_relation_strength(candidate, None),
                'path_length': compute_path_length_score(candidate),
                'centrality_score': candidate.get('centrality', 0.5),
                
                # ä¸šåŠ¡ç‰¹å¾
                'domain_match': compute_domain_match(candidate, query_context),
                'metric_importance': compute_metric_importance(candidate),
                'usage_frequency': compute_usage_frequency(candidate),
                
                # ç”¨æˆ·ç‰¹å¾
                'user_preference': compute_user_preference(candidate, user_profile),
            }
            
            features.append(feature)
        
        return features


# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    # åˆå§‹åŒ–å¼•æ“
    engine = SemanticFusionRerankEngine(mode='rule')
    
    # æ¨¡æ‹Ÿè¾“å…¥
    vector_candidates = [
        {
            'metricId': 'metric_001',
            'metricName': 'GMV',
            'similarity': 0.88,
            'vector': np.random.rand(768)
        },
        {
            'metricId': 'metric_002',
            'metricName': 'è®¢å•é‡‘é¢',
            'similarity': 0.82,
            'vector': np.random.rand(768)
        }
    ]
    
    graph_candidates = [
        {
            'metricId': 'metric_001',
            'metricName': 'GMV',
            'matchType': 'EXACT',
            'confidence': 0.95,
            'matchPath': []
        }
    ]
    
    query = "æœ€è¿‘7å¤©GMV"
    query_vector = np.random.rand(768)
    query_context = {
        'intent': 'METRIC_QUERY',
        'entities': [{'type': 'Metric', 'value': 'GMV'}],
        'businessDomains': ['äº¤æ˜“åŸŸ']
    }
    
    # æ‰§è¡Œèåˆæ’åº
    result = engine.fuse_and_rank(
        vector_candidates,
        graph_candidates,
        query,
        query_vector,
        query_context,
        top_k=10
    )
    
    print("ğŸ¯ èåˆæ’åºç»“æœ:")
    print(f"æ‰§è¡Œæ—¶é—´: {result['executionTime']}")
    print(f"ç»Ÿè®¡ä¿¡æ¯: {result['statistics']}")
    print("
Top-3ç»“æœ:")
    for i, item in enumerate(result['rankedResults'][:3], 1):
        print(f"{i}. {item['candidate']['metricName']} - å¾—åˆ†: {item['finalScore']:.3f}")
```

---

## ä¸‰ã€æ ¸å¿ƒæŠ€æœ¯ 2: æœ¬ä½“å›¾è°±éªŒè¯å™¨ (Ontology Validator)

### 3.1 æŠ€æœ¯æ¶æ„

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           æœ¬ä½“å›¾è°±éªŒè¯å™¨ (Ontology Validator)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  è¾“å…¥: å€™é€‰æŒ‡æ ‡ + æŸ¥è¯¢ç»´åº¦ + è¿‡æ»¤æ¡ä»¶ + ç”¨æˆ·ä¿¡æ¯          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  éªŒè¯å±‚1: ç»´åº¦å…¼å®¹æ€§éªŒè¯                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥1: æŒ‡æ ‡æ˜¯å¦æ”¯æŒè¯·æ±‚çš„ç»´åº¦             â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥2: ç»´åº¦ç»„åˆæ˜¯å¦åˆæ³•                   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥3: ç»´åº¦åŸºæ•°æ˜¯å¦ä¼šçˆ†ç‚¸                 â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  éªŒè¯å±‚2: ä¸šåŠ¡çº¦æŸéªŒè¯                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥4: æ—¶é—´ç²’åº¦çº¦æŸ                       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥5: æ•°æ®æ–°é²œåº¦çº¦æŸ                     â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥6: ä¸šåŠ¡è§„åˆ™çº¦æŸ                       â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  éªŒè¯å±‚3: æ•°æ®æƒé™éªŒè¯                          â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥7: ç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®è¯¥æŒ‡æ ‡           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥8: ç»´åº¦çº§åˆ«çš„æƒé™æ§åˆ¶                 â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  éªŒè¯å±‚4: æŸ¥è¯¢åˆç†æ€§éªŒè¯                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥9: æ—¶é—´èŒƒå›´æ˜¯å¦è¿‡å¤§                   â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥10: é¢„ä¼°ç»“æœé›†å¤§å°                    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚ æ£€æŸ¥11: æŸ¥è¯¢å¤æ‚åº¦è¯„ä¼°                    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  è¾“å‡º: éªŒè¯ç»“æœ + é”™è¯¯/è­¦å‘Šä¿¡æ¯ + ä¿®å¤å»ºè®®               â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 3.2 è¯¦ç»†å®ç°

```python
from typing import List, Dict, Any, Tuple
from enum import Enum
from dataclasses import dataclass
from datetime import datetime, timedelta

class ValidationStatus(Enum):
    """éªŒè¯çŠ¶æ€"""
    PASSED = "PASSED"      # é€šè¿‡
    WARNING = "WARNING"    # è­¦å‘Š(å¯ç»§ç»­)
    FAILED = "FAILED"      # å¤±è´¥(ä¸å¯ç»§ç»­)

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    status: ValidationStatus
    check_type: str
    message: str
    suggestion: str = None
    details: Dict = None

class OntologyValidator:
    """
    æœ¬ä½“å›¾è°±éªŒè¯å™¨
    éªŒè¯æŸ¥è¯¢çš„åˆæ³•æ€§å’Œä¸šåŠ¡çº¦æŸ
    """
    
    def __init__(self, graph_client, config):
        """
        Args:
            graph_client: Neo4jå®¢æˆ·ç«¯
            config: é…ç½®ä¿¡æ¯(é˜ˆå€¼ã€è§„åˆ™ç­‰)
        """
        self.graph = graph_client
        self.config = config
    
    def validate(
        self,
        metric_id: str,
        dimensions: List[str],
        filters: List[Dict],
        user_id: str,
        query_context: Dict = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´éªŒè¯æµç¨‹
        
        Args:
            metric_id: æŒ‡æ ‡ID
            dimensions: è¯·æ±‚çš„ç»´åº¦åˆ—è¡¨
            filters: è¿‡æ»¤æ¡ä»¶
            user_id: ç”¨æˆ·ID
            query_context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
        
        Returns:
            {
                'validationResult': 'PASSED' | 'WARNING' | 'FAILED',
                'checks': [ValidationResult, ...],
                'suggestions': [str, ...],
                'canProceed': bool
            }
        """
        results = []
        
        # è·å–æŒ‡æ ‡å…ƒæ•°æ®
        metric = self._get_metric_metadata(metric_id)
        if not metric:
            return self._build_error_response("æŒ‡æ ‡ä¸å­˜åœ¨")
        
        # éªŒè¯å±‚1: ç»´åº¦å…¼å®¹æ€§
        results.extend(self._validate_dimension_compatibility(
            metric, dimensions
        ))
        
        # éªŒè¯å±‚2: ä¸šåŠ¡çº¦æŸ
        results.extend(self._validate_business_constraints(
            metric, filters, query_context
        ))
        
        # éªŒè¯å±‚3: æ•°æ®æƒé™
        results.extend(self._validate_data_permission(
            metric, dimensions, user_id
        ))
        
        # éªŒè¯å±‚4: æŸ¥è¯¢åˆç†æ€§
        results.extend(self._validate_query_reasonability(
            metric, dimensions, filters
        ))
        
        # æ±‡æ€»ç»“æœ
        return self._summarize_results(results)
    
    def _validate_dimension_compatibility(
        self,
        metric: Dict,
        requested_dimensions: List[str]
    ) -> List[ValidationResult]:
        """
        éªŒè¯ç»´åº¦å…¼å®¹æ€§
        """
        results = []
        
        # æ£€æŸ¥1: æŒ‡æ ‡æ˜¯å¦æ”¯æŒè¯·æ±‚çš„ç»´åº¦
        supported_dimensions = self._get_supported_dimensions(metric['metricId'])
        
        unsupported = set(requested_dimensions) - set(supported_dimensions)
        
        if unsupported:
            results.append(ValidationResult(
                status=ValidationStatus.FAILED,
                check_type="DIMENSION_COMPATIBILITY",
                message=f"æŒ‡æ ‡ä¸æ”¯æŒä»¥ä¸‹ç»´åº¦: {', '.join(unsupported)}",
                suggestion=f"æ”¯æŒçš„ç»´åº¦: {', '.join(supported_dimensions)}",
                details={'unsupported': list(unsupported)}
            ))
        else:
            results.append(ValidationResult(
                status=ValidationStatus.PASSED,
                check_type="DIMENSION_COMPATIBILITY",
                message="æŒ‡æ ‡æ”¯æŒæ‰€æœ‰è¯·æ±‚çš„ç»´åº¦"
            ))
        
        # æ£€æŸ¥2: ç»´åº¦ç»„åˆæ˜¯å¦åˆæ³•
        if len(requested_dimensions) > 1:
            incompatible_pairs = self._check_dimension_conflicts(
                requested_dimensions
            )
            
            if incompatible_pairs:
                results.append(ValidationResult(
                    status=ValidationStatus.WARNING,
                    check_type="DIMENSION_COMBINATION",
                    message=f"ç»´åº¦ç»„åˆå¯èƒ½ä¸å…¼å®¹: {incompatible_pairs}",
                    suggestion="å»ºè®®åˆ†åˆ«æŸ¥è¯¢è¿™äº›ç»´åº¦"
                ))
        
        # æ£€æŸ¥3: ç»´åº¦åŸºæ•°çˆ†ç‚¸æ£€æŸ¥
        cardinality_warning = self._check_cardinality_explosion(
            metric, requested_dimensions
        )
        
        if cardinality_warning:
            results.append(cardinality_warning)
        
        return results
    
    def _validate_business_constraints(
        self,
        metric: Dict,
        filters: List[Dict],
        query_context: Dict
    ) -> List[ValidationResult]:
        """
        éªŒè¯ä¸šåŠ¡çº¦æŸ
        """
        results = []
        
        # æ£€æŸ¥4: æ—¶é—´ç²’åº¦çº¦æŸ
        min_granularity = metric.get('minGranularity', 'day')
        requested_granularity = query_context.get('timeGranularity', 'day')
        
        granularity_order = ['hour', 'day', 'week', 'month', 'year']
        
        if granularity_order.index(requested_granularity) < \
           granularity_order.index(min_granularity):
            results.append(ValidationResult(
                status=ValidationStatus.FAILED,
                check_type="TIME_GRANULARITY",
                message=f"æŒ‡æ ‡ä¸æ”¯æŒ{requested_granularity}ç²’åº¦",
                suggestion=f"æœ€å°ç²’åº¦ä¸º{min_granularity}"
            ))
        else:
            results.append(ValidationResult(
                status=ValidationStatus.PASSED,
                check_type="TIME_GRANULARITY",
                message="æ—¶é—´ç²’åº¦ç¬¦åˆè¦æ±‚"
            ))
        
        # æ£€æŸ¥5: æ•°æ®æ–°é²œåº¦çº¦æŸ
        refresh_frequency = metric.get('refreshFrequency', 'daily')
        time_filter = next((f for f in filters if f.get('dimension') == 'æ—¶é—´'), None)
        
        if time_filter and refresh_frequency == 'daily':
            time_range = time_filter.get('value', '')
            if 'today' in time_range.lower() or str(datetime.now().date()) in time_range:
                results.append(ValidationResult(
                    status=ValidationStatus.WARNING,
                    check_type="DATA_FRESHNESS",
                    message="ä»Šæ—¥æ•°æ®å°šæœªæ›´æ–°",
                    suggestion=f"æ•°æ®æ›´æ–°é¢‘ç‡ä¸º{refresh_frequency},å»ºè®®æŸ¥è¯¢æ˜¨æ—¥åŠä¹‹å‰çš„æ•°æ®"
                ))
        
        # æ£€æŸ¥6: ä¸šåŠ¡è§„åˆ™çº¦æŸ
        business_rules = metric.get('businessRules', [])
        for rule in business_rules:
            violation = self._check_business_rule(rule, filters, query_context)
            if violation:
                results.append(violation)
        
        return results
    
    def _validate_data_permission(
        self,
        metric: Dict,
        dimensions: List[str],
        user_id: str
    ) -> List[ValidationResult]:
        """
        éªŒè¯æ•°æ®æƒé™
        """
        results = []
        
        # æ£€æŸ¥7: ç”¨æˆ·æ˜¯å¦æœ‰æƒé™è®¿é—®è¯¥æŒ‡æ ‡
        has_metric_permission = self._check_metric_permission(
            metric['metricId'], user_id
        )
        
        if not has_metric_permission:
            results.append(ValidationResult(
                status=ValidationStatus.FAILED,
                check_type="DATA_PERMISSION",
                message="æ— æƒé™è®¿é—®è¯¥æŒ‡æ ‡",
                suggestion="è¯·è”ç³»ç®¡ç†å‘˜ç”³è¯·æƒé™"
            ))
            return results  # å¦‚æœæŒ‡æ ‡æƒé™ä¸é€šè¿‡,ç›´æ¥è¿”å›
        
        results.append(ValidationResult(
            status=ValidationStatus.PASSED,
            check_type="DATA_PERMISSION",
            message="ç”¨æˆ·æœ‰æƒé™è®¿é—®è¯¥æŒ‡æ ‡"
        ))
        
        # æ£€æŸ¥8: ç»´åº¦çº§åˆ«çš„æƒé™æ§åˆ¶
        for dimension in dimensions:
            has_dimension_permission = self._check_dimension_permission(
                user_id, dimension
            )
            
            if not has_dimension_permission:
                results.append(ValidationResult(
                    status=ValidationStatus.WARNING,
                    check_type="DIMENSION_PERMISSION",
                    message=f"å¯¹ç»´åº¦'{dimension}'çš„è®¿é—®æƒé™å—é™",
                    suggestion="å¯èƒ½åªèƒ½çœ‹åˆ°éƒ¨åˆ†æ•°æ®"
                ))
        
        return results
    
    def _validate_query_reasonability(
        self,
        metric: Dict,
        dimensions: List[str],
        filters: List[Dict]
    ) -> List[ValidationResult]:
        """
        éªŒè¯æŸ¥è¯¢åˆç†æ€§
        """
        results = []
        
        # æ£€æŸ¥9: æ—¶é—´èŒƒå›´æ˜¯å¦è¿‡å¤§
        time_filter = next((f for f in filters if f.get('dimension') == 'æ—¶é—´'), None)
        
        if time_filter:
            time_range_days = self._parse_time_range(time_filter.get('value', ''))
            max_time_range = self.config.get('maxTimeRangeDays', 365)
            
            if time_range_days > max_time_range:
                results.append(ValidationResult(
                    status=ValidationStatus.WARNING,
                    check_type="TIME_RANGE",
                    message=f"æ—¶é—´èŒƒå›´è¿‡å¤§({time_range_days}å¤©)",
                    suggestion=f"å»ºè®®æ—¶é—´èŒƒå›´ä¸è¶…è¿‡{max_time_range}å¤©"
                ))
        
        # æ£€æŸ¥10: é¢„ä¼°ç»“æœé›†å¤§å°
        estimated_rows = self._estimate_result_size(metric, dimensions, filters)
        max_rows = self.config.get('maxResultRows', 1000000)
        
        if estimated_rows > max_rows:
            results.append(ValidationResult(
                status=ValidationStatus.WARNING,
                check_type="RESULT_SIZE",
                message=f"é¢„ä¼°ç»“æœé›†è¿‡å¤§(çº¦{estimated_rows:,}è¡Œ)",
                suggestion="å»ºè®®å¢åŠ è¿‡æ»¤æ¡ä»¶æˆ–å‡å°‘ç»´åº¦"
            ))
        else:
            results.append(ValidationResult(
                status=ValidationStatus.PASSED,
                check_type="RESULT_SIZE",
                message=f"é¢„ä¼°ç»“æœé›†å¤§å°åˆç†(çº¦{estimated_rows:,}è¡Œ)"
            ))
        
        # æ£€æŸ¥11: æŸ¥è¯¢å¤æ‚åº¦è¯„ä¼°
        complexity_score = self._calculate_query_complexity(
            metric, dimensions, filters
        )
        
        if complexity_score > 0.8:
            results.append(ValidationResult(
                status=ValidationStatus.WARNING,
                check_type="QUERY_COMPLEXITY",
                message=f"æŸ¥è¯¢å¤æ‚åº¦è¾ƒé«˜(å¾—åˆ†{complexity_score:.2f})",
                suggestion="å¯èƒ½å¯¼è‡´æŸ¥è¯¢è¶…æ—¶,å»ºè®®ç®€åŒ–æŸ¥è¯¢æ¡ä»¶"
            ))
        
        return results
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _get_metric_metadata(self, metric_id: str) -> Dict:
        """ä»å›¾è°±è·å–æŒ‡æ ‡å…ƒæ•°æ®"""
        result = self.graph.run("""
            MATCH (m:Metric {metricId: $metricId})
            RETURN m
        """, {"metricId": metric_id}).single()
        
        return dict(result['m']) if result else None
    
    def _get_supported_dimensions(self, metric_id: str) -> List[str]:
        """è·å–æŒ‡æ ‡æ”¯æŒçš„ç»´åº¦"""
        result = self.graph.run("""
            MATCH (m:Metric {metricId: $metricId})
                  -[:hasDimension]->(d:Dimension)
            RETURN collect(d.dimensionName) as dimensions
        """, {"metricId": metric_id}).single()
        
        return result['dimensions'] if result else []
    
    def _check_dimension_conflicts(
        self, 
        dimensions: List[str]
    ) -> List[Tuple[str, str]]:
        """æ£€æŸ¥ç»´åº¦å†²çª"""
        # å®šä¹‰å†²çªè§„åˆ™
        conflict_rules = {
            ('æ—¶é—´', 'æ—¥æœŸ'): "æ—¶é—´å’Œæ—¥æœŸç»´åº¦é‡å¤",
            ('çœä»½', 'åŸå¸‚'): "çœä»½å’ŒåŸå¸‚å­˜åœ¨å±‚çº§å…³ç³»,å»ºè®®åªé€‰ä¸€ä¸ª"
        }
        
        conflicts = []
        for (dim1, dim2), reason in conflict_rules.items():
            if dim1 in dimensions and dim2 in dimensions:
                conflicts.append((dim1, dim2))
        
        return conflicts
    
    def _check_cardinality_explosion(
        self,
        metric: Dict,
        dimensions: List[str]
    ) -> ValidationResult:
        """æ£€æŸ¥ç»´åº¦åŸºæ•°çˆ†ç‚¸"""
        # è·å–æ¯ä¸ªç»´åº¦çš„åŸºæ•°
        cardinalities = {}
        for dim in dimensions:
            cardinality = self._get_dimension_cardinality(dim)
            cardinalities[dim] = cardinality
        
        # è®¡ç®—æ€»åŸºæ•°(ä¹˜ç§¯)
        total_cardinality = 1
        for c in cardinalities.values():
            total_cardinality *= c
        
        threshold = self.config.get('cardinalityThreshold', 1000000)
        
        if total_cardinality > threshold:
            return ValidationResult(
                status=ValidationStatus.WARNING,
                check_type="CARDINALITY_EXPLOSION",
                message=f"ç»´åº¦ç»„åˆåŸºæ•°è¿‡å¤§(çº¦{total_cardinality:,})",
                suggestion="å»ºè®®å‡å°‘ç»´åº¦æˆ–å¢åŠ è¿‡æ»¤æ¡ä»¶",
                details={'cardinalities': cardinalities}
            )
        
        return None
    
    def _get_dimension_cardinality(self, dimension_name: str) -> int:
        """è·å–ç»´åº¦åŸºæ•°"""
        result = self.graph.run("""
            MATCH (d:Dimension {dimensionName: $dimensionName})
            RETURN d.cardinality as cardinality
        """, {"dimensionName": dimension_name}).single()
        
        return result['cardinality'] if result else 100  # é»˜è®¤å€¼
    
    def _check_business_rule(
        self,
        rule: Dict,
        filters: List[Dict],
        query_context: Dict
    ) -> ValidationResult:
        """æ£€æŸ¥ä¸šåŠ¡è§„åˆ™"""
        # ç¤ºä¾‹: æ£€æŸ¥"GMVå¿…é¡»æŒ‰åœ°åŒºç»´åº¦æŸ¥è¯¢"
        if rule.get('type') == 'REQUIRED_DIMENSION':
            required_dim = rule.get('dimension')
            if required_dim not in [f.get('dimension') for f in filters]:
                return ValidationResult(
                    status=ValidationStatus.FAILED,
                    check_type="BUSINESS_RULE",
                    message=f"ä¸šåŠ¡è§„åˆ™è¦æ±‚: å¿…é¡»åŒ…å«'{required_dim}'ç»´åº¦",
                    suggestion=f"è¯·æ·»åŠ {required_dim}ç»´åº¦"
                )
        
        return None
    
    def _check_metric_permission(self, metric_id: str, user_id: str) -> bool:
        """æ£€æŸ¥æŒ‡æ ‡æƒé™"""
        # æŸ¥è¯¢æƒé™å›¾è°±
        result = self.graph.run("""
            MATCH (u:User {userId: $userId})
                  -[:hasPermission]->(m:Metric {metricId: $metricId})
            RETURN count(*) > 0 as hasPermission
        """, {"userId": user_id, "metricId": metric_id}).single()
        
        return result['hasPermission'] if result else False
    
    def _check_dimension_permission(self, user_id: str, dimension: str) -> bool:
        """æ£€æŸ¥ç»´åº¦æƒé™"""
        # ç®€åŒ–å®ç°,å®é™…å¯èƒ½æ›´å¤æ‚
        return True
    
    def _parse_time_range(self, time_range_str: str) -> int:
        """è§£ææ—¶é—´èŒƒå›´,è¿”å›å¤©æ•°"""
        # ç®€åŒ–å®ç°
        if 'to' in time_range_str:
            start, end = time_range_str.split('to')
            start_date = datetime.fromisoformat(start.strip())
            end_date = datetime.fromisoformat(end.strip())
            return (end_date - start_date).days
        return 7  # é»˜è®¤7å¤©
    
    def _estimate_result_size(
        self,
        metric: Dict,
        dimensions: List[str],
        filters: List[Dict]
    ) -> int:
        """é¢„ä¼°ç»“æœé›†å¤§å°"""
        # åŸºç¡€è¡Œæ•°(æŒ‡æ ‡çš„å†å²æ•°æ®é‡)
        base_rows = metric.get('historicalDataSize', 10000)
        
        # ç»´åº¦åŸºæ•°ä¹˜ç§¯
        dim_cardinality = 1
        for dim in dimensions:
            dim_cardinality *= self._get_dimension_cardinality(dim)
        
        # è¿‡æ»¤æ¡ä»¶çš„é€‰æ‹©æ€§
        selectivity = 1.0
        for f in filters:
            selectivity *= 0.1  # å‡è®¾æ¯ä¸ªè¿‡æ»¤æ¡ä»¶å‡å°‘90%æ•°æ®
        
        estimated = int(base_rows * dim_cardinality * selectivity)
        
        return max(estimated, 1)
    
    def _calculate_query_complexity(
        self,
        metric: Dict,
        dimensions: List[str],
        filters: List[Dict]
    ) -> float:
        """è®¡ç®—æŸ¥è¯¢å¤æ‚åº¦å¾—åˆ† [0, 1]"""
        complexity = 0.0
        
        # ç»´åº¦æ•°é‡è´¡çŒ®
        complexity += min(len(dimensions) * 0.1, 0.3)
        
        # æŒ‡æ ‡è®¡ç®—å¤æ‚åº¦
        if metric.get('metricType') == 'DerivedMetric':
            complexity += 0.2
        
        # æ—¶é—´èŒƒå›´è´¡çŒ®
        time_filter = next((f for f in filters if f.get('dimension') == 'æ—¶é—´'), None)
        if time_filter:
            time_range_days = self._parse_time_range(time_filter.get('value', ''))
            complexity += min(time_range_days / 365, 0.3)
        
        # è¿‡æ»¤æ¡ä»¶å¤æ‚åº¦
        complexity += min(len(filters) * 0.05, 0.2)
        
        return min(complexity, 1.0)
    
    def _summarize_results(
        self,
        results: List[ValidationResult]
    ) -> Dict[str, Any]:
        """æ±‡æ€»éªŒè¯ç»“æœ"""
        # ç¡®å®šæœ€ç»ˆçŠ¶æ€
        has_failed = any(r.status == ValidationStatus.FAILED for r in results)
        has_warning = any(r.status == ValidationStatus.WARNING for r in results)
        
        if has_failed:
            final_status = "FAILED"
            can_proceed = False
        elif has_warning:
            final_status = "WARNING"
            can_proceed = True
        else:
            final_status = "PASSED"
            can_proceed = True
        
        # æ”¶é›†å»ºè®®
        suggestions = [r.suggestion for r in results if r.suggestion]
        
        return {
            'validationResult': final_status,
            'checks': [
                {
                    'checkType': r.check_type,
                    'status': r.status.value,
                    'message': r.message,
                    'suggestion': r.suggestion,
                    'details': r.details
                }
                for r in results
            ],
            'suggestions': suggestions,
            'canProceed': can_proceed
        }
    
    def _build_error_response(self, error_message: str) -> Dict:
        """æ„å»ºé”™è¯¯å“åº”"""
        return {
            'validationResult': 'FAILED',
            'checks': [{
                'checkType': 'SYSTEM_ERROR',
                'status': 'FAILED',
                'message': error_message
            }],
            'suggestions': [],
            'canProceed': False
        }


# ä½¿ç”¨ç¤ºä¾‹
def example_validator_usage():
    from neo4j import GraphDatabase
    
    # åˆå§‹åŒ–
    graph_client = GraphDatabase.driver("bolt://localhost:7687")
    config = {
        'maxTimeRangeDays': 365,
        'maxResultRows': 1000000,
        'cardinalityThreshold': 1000000
    }
    
    validator = OntologyValidator(graph_client, config)
    
    # æ‰§è¡ŒéªŒè¯
    result = validator.validate(
        metric_id="metric_001",
        dimensions=["æ—¶é—´", "åœ°åŒº"],
        filters=[
            {"dimension": "æ—¶é—´", "value": "2026-01-01 to 2026-02-03"},
            {"dimension": "åœ°åŒº", "value": "åä¸œ"}
        ],
        user_id="user_001",
        query_context={"timeGranularity": "day"}
    )
    
    print("éªŒè¯ç»“æœ:")
    print(f"çŠ¶æ€: {result['validationResult']}")
    print(f"å¯ä»¥ç»§ç»­: {result['canProceed']}")
    print("
æ£€æŸ¥è¯¦æƒ…:")
    for check in result['checks']:
        print(f"  [{check['status']}] {check['checkType']}: {check['message']}")
    
    if result['suggestions']:
        print("
å»ºè®®:")
        for suggestion in result['suggestions']:
            print(f"  - {suggestion}")
```

---

## å››ã€æ ¸å¿ƒæŠ€æœ¯ 3: è¯­ä¹‰æ¨ç†å¼•æ“ (Semantic Reasoning Engine)

### 4.1 æŠ€æœ¯æ¶æ„

```plaintext
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         è¯­ä¹‰æ¨ç†å¼•æ“ (Semantic Reasoning Engine)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  è¾“å…¥: æŸ¥è¯¢æ„å›¾ + å®ä½“ + å›¾è°±ä¸Šä¸‹æ–‡                       â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  æ¨ç†ç±»å‹1: ä¼ é€’æ€§æ¨ç† (Transitive Reasoning)   â”‚    â”‚
â”‚  â”‚  - å…³ç³»ä¼ é€’: Aâ†’B, Bâ†’C â‡’ Aâ†’C                    â”‚    â”‚
â”‚  â”‚  - åº”ç”¨åœºæ™¯: æŒ‡æ ‡è¡€ç¼˜è¿½æº¯ã€ä¸Šä¸‹æ¸¸åˆ†æ            â”‚    â”‚
â”‚  â”‚  - ç®—æ³•: å›¾éå† + è·¯å¾„èšåˆ                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  æ¨ç†ç±»å‹2: å› æœæ¨ç† (Causal Reasoning)         â”‚    â”‚
â”‚  â”‚  - å› æœé“¾åˆ†æ: GMV â† è®¢å•é‡ â† æµé‡              â”‚    â”‚
â”‚  â”‚  - åº”ç”¨åœºæ™¯: æ ¹å› åˆ†æã€å½±å“å› å­åˆ†æ              â”‚    â”‚
â”‚  â”‚  - ç®—æ³•: å› æœå›¾éå† + å¼ºåº¦åŠ æƒ                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  æ¨ç†ç±»å‹3: ç»§æ‰¿æ¨ç† (Inheritance Reasoning)    â”‚    â”‚
â”‚  â”‚  - ç±»å±‚æ¬¡æ¨ç†: DerivedMetric âŠ† Metric          â”‚    â”‚
â”‚  â”‚  - åº”ç”¨åœºæ™¯: æŒ‡æ ‡åˆ†ç±»ã€å±æ€§ç»§æ‰¿                  â”‚    â”‚
â”‚  â”‚  - ç®—æ³•: æœ¬ä½“å±‚æ¬¡éå†                            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  æ¨ç†ç±»å‹4: å…³è”æ¨ç† (Association Reasoning)    â”‚    â”‚
â”‚  â”‚  - ç›¸å…³æŒ‡æ ‡å‘ç°: GMV â†” å®¢å•ä»· â†” è®¢å•é‡          â”‚    â”‚
â”‚  â”‚  - åº”ç”¨åœºæ™¯: æŒ‡æ ‡æ¨èã€å…³è”åˆ†æ                  â”‚    â”‚
â”‚  â”‚  - ç®—æ³•: ååŒè¿‡æ»¤ + å›¾å…³è”åº¦è®¡ç®—                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                          â”‚
â”‚  è¾“å‡º: æ¨ç†ç»“æœ + æ¨ç†è·¯å¾„ + ç½®ä¿¡åº¦ + å¯è§£é‡Šæ€§è¯´æ˜        â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.2 è¯¦ç»†å®ç°

```python
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict
import heapq

@dataclass
class ReasoningPath:
    """æ¨ç†è·¯å¾„"""
    nodes: List[str]          # èŠ‚ç‚¹åºåˆ—
    relations: List[str]      # å…³ç³»åºåˆ—
    confidence: float         # ç½®ä¿¡åº¦
    explanation: str          # å¯è§£é‡Šæ€§è¯´æ˜

class SemanticReasoningEngine:
    """
    è¯­ä¹‰æ¨ç†å¼•æ“
    åŸºäºå›¾è°±è¿›è¡Œè¯­ä¹‰æ¨ç†
    """
    
    def __init__(self, graph_client, config=None):
        """
        Args:
            graph_client: Neo4jå®¢æˆ·ç«¯
            config: é…ç½®(æœ€å¤§æ¨ç†æ·±åº¦ç­‰)
        """
        self.graph = graph_client
        self.config = config or {'maxDepth': 3, 'minConfidence': 0.5}
    
    def reason(
        self,
        query_intent: str,
        entities: List[Dict],
        context: Dict = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œè¯­ä¹‰æ¨ç†
        
        Args:
            query_intent: æŸ¥è¯¢æ„å›¾ (METRIC_QUERY, ROOT_CAUSE_ANALYSIS, etc.)
            entities: è¯†åˆ«å‡ºçš„å®ä½“åˆ—è¡¨
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
        
        Returns:
            {
                'reasoningType': str,
                'results': [...],
                'paths': [ReasoningPath, ...],
                'explanation': str
            }
        """
        # æ ¹æ®æ„å›¾é€‰æ‹©æ¨ç†ç±»å‹
        if query_intent == 'ROOT_CAUSE_ANALYSIS':
            return self.causal_reasoning(entities, context)
        
        elif query_intent == 'METRIC_LINEAGE':
            return self.transitive_reasoning(entities, 'derivedFrom')
        
        elif query_intent == 'METRIC_RECOMMENDATION':
            return self.association_reasoning(entities, context)
        
        elif query_intent == 'METRIC_CLASSIFICATION':
            return self.inheritance_reasoning(entities)
        
        else:
            # é»˜è®¤: å®ä½“æ‰©å±•æ¨ç†
            return self.entity_expansion_reasoning(entities)
    
    # ========== æ¨ç†ç±»å‹1: ä¼ é€’æ€§æ¨ç† ==========
    
    def transitive_reasoning(
        self,
        entities: List[Dict],
        relation_type: str,
        max_depth: int = None
    ) -> Dict[str, Any]:
        """
        ä¼ é€’æ€§æ¨ç†
        
        åº”ç”¨åœºæ™¯: æŒ‡æ ‡è¡€ç¼˜è¿½æº¯ã€ä¸Šä¸‹æ¸¸åˆ†æ
        
        Args:
            entities: èµ·å§‹å®ä½“
            relation_type: å…³ç³»ç±»å‹ (derivedFrom, dependsOn, etc.)
            max_depth: æœ€å¤§æ¨ç†æ·±åº¦
        
        Returns:
            æ¨ç†ç»“æœ
        """
        max_depth = max_depth or self.config['maxDepth']
        
        results = []
        all_paths = []
        
        for entity in entities:
            if entity['type'] != 'Metric':
                continue
            
            # æŸ¥è¯¢ä¼ é€’é—­åŒ…
            cypher = f"""
                MATCH path = (start:Metric {{metricCode: $entityValue}})
                             -[:{relation_type}*1..{max_depth}]->(end:Metric)
                RETURN path,
                       [node in nodes(path) | node.metricName] as nodeNames,
                       [rel in relationships(path) | type(rel)] as relTypes,
                       reduce(conf=1.0, rel in relationships(path) | 
                              conf * coalesce(rel.confidence, 0.8)) as pathConfidence
                ORDER BY pathConfidence DESC
                LIMIT 20
            """
            
            query_result = self.graph.run(cypher, {
                "entityValue": entity['value']
            })
            
            for record in query_result:
                path_nodes = record['nodeNames']
                path_rels = record['relTypes']
                confidence = record['pathConfidence']
                
                # æ„å»ºæ¨ç†è·¯å¾„
                reasoning_path = ReasoningPath(
                    nodes=path_nodes,
                    relations=path_rels,
                    confidence=confidence,
                    explanation=self._build_transitive_explanation(
                        path_nodes, path_rels, relation_type
                    )
                )
                
                all_paths.append(reasoning_path)
                
                # æ·»åŠ ç»ˆç‚¹èŠ‚ç‚¹åˆ°ç»“æœ
                end_node = path_nodes[-1]
                if end_node not in [r['metricName'] for r in results]:
                    results.append({
                        'metricName': end_node,
                        'relationPath': ' â†’ '.join(path_nodes),
                        'confidence': confidence
                    })
        
        return {
            'reasoningType': 'TRANSITIVE',
            'relation': relation_type,
            'results': results,
            'paths': [self._path_to_dict(p) for p in all_paths],
            'explanation': f"é€šè¿‡'{relation_type}'å…³ç³»è¿›è¡Œä¼ é€’æ€§æ¨ç†,å‘ç°{len(results)}ä¸ªç›¸å…³æŒ‡æ ‡"
        }
    
    def _build_transitive_explanation(
        self,
        nodes: List[str],
        relations: List[str],
        relation_type: str
    ) -> str:
        """æ„å»ºä¼ é€’æ€§æ¨ç†çš„è§£é‡Š"""
        steps = []
        for i in range(len(relations)):
            steps.append(f"{nodes[i]} -{relation_type}â†’ {nodes[i+1]}")
        
        return " â†’ ".join(steps)
    
    # ========== æ¨ç†ç±»å‹2: å› æœæ¨ç† ==========
    
    def causal_reasoning(
        self,
        entities: List[Dict],
        context: Dict = None
    ) -> Dict[str, Any]:
        """
        å› æœæ¨ç†
        
        åº”ç”¨åœºæ™¯: æ ¹å› åˆ†æã€å½±å“å› å­åˆ†æ
        
        Args:
            entities: ç›®æ ‡å®ä½“(è¢«è§£é‡Šçš„æŒ‡æ ‡)
            context: ä¸Šä¸‹æ–‡(åŒ…å«å˜åŒ–ä¿¡æ¯)
        
        Returns:
            å› æœæ¨ç†ç»“æœ
        """
        target_entity = entities[0]  # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯ç›®æ ‡æŒ‡æ ‡
        
        # æŸ¥è¯¢å› æœå›¾è°±
        cypher = """
            MATCH path = (target:Metric {metricCode: $targetCode})
                         <-[:causedBy*1..3]-(cause:Metric)
            WITH path,
                 [node in nodes(path) | node.metricName] as nodeNames,
                 reduce(strength=1.0, rel in relationships(path) | 
                        strength * rel.strength) as pathStrength
            WHERE pathStrength > $minStrength
            RETURN nodeNames, pathStrength,
                   cause.metricCode as causeCode,
                   cause.metricName as causeName
            ORDER BY pathStrength DESC
            LIMIT 10
        """
        
        result = self.graph.run(cypher, {
            "targetCode": target_entity['value'],
            "minStrength": self.config.get('minConfidence', 0.5)
        })
        
        causal_factors = []
        paths = []
        
        for record in result:
            cause_name = record['causeName']
            path_strength = record['pathStrength']
            node_names = record['nodeNames']
            
            # è·å–è¯¥å› æœå› å­çš„å®é™…å˜åŒ–æ•°æ®
            change_rate = self._get_metric_change_rate(
                record['causeCode'],
                context
            )
            
            # è®¡ç®—å½±å“å¾—åˆ†
            impact_score = path_strength * abs(change_rate)
            
            causal_factors.append({
                'causeName': cause_name,
                'causalStrength': path_strength,
                'changeRate': change_rate,
                'impactScore': impact_score,
                'causalPath': ' â† '.join(reversed(node_names))
            })
            
            # æ„å»ºæ¨ç†è·¯å¾„
            paths.append(ReasoningPath(
                nodes=list(reversed(node_names)),
                relations=['causedBy'] * (len(node_names) - 1),
                confidence=path_strength,
                explanation=f"{cause_name}é€šè¿‡å› æœé“¾å½±å“{target_entity['value']},å¼ºåº¦{path_strength:.2f}"
            ))
        
        # æŒ‰å½±å“å¾—åˆ†æ’åº
        causal_factors.sort(key=lambda x: x['impactScore'], reverse=True)
        
        # ç”Ÿæˆè§£é‡Š
        if causal_factors:
            top_cause = causal_factors[0]
            explanation = (
                f"{target_entity['value']}çš„ä¸»è¦å½±å“å› ç´ æ˜¯{top_cause['causeName']}"
                f"(å› æœå¼ºåº¦{top_cause['causalStrength']:.2f}, "
                f"å˜åŒ–ç‡{top_cause['changeRate']:.1%})"
            )
        else:
            explanation = f"æœªæ‰¾åˆ°{target_entity['value']}çš„æ˜æ˜¾å› æœå› ç´ "
        
        return {
            'reasoningType': 'CAUSAL',
            'target': target_entity['value'],
            'causalFactors': causal_factors,
            'paths': [self._path_to_dict(p) for p in paths],
            'explanation': explanation
        }
    
    def _get_metric_change_rate(
        self,
        metric_code: str,
        context: Dict
    ) -> float:
        """
        è·å–æŒ‡æ ‡çš„å˜åŒ–ç‡
        
        å®é™…åº”ç”¨ä¸­éœ€è¦æŸ¥è¯¢æ•°ä»“
        è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        """
        # æ¨¡æ‹Ÿ: ä»ä¸Šä¸‹æ–‡æˆ–æ•°ä»“è·å–å˜åŒ–ç‡
        mock_changes = {
            'GMV': -0.085,
            'è®¢å•é‡': -0.123,
            'å®¢å•ä»·': 0.042,
            'æµé‡': -0.108
        }
        
        return mock_changes.get(metric_code, 0.0)
    
    # ========== æ¨ç†ç±»å‹3: ç»§æ‰¿æ¨ç† ==========
    
    def inheritance_reasoning(
        self,
        entities: List[Dict]
    ) -> Dict[str, Any]:
        """
        ç»§æ‰¿æ¨ç†
        
        åº”ç”¨åœºæ™¯: æŒ‡æ ‡åˆ†ç±»ã€å±æ€§ç»§æ‰¿
        
        Args:
            entities: æŸ¥è¯¢å®ä½“
        
        Returns:
            ç»§æ‰¿æ¨ç†ç»“æœ
        """
        entity = entities[0]
        
        # æŸ¥è¯¢ç±»å±‚æ¬¡ç»“æ„
        cypher = """
            MATCH path = (specific:MetricType)
                         -[:subClassOf*0..5]->(general:MetricType)
            WHERE specific.typeName = $typeName
            RETURN [node in nodes(path) | node.typeName] as hierarchy,
                   general.typeName as generalType,
                   general.properties as inheritedProperties
            ORDER BY length(path) DESC
        """
        
        result = self.graph.run(cypher, {
            "typeName": entity.get('metricType', 'Metric')
        })
        
        hierarchies = []
        inherited_props = {}
        
        for record in result:
            hierarchy = record['hierarchy']
            general_type = record['generalType']
            properties = record['inheritedProperties'] or {}
            
            hierarchies.append({
                'hierarchy': ' â†’ '.join(hierarchy),
                'generalType': general_type
            })
            
            # æ”¶é›†ç»§æ‰¿çš„å±æ€§
            inherited_props.update(properties)
        
        return {
            'reasoningType': 'INHERITANCE',
            'entity': entity['value'],
            'hierarchies': hierarchies,
            'inheritedProperties': inherited_props,
            'explanation': f"{entity['value']}ç»§æ‰¿äº†{len(inherited_props)}ä¸ªå±æ€§"
        }
    
    # ========== æ¨ç†ç±»å‹4: å…³è”æ¨ç† ==========
    
    def association_reasoning(
        self,
        entities: List[Dict],
        context: Dict = None
    ) -> Dict[str, Any]:
        """
        å…³è”æ¨ç†
        
        åº”ç”¨åœºæ™¯: æŒ‡æ ‡æ¨èã€å…³è”åˆ†æ
        
        Args:
            entities: æŸ¥è¯¢å®ä½“
            context: ä¸Šä¸‹æ–‡
        
        Returns:
            å…³è”æ¨ç†ç»“æœ
        """
        entity = entities[0]
        
        # æŸ¥è¯¢å…³è”æŒ‡æ ‡
        cypher = """
            MATCH (m:Metric {metricCode: $metricCode})
                  -[r:correlatesWith|relatedTo]-(related:Metric)
            RETURN related.metricCode as relatedCode,
                   related.metricName as relatedName,
                   type(r) as relationType,
                   r.score as associationScore
            ORDER BY associationScore DESC
            LIMIT 10
        """
        
        result = self.graph.run(cypher, {
            "metricCode": entity['value']
        })
        
        associations = []
        
        for record in result:
            associations.append({
                'relatedMetric': record['relatedName'],
                'relationType': record['relationType'],
                'associationScore': record['associationScore'],
                'explanation': f"ä¸{entity['value']}å­˜åœ¨{record['relationType']}å…³ç³»"
            })
        
        return {
            'reasoningType': 'ASSOCIATION',
            'sourceMetric': entity['value'],
            'associations': associations,
            'explanation': f"å‘ç°{len(associations)}ä¸ªä¸{entity['value']}ç›¸å…³çš„æŒ‡æ ‡"
        }
    
    # ========== å®ä½“æ‰©å±•æ¨ç† ==========
    
    def entity_expansion_reasoning(
        self,
        entities: List[Dict]
    ) -> Dict[str, Any]:
        """
        å®ä½“æ‰©å±•æ¨ç†
        
        é€šè¿‡åŒä¹‰è¯ã€åˆ«åç­‰æ‰©å±•å®ä½“
        
        Args:
            entities: åŸå§‹å®ä½“
        
        Returns:
            æ‰©å±•åçš„å®ä½“
        """
        expanded = []
        
        for entity in entities:
            # æŸ¥è¯¢åŒä¹‰å®ä½“
            cypher = """
                MATCH (e {name: $entityName})
                      -[:sameAs|aliasOf]-(synonym)
                RETURN synonym.name as synonymName,
                       type(synonym) as synonymType
            """
            
            result = self.graph.run(cypher, {
                "entityName": entity['value']
            })
            
            synonyms = [record['synonymName'] for record in result]
            
            expanded.append({
                'original': entity['value'],
                'type': entity['type'],
                'synonyms': synonyms,
                'expanded': [entity['value']] + synonyms
            })
        
        return {
            'reasoningType': 'ENTITY_EXPANSION',
            'expandedEntities': expanded,
            'explanation': f"é€šè¿‡åŒä¹‰å…³ç³»æ‰©å±•äº†{len(expanded)}ä¸ªå®ä½“"
        }
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _path_to_dict(self, path: ReasoningPath) -> Dict:
        """å°†æ¨ç†è·¯å¾„è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'nodes': path.nodes,
            'relations': path.relations,
            'confidence': path.confidence,
            'explanation': path.explanation
        }


# ä½¿ç”¨ç¤ºä¾‹
def example_reasoning_usage():
    from neo4j import GraphDatabase
    
    # åˆå§‹åŒ–
    graph_client = GraphDatabase.driver("bolt://localhost:7687")
    engine = SemanticReasoningEngine(graph_client)
    
    # åœºæ™¯1: æ ¹å› åˆ†æ
    print("=== åœºæ™¯1: æ ¹å› åˆ†æ ===")
    result = engine.reason(
        query_intent='ROOT_CAUSE_ANALYSIS',
        entities=[{'type': 'Metric', 'value': 'GMV'}],
        context={'timeRange': '2026-02-03'}
    )
    
    print(f"æ¨ç†ç±»å‹: {result['reasoningType']}")
    print(f"è§£é‡Š: {result['explanation']}")
    print("
å› æœå› ç´ :")
    for factor in result['causalFactors'][:3]:
        print(f"  {factor['causeName']}: "
              f"å½±å“å¾—åˆ†{factor['impactScore']:.3f}, "
              f"å˜åŒ–ç‡{factor['changeRate']:.1%}")
    
    # åœºæ™¯2: æŒ‡æ ‡è¡€ç¼˜è¿½æº¯
    print("
=== åœºæ™¯2: æŒ‡æ ‡è¡€ç¼˜è¿½æº¯ ===")
    result = engine.transitive_reasoning(
        entities=[{'type': 'Metric', 'value': 'GMV'}],
        relation_type='derivedFrom',
        max_depth=3
    )
    
    print(f"å‘ç°{len(result['results'])}ä¸ªä¸Šæ¸¸æŒ‡æ ‡:")
    for r in result['results'][:5]:
        print(f"  {r['relationPath']} (ç½®ä¿¡åº¦{r['confidence']:.2f})")
```

---

## äº”ã€å¯æµ‹è¯•çš„ MVP å®ç°

### 5.1 MVP æ¶æ„

```python
"""
æ··åˆæ–¹æ¡ˆMVP - æœ€å°å¯æµ‹è¯•ç‰ˆæœ¬
æ•´åˆä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯: Rerank + Validator + Reasoning
"""

import numpy as np
from typing import List, Dict, Any
import time
import json

class HybridSemanticMVP:
    """
    æ··åˆè¯­ä¹‰æ–¹æ¡ˆMVP
    æ•´åˆå‘é‡å¬å›ã€å›¾è°±å¬å›ã€è¯­ä¹‰èåˆã€éªŒè¯ã€æ¨ç†
    """
    
    def __init__(self):
        """åˆå§‹åŒ–MVP"""
        # æ ¸å¿ƒç»„ä»¶
        self.rerank_engine = SemanticFusionRerankEngine(mode='rule')
        self.validator = None  # éœ€è¦Neo4jè¿æ¥
        self.reasoning_engine = None  # éœ€è¦Neo4jè¿æ¥
        
        # æ¨¡æ‹Ÿæ•°æ®(ç”¨äºæµ‹è¯•)
        self.mock_data = self._init_mock_data()
    
    def query(
        self,
        query_text: str,
        user_id: str = "test_user",
        top_k: int = 10
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€æŸ¥è¯¢æ¥å£
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            top_k: è¿”å›Top-Kç»“æœ
        
        Returns:
            å®Œæ•´çš„æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()
        
        print(f"
{'='*60}")
        print(f"ğŸ” æŸ¥è¯¢: {query_text}")
        print(f"{'='*60}
")
        
        # Step 1: æ„å›¾è¯†åˆ«ä¸å®ä½“æŠ½å–
        print("ğŸ“‹ Step 1: æ„å›¾è¯†åˆ«...")
        intent_result = self._mock_intent_recognition(query_text)
        print(f"  âœ“ æ„å›¾: {intent_result['intent']}")
        print(f"  âœ“ å®ä½“: {[e['value'] for e in intent_result['entities']]}")
        
        # Step 2: åŒè·¯å¬å›
        print("
ğŸ“‹ Step 2: åŒè·¯å¬å›...")
        
        # 2a. å‘é‡å¬å›
        print("  â†’ å‘é‡å¬å›...")
        vector_candidates = self._mock_vector_recall(query_text, top_k=50)
        print(f"    âœ“ å¬å›{len(vector_candidates)}ä¸ªå€™é€‰")
        
        # 2b. å›¾è°±å¬å›
        print("  â†’ å›¾è°±å¬å›...")
        graph_candidates = self._mock_graph_recall(intent_result['entities'], top_k=30)
        print(f"    âœ“ å¬å›{len(graph_candidates)}ä¸ªå€™é€‰")
        
        # Step 3: è¯­ä¹‰èåˆä¸ç²¾æ’
        print("
ğŸ“‹ Step 3: è¯­ä¹‰èåˆä¸ç²¾æ’...")
        query_vector = np.random.rand(768)  # æ¨¡æ‹ŸæŸ¥è¯¢å‘é‡
        
        rerank_result = self.rerank_engine.fuse_and_rank(
            vector_candidates,
            graph_candidates,
            query_text,
            query_vector,
            intent_result,
            user_profile=None,
            top_k=top_k
        )
        
        print(f"  âœ“ èåˆå®Œæˆ,è€—æ—¶{rerank_result['executionTime']['total']}")
        print(f"  âœ“ Top-1: {rerank_result['rankedResults'][0]['candidate']['metricName']}")
        
        # Step 4: æœ¬ä½“éªŒè¯(æ¨¡æ‹Ÿ)
        print("
ğŸ“‹ Step 4: æœ¬ä½“éªŒè¯...")
        top_candidate = rerank_result['rankedResults'][0]['candidate']
        validation_result = self._mock_validation(
            top_candidate,
            intent_result
        )
        print(f"  âœ“ éªŒè¯çŠ¶æ€: {validation_result['validationResult']}")
        
        # Step 5: è¯­ä¹‰æ¨ç†(å¦‚æœéœ€è¦)
        reasoning_result = None
        if intent_result['intent'] == 'ROOT_CAUSE_ANALYSIS':
            print("
ğŸ“‹ Step 5: è¯­ä¹‰æ¨ç†(æ ¹å› åˆ†æ)...")
            reasoning_result = self._mock_reasoning(intent_result['entities'])
            print(f"  âœ“ å‘ç°{len(reasoning_result['causalFactors'])}ä¸ªå› æœå› ç´ ")
        
        total_time = (time.time() - start_time) * 1000
        
        # æ„å»ºæœ€ç»ˆå“åº”
        response = {
            'query': query_text,
            'intent': intent_result['intent'],
            'entities': intent_result['entities'],
            'results': rerank_result['rankedResults'][:top_k],
            'validation': validation_result,
            'reasoning': reasoning_result,
            'performance': {
                'totalTime': f"{total_time:.2f}ms",
                'breakdown': rerank_result['executionTime']
            }
        }
        
        print(f"
{'='*60}")
        print(f"âœ… æŸ¥è¯¢å®Œæˆ,æ€»è€—æ—¶: {total_time:.2f}ms")
        print(f"{'='*60}
")
        
        return response
    
    # ========== æ¨¡æ‹Ÿæ–¹æ³•(ç”¨äºæµ‹è¯•) ==========
    
    def _init_mock_data(self) -> Dict:
        """åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®"""
        return {
            'metrics': [
                {
                    'metricId': 'metric_001',
                    'metricName': 'GMV',
                    'metricCode': 'GMV',
                    'businessDomain': 'äº¤æ˜“åŸŸ',
                    'metricType': 'DerivedMetric',
                    'description': 'æˆäº¤æ€»é¢',
                    'synonyms': ['äº¤æ˜“é¢', 'æˆäº¤é‡‘é¢'],
                    'vector': np.random.rand(768),
                    'importance': 5,
                    'usageCount': 5000
                },
                {
                    'metricId': 'metric_002',
                    'metricName': 'è®¢å•é‡',
                    'metricCode': 'ORDER_COUNT',
                    'businessDomain': 'äº¤æ˜“åŸŸ',
                    'metricType': 'AtomicMetric',
                    'description': 'è®¢å•æ•°é‡',
                    'synonyms': ['è®¢å•æ•°'],
                    'vector': np.random.rand(768),
                    'importance': 4,
                    'usageCount': 3000
                },
                {
                    'metricId': 'metric_003',
                    'metricName': 'å®¢å•ä»·',
                    'metricCode': 'AVG_ORDER_VALUE',
                    'businessDomain': 'äº¤æ˜“åŸŸ',
                    'metricType': 'DerivedMetric',
                    'description': 'å¹³å‡è®¢å•é‡‘é¢',
                    'synonyms': ['å¹³å‡è®¢å•ä»·å€¼'],
                    'vector': np.random.rand(768),
                    'importance': 4,
                    'usageCount': 2500
                }
            ]
        }
    
    def _mock_intent_recognition(self, query: str) -> Dict:
        """æ¨¡æ‹Ÿæ„å›¾è¯†åˆ«"""
        if 'ä¸ºä»€ä¹ˆ' in query or 'ä¸‹é™' in query or 'ä¸Šå‡' in query:
            intent = 'ROOT_CAUSE_ANALYSIS'
        else:
            intent = 'METRIC_QUERY'
        
        # ç®€å•çš„å®ä½“è¯†åˆ«
        entities = []
        for metric in self.mock_data['metrics']:
            if metric['metricName'] in query or metric['metricCode'] in query:
                entities.append({
                    'type': 'Metric',
                    'value': metric['metricCode'],
                    'confidence': 0.9
                })
        
        return {
            'intent': intent,
            'entities': entities,
            'businessDomains': ['äº¤æ˜“åŸŸ']
        }
    
    def _mock_vector_recall(self, query: str, top_k: int) -> List[Dict]:
        """æ¨¡æ‹Ÿå‘é‡å¬å›"""
        candidates = []
        
        for metric in self.mock_data['metrics']:
            # æ¨¡æ‹Ÿç›¸ä¼¼åº¦è®¡ç®—
            similarity = np.random.uniform(0.7, 0.95)
            
            candidates.append({
                **metric,
                'similarity': similarity,
                'recallSource': 'VECTOR'
            })
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        candidates.sort(key=lambda x: x['similarity'], reverse=True)
        
        return candidates[:top_k]
    
    def _mock_graph_recall(self, entities: List[Dict], top_k: int) -> List[Dict]:
        """æ¨¡æ‹Ÿå›¾è°±å¬å›"""
        if not entities:
            return []
        
        candidates = []
        
        for metric in self.mock_data['metrics']:
            # æ¨¡æ‹Ÿå›¾è°±åŒ¹é…
            if any(e['value'] == metric['metricCode'] for e in entities):
                match_type = 'EXACT'
                confidence = 0.95
            else:
                match_type = 'RELATION'
                confidence = np.random.uniform(0.6, 0.85)
            
            candidates.append({
                **metric,
                'matchType': match_type,
                'confidence': confidence,
                'matchPath': [],
                'recallSource': 'GRAPH'
            })
        
        candidates.sort(key=lambda x: x['confidence'], reverse=True)
        
        return candidates[:top_k]
    
    def _mock_validation(self, candidate: Dict, intent_result: Dict) -> Dict:
        """æ¨¡æ‹ŸéªŒè¯"""
        return {
            'validationResult': 'PASSED',
            'checks': [
                {
                    'checkType': 'DIMENSION_COMPATIBILITY',
                    'status': 'PASSED',
                    'message': 'æŒ‡æ ‡æ”¯æŒæ‰€æœ‰è¯·æ±‚çš„ç»´åº¦'
                },
                {
                    'checkType': 'DATA_PERMISSION',
                    'status': 'PASSED',
                    'message': 'ç”¨æˆ·æœ‰æƒé™è®¿é—®è¯¥æŒ‡æ ‡'
                }
            ],
            'suggestions': [],
            'canProceed': True
        }
    
    def _mock_reasoning(self, entities: List[Dict]) -> Dict:
        """æ¨¡æ‹Ÿæ¨ç†"""
        return {
            'reasoningType': 'CAUSAL',
            'target': entities[0]['value'] if entities else 'GMV',
            'causalFactors': [
                {
                    'causeName': 'è®¢å•é‡',
                    'causalStrength': 0.85,
                    'changeRate': -0.123,
                    'impactScore': 0.105
                },
                {
                    'causeName': 'æµé‡',
                    'causalStrength': 0.72,
                    'changeRate': -0.108,
                    'impactScore': 0.078
                }
            ],
            'explanation': 'GMVçš„ä¸»è¦å½±å“å› ç´ æ˜¯è®¢å•é‡(å› æœå¼ºåº¦0.85, å˜åŒ–ç‡-12.3%)'
        }


# ========== MVPæµ‹è¯•ç”¨ä¾‹ ==========

def test_mvp():
    """æµ‹è¯•MVP"""
    mvp = HybridSemanticMVP()
    
    # æµ‹è¯•ç”¨ä¾‹1: ç®€å•æŒ‡æ ‡æŸ¥è¯¢
    print("
" + "="*80)
    print("æµ‹è¯•ç”¨ä¾‹1: ç®€å•æŒ‡æ ‡æŸ¥è¯¢")
    print("="*80)
    result1 = mvp.query("æœ€è¿‘7å¤©GMV")
    print_result_summary(result1)
    
    # æµ‹è¯•ç”¨ä¾‹2: æ ¹å› åˆ†æ
    print("
" + "="*80)
    print("æµ‹è¯•ç”¨ä¾‹2: æ ¹å› åˆ†æ")
    print("="*80)
    result2 = mvp.query("ä¸ºä»€ä¹ˆä»Šå¤©GMVä¸‹é™äº†?")
    print_result_summary(result2)
    
    # ä¿å­˜ç»“æœ
    with open('mvp_test_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'test1': result1,
            'test2': result2
        }, f, ensure_ascii=False, indent=2, default=str)
    
    print("
âœ… æµ‹è¯•å®Œæˆ,ç»“æœå·²ä¿å­˜åˆ° mvp_test_results.json")


def print_result_summary(result: Dict):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("
ğŸ“Š æŸ¥è¯¢ç»“æœæ‘˜è¦:")
    print(f"  æŸ¥è¯¢: {result['query']}")
    print(f"  æ„å›¾: {result['intent']}")
    print(f"  æ€»è€—æ—¶: {result['performance']['totalTime']}")
    
    print("
ğŸ¯ Top-3ç»“æœ:")
    for i, item in enumerate(result['results'][:3], 1):
        candidate = item['candidate']
        score = item['finalScore']
        print(f"  {i}. {candidate['metricName']} - å¾—åˆ†: {score:.3f}")
        print(f"     æ¥æº: {candidate.get('recallSource', 'N/A')}")
    
    if result.get('reasoning'):
        print("
ğŸ” æ¨ç†ç»“æœ:")
        reasoning = result['reasoning']
        print(f"  {reasoning['explanation']}")
        if reasoning.get('causalFactors'):
            print("  ä¸»è¦å› æœå› ç´ :")
            for factor in reasoning['causalFactors'][:3]:
                print(f"    - {factor['causeName']}: "
                      f"å½±å“å¾—åˆ†{factor['impactScore']:.3f}")


if __name__ == '__main__':
    # è¿è¡ŒMVPæµ‹è¯•
    test_mvp()
```

---

## å…­ã€MVP éƒ¨ç½²ä¸æµ‹è¯•æŒ‡å—

### 6.1 ç¯å¢ƒå‡†å¤‡

```bash
# 1. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows

# 2. å®‰è£…ä¾èµ–
pip install numpy xgboost scikit-learn neo4j pymilvus

# 3. (å¯é€‰) å¯åŠ¨Neo4jå’ŒMilvus
# Neo4j: docker run -p 7474:7474 -p 7687:7687 neo4j
# Milvus: å‚è€ƒå®˜æ–¹æ–‡æ¡£
```

### 6.2 è¿è¡Œ MVP

```python
# ä¿å­˜ä¸Šè¿°ä»£ç ä¸º hybrid_semantic_mvp.py

# è¿è¡Œæµ‹è¯•
python hybrid_semantic_mvp.py

# é¢„æœŸè¾“å‡º:
# - ä¸¤ä¸ªæµ‹è¯•ç”¨ä¾‹çš„å®Œæ•´æ‰§è¡Œæ—¥å¿—
# - æ€§èƒ½æŒ‡æ ‡(è€—æ—¶åˆ†è§£)
# - Top-Kç»“æœ
# - éªŒè¯ç»“æœ
# - æ¨ç†ç»“æœ(å¦‚æœé€‚ç”¨)
```

### 6.3 MVP åŠŸèƒ½æ¸…å•

âœ… **å·²å®ç°**ï¼š

- æ„å›¾è¯†åˆ«ï¼ˆæ¨¡æ‹Ÿï¼‰

- åŒè·¯å¬å›ï¼ˆå‘é‡+å›¾è°±ï¼Œæ¨¡æ‹Ÿï¼‰

- è¯­ä¹‰èåˆä¸ç²¾æ’ï¼ˆåŸºäºè§„åˆ™ï¼‰

- å¤šç»´ç‰¹å¾æå–ï¼ˆ11 ä¸ªç‰¹å¾ï¼‰

- æœ¬ä½“éªŒè¯ï¼ˆæ¨¡æ‹Ÿï¼‰

- è¯­ä¹‰æ¨ç†ï¼ˆæ¨¡æ‹Ÿï¼‰

- å®Œæ•´çš„æŸ¥è¯¢æµç¨‹

- æ€§èƒ½ç›‘æ§

ğŸ”„ **å¾…é›†æˆ**ï¼ˆéœ€è¦çœŸå®ç¯å¢ƒï¼‰:

- çœŸå®çš„å‘é‡æ•°æ®åº“ï¼ˆMilvusï¼‰

- çœŸå®çš„å›¾æ•°æ®åº“ï¼ˆNeo4jï¼‰

- çœŸå®çš„ embedding æ¨¡å‹

- Learning to Rank æ¨¡å‹è®­ç»ƒ

---

## ä¸ƒã€æ€»ç»“ä¸ä¸‹ä¸€æ­¥

### 7.1 æ ¸å¿ƒæŠ€æœ¯æ€»ç»“

| æŠ€æœ¯æ¨¡å—           | æ ¸å¿ƒéš¾ç‚¹               | è§£å†³æ–¹æ¡ˆ               | MVP çŠ¶æ€ |
| ------------------ | ---------------------- | ---------------------- | -------- |
| **è¯­ä¹‰èåˆä¸ç²¾æ’** | å¼‚æ„æ•°æ®èåˆã€ç‰¹å¾å·¥ç¨‹ | 11 ç»´ç‰¹å¾+è§„åˆ™æ‰“åˆ†/LTR | âœ… å®Œæˆ   |
| **æœ¬ä½“å›¾è°±éªŒè¯**   | ä¸šåŠ¡è§„åˆ™å¼•æ“ã€çº¦æŸéªŒè¯ | 4 å±‚éªŒè¯+11 é¡¹æ£€æŸ¥     | âœ… å®Œæˆ   |
| **è¯­ä¹‰æ¨ç†å¼•æ“**   | å›¾è°±æ¨ç†ç®—æ³•ã€å¯è§£é‡Šæ€§ | 4 ç±»æ¨ç†+è·¯å¾„è¿½æº¯      | âœ… å®Œæˆ   |

### 7.2 MVP ä»·å€¼

1. **å¯æµ‹è¯•**ï¼š æ— éœ€å¤–éƒ¨ä¾èµ–å³å¯è¿è¡Œ

2. **å¯æ‰©å±•**ï¼š æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºé›†æˆçœŸå®ç»„ä»¶

3. **å¯æ¼”ç¤º**ï¼š å®Œæ•´çš„ç«¯åˆ°ç«¯æµç¨‹

4. **å¯åº¦é‡**ï¼š è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

### 7.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**ç«‹å³å¯åš**ï¼š

1. âœ… è¿è¡Œ MVP æµ‹è¯•

2. âœ… ç†è§£æ ¸å¿ƒç®—æ³•

3. âœ… è°ƒæ•´ç‰¹å¾æƒé‡

**1 å‘¨å†…**ï¼š

1. é›†æˆçœŸå®çš„ Milvus å‘é‡åº“

2. é›†æˆçœŸå®çš„ Neo4j å›¾æ•°æ®åº“

3. å‡†å¤‡çœŸå®çš„æŒ‡æ ‡æ•°æ®

**2 å‘¨å†…**ï¼š

1. è®­ç»ƒ embedding æ¨¡å‹

2. æ”¶é›†æ ‡æ³¨æ•°æ®

3. è®­ç»ƒ LTR æ¨¡å‹

**1 ä¸ªæœˆå†…**ï¼š

1. å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•

2. æ€§èƒ½ä¼˜åŒ–

3. ä¸Šçº¿ MVP

---

## é™„å½•ï¼š å…³é”®ä»£ç ç‰‡æ®µç´¢å¼•

- **ç‰¹å¾æå–**ï¼š ç¬¬ 2.2 èŠ‚

- **è§„åˆ™æ‰“åˆ†**ï¼š ç¬¬ 2.3.1 èŠ‚

- **Learning to Rank**: ç¬¬ 2.3.2 èŠ‚

- **Rerank å¼•æ“**ï¼š ç¬¬ 2.4 èŠ‚

- **æœ¬ä½“éªŒè¯å™¨**ï¼š ç¬¬ 3.2 èŠ‚

- **è¯­ä¹‰æ¨ç†å¼•æ“**ï¼š ç¬¬ 4.2 èŠ‚

- **å®Œæ•´ MVP**: ç¬¬ 5.1 èŠ‚

- **æµ‹è¯•ç”¨ä¾‹**ï¼š ç¬¬ 5.1 èŠ‚æœ«å°¾