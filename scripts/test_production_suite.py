#!/usr/bin/env python3
"""
ç”Ÿäº§çº§ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶
æµ‹è¯•æ‰€æœ‰çœŸå®ç»„ä»¶:Vector Search, Graph Search, LLM, SQL Generation
"""
import sys
import os
import time
import json
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.recall.vector.qdrant_store import QdrantVectorStore
from src.recall.vector.vectorizer import MetricVectorizer
from src.recall.graph.graph_store import GraphStore
from src.inference.zhipu_intent import ZhipuIntentRecognizer
from src.mql.sql_generator_v2 import SQLGeneratorV2
from src.inference.intent import QueryIntent, TimeGranularity, AggregationType
from src.config.metric_loader import metric_loader

class ProductionTestSuite:
    """ç”Ÿäº§çº§æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.results = {
            "vector_search": [],
            "graph_search": [],
            "llm_intent": [],
            "sql_generation": [],
            "e2e_flow": []
        }
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
    
    def test_vector_search(self):
        """æµ‹è¯• Qdrant å‘é‡æ£€ç´¢ (çœŸå®)"""
        print("\n" + "="*80)
        print("ğŸ” TEST 1: Vector Search (Qdrant) - Production Component")
        print("="*80)
        
        test_cases = [
            {"query": "é”€å”®é¢", "expected_metric": "GMV", "min_score": 0.3},
            {"query": "æ—¥æ´»ç”¨æˆ·", "expected_metric": "DAU", "min_score": 0.3},
            {"query": "è®¢å•æ•°é‡", "expected_metric": "è®¢å•é‡", "min_score": 0.3},
            {"query": "ç”¨æˆ·ç•™å­˜", "expected_metric": "ç•™å­˜ç‡", "min_score": 0.2},
            {"query": "æŠ•èµ„å›æŠ¥", "expected_metric": "ROI", "min_score": 0.2},
        ]
        
        try:
            vector_store = QdrantVectorStore()
            vectorizer = MetricVectorizer()
            
            for i, case in enumerate(test_cases, 1):
                self.total_tests += 1
                print(f"\n  Test 1.{i}: Query='{case['query']}' -> Expected='{case['expected_metric']}'")
                
                query_vec = vectorizer.model.encode(case['query'], normalize_embeddings=True)
                results = vector_store.search(query_vec, top_k=3, score_threshold=0.1)
                
                if results:
                    top_result = results[0]
                    metric_name = top_result['payload']['name']
                    score = top_result['score']
                    
                    passed = (metric_name == case['expected_metric'] and score >= case['min_score'])
                    
                    print(f"    Result: {metric_name} (score={score:.4f})")
                    print(f"    Status: {'âœ… PASS' if passed else 'âŒ FAIL'}")
                    
                    if passed:
                        self.passed_tests += 1
                    else:
                        self.failed_tests += 1
                    
                    self.results["vector_search"].append({
                        "query": case['query'],
                        "expected": case['expected_metric'],
                        "actual": metric_name,
                        "score": score,
                        "passed": passed
                    })
                else:
                    print(f"    Status: âŒ FAIL (No results)")
                    self.failed_tests += 1
                    
        except Exception as e:
            print(f"  âŒ Vector Search Test Failed: {e}")
            self.failed_tests += len(test_cases)
    
    def test_graph_search(self):
        """æµ‹è¯• Neo4j å›¾è°±æ£€ç´¢ (çœŸå®)"""
        print("\n" + "="*80)
        print("ğŸ•¸ï¸  TEST 2: Graph Search (Neo4j) - Production Component")
        print("="*80)
        
        test_cases = [
            {"domain": "ç”µå•†", "min_metrics": 3},
            {"domain": "ç”¨æˆ·", "min_metrics": 3},
        ]
        
        try:
            graph_store = GraphStore()
            
            for i, case in enumerate(test_cases, 1):
                self.total_tests += 1
                print(f"\n  Test 2.{i}: Domain='{case['domain']}' -> Min Metrics={case['min_metrics']}")
                
                results = graph_store.search_by_domain(case['domain'])
                metric_count = len(results)
                
                passed = metric_count >= case['min_metrics']
                
                print(f"    Found: {metric_count} metrics")
                if results:
                    print(f"    Metrics: {[r['name'] for r in results[:5]]}")
                print(f"    Status: {'âœ… PASS' if passed else 'âŒ FAIL'}")
                
                if passed:
                    self.passed_tests += 1
                else:
                    self.failed_tests += 1
                
                self.results["graph_search"].append({
                    "domain": case['domain'],
                    "expected_min": case['min_metrics'],
                    "actual_count": metric_count,
                    "passed": passed
                })
                
            graph_store.close()
            
        except Exception as e:
            print(f"  âŒ Graph Search Test Failed: {e}")
            self.failed_tests += len(test_cases)
    
    def test_llm_intent(self):
        """æµ‹è¯• ZhipuAI LLM æ„å›¾è¯†åˆ« (çœŸå®)"""
        print("\n" + "="*80)
        print("ğŸ§  TEST 3: LLM Intent Recognition (ZhipuAI) - Production Component")
        print("="*80)
        
        test_cases = [
            {
                "query": "æœ¬æœˆæŒ‰æ¸ é“ç»Ÿè®¡DAU",
                "expected_dimensions": ["æ¸ é“"],
                "expected_time": "æœ¬æœˆ"
            },
            {
                "query": "æŒ‰åœ°åŒºçš„æˆäº¤é‡‘é¢åŒæ¯”",
                "expected_dimensions": ["åœ°åŒº"],
                "expected_comparison": "yoy"
            },
            {
                "query": "æœ€è¿‘7å¤©çš„GMV",
                "expected_time": "7",
                "expected_metric": "GMV"
            },
        ]
        
        try:
            llm_recognizer = ZhipuIntentRecognizer(model="glm-4-flash")
            
            for i, case in enumerate(test_cases, 1):
                self.total_tests += 1
                print(f"\n  Test 3.{i}: Query='{case['query']}'")
                
                result = llm_recognizer.recognize(case['query'])
                
                if result:
                    passed = True
                    
                    # Check dimensions
                    if "expected_dimensions" in case:
                        dims_match = set(result.dimensions) == set(case['expected_dimensions'])
                        passed = passed and dims_match
                        print(f"    Dimensions: {result.dimensions} (Expected: {case['expected_dimensions']}) {'âœ…' if dims_match else 'âŒ'}")
                    
                    # Check time
                    if "expected_time" in case and result.time_range:
                        time_desc = result.time_range.get('description', '') + result.time_range.get('value', '')
                        time_match = case['expected_time'] in time_desc
                        passed = passed and time_match
                        print(f"    Time: {result.time_range} {'âœ…' if time_match else 'âŒ'}")
                    
                    # Check comparison
                    if "expected_comparison" in case:
                        comp_match = result.comparison_type == case['expected_comparison']
                        passed = passed and comp_match
                        print(f"    Comparison: {result.comparison_type} {'âœ…' if comp_match else 'âŒ'}")
                    
                    print(f"    Confidence: {result.confidence}")
                    print(f"    Tokens: {result.tokens_used.get('total_tokens', 0)}")
                    print(f"    Status: {'âœ… PASS' if passed else 'âŒ FAIL'}")
                    
                    if passed:
                        self.passed_tests += 1
                    else:
                        self.failed_tests += 1
                    
                    self.results["llm_intent"].append({
                        "query": case['query'],
                        "result": {
                            "dimensions": result.dimensions,
                            "time_range": result.time_range,
                            "comparison": result.comparison_type
                        },
                        "passed": passed
                    })
                else:
                    print(f"    Status: âŒ FAIL (No result)")
                    self.failed_tests += 1
                    
        except Exception as e:
            print(f"  âŒ LLM Intent Test Failed: {e}")
            import traceback
            traceback.print_exc()
            self.failed_tests += len(test_cases)
    
    def test_sql_generation(self):
        """æµ‹è¯• SQL ç”Ÿæˆ (çœŸå®)"""
        print("\n" + "="*80)
        print("ğŸ“ TEST 4: SQL Generation - Production Component")
        print("="*80)
        
        test_cases = [
            {
                "name": "Simple Query",
                "intent": {
                    "query": "GMV",
                    "core_query": "GMV",
                    "time_range": (datetime(2026, 2, 1), datetime(2026, 2, 8)),
                    "time_granularity": TimeGranularity.DAY,
                    "aggregation_type": AggregationType.SUM,
                    "dimensions": [],
                    "comparison_type": None,
                    "filters": {}
                },
                "expected_keywords": ["SELECT", "FROM", "WHERE", "date"]
            },
            {
                "name": "Dimension Query",
                "intent": {
                    "query": "æŒ‰æ¸ é“ç»Ÿè®¡DAU",
                    "core_query": "DAU",
                    "time_range": (datetime(2026, 2, 1), datetime(2026, 2, 8)),
                    "time_granularity": TimeGranularity.DAY,
                    "aggregation_type": AggregationType.AVG,
                    "dimensions": ["æ¸ é“"],
                    "comparison_type": None,
                    "filters": {}
                },
                "expected_keywords": ["SELECT", "GROUP BY", "JOIN", "dim_channel"]
            },
        ]
        
        try:
            sql_generator = SQLGeneratorV2()
            
            for i, case in enumerate(test_cases, 1):
                self.total_tests += 1
                print(f"\n  Test 4.{i}: {case['name']}")
                
                query_intent = QueryIntent(**case['intent'])
                sql, params = sql_generator.generate(query_intent)
                
                passed = all(keyword in sql for keyword in case['expected_keywords'])
                
                print(f"    SQL Length: {len(sql)} chars")
                print(f"    Keywords Check: {case['expected_keywords']}")
                for keyword in case['expected_keywords']:
                    found = keyword in sql
                    print(f"      - {keyword}: {'âœ…' if found else 'âŒ'}")
                
                print(f"    Status: {'âœ… PASS' if passed else 'âŒ FAIL'}")
                
                if passed:
                    self.passed_tests += 1
                else:
                    self.failed_tests += 1
                
                self.results["sql_generation"].append({
                    "name": case['name'],
                    "sql_length": len(sql),
                    "passed": passed
                })
                
        except Exception as e:
            print(f"  âŒ SQL Generation Test Failed: {e}")
            import traceback
            traceback.print_exc()
            self.failed_tests += len(test_cases)
    
    def test_e2e_flow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹ (çœŸå®)"""
        print("\n" + "="*80)
        print("ğŸ”„ TEST 5: End-to-End Production Flow")
        print("="*80)
        
        import requests
        
        test_cases = [
            {"query": "æœ€è¿‘7å¤©çš„GMV", "expected_metric": "GMV"},
            {"query": "æœ¬æœˆæŒ‰æ¸ é“ç»Ÿè®¡DAU", "expected_metric": "DAU", "expected_dims": ["æ¸ é“"]},
            {"query": "ç”µå•†è®¢å•é‡", "expected_metric": "è®¢å•é‡"},
        ]
        
        for i, case in enumerate(test_cases, 1):
            self.total_tests += 1
            print(f"\n  Test 5.{i}: Query='{case['query']}'")
            
            try:
                response = requests.post(
                    "http://localhost:8000/api/v3/query",
                    json={"query": case['query']},
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # Check metric
                    metric_match = case['expected_metric'] in data['intent']['core_query']
                    print(f"    Metric: {data['intent']['core_query']} {'âœ…' if metric_match else 'âŒ'}")
                    
                    # Check dimensions
                    dims_match = True
                    if "expected_dims" in case:
                        dims_match = set(data['intent']['dimensions']) == set(case['expected_dims'])
                        print(f"    Dimensions: {data['intent']['dimensions']} {'âœ…' if dims_match else 'âŒ'}")
                    
                    # Check SQL generated
                    sql_generated = data.get('sql') and data['sql'] != "-- SQL generation failed"
                    print(f"    SQL Generated: {'âœ…' if sql_generated else 'âŒ'}")
                    
                    # Check data returned
                    data_returned = len(data.get('data', [])) > 0
                    print(f"    Data Returned: {len(data.get('data', []))} records {'âœ…' if data_returned else 'âŒ'}")
                    
                    passed = metric_match and dims_match and sql_generated and data_returned
                    
                    print(f"    Status: {'âœ… PASS' if passed else 'âŒ FAIL'}")
                    
                    if passed:
                        self.passed_tests += 1
                    else:
                        self.failed_tests += 1
                    
                    self.results["e2e_flow"].append({
                        "query": case['query'],
                        "metric": data['intent']['core_query'],
                        "sql_generated": sql_generated,
                        "passed": passed
                    })
                else:
                    print(f"    Status: âŒ FAIL (HTTP {response.status_code})")
                    self.failed_tests += 1
                    
            except Exception as e:
                print(f"    Status: âŒ FAIL ({e})")
                self.failed_tests += 1
    
    def run_all_tests(self, iterations=2):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯• (æŒ‡å®šæ¬¡æ•°)"""
        print("\n" + "ğŸš€"*40)
        print(f"ç”Ÿäº§çº§æµ‹è¯•å¥—ä»¶ - Running {iterations} iterations")
        print("ğŸš€"*40)
        
        for iteration in range(1, iterations + 1):
            print(f"\n{'#'*80}")
            print(f"# ITERATION {iteration}/{iterations}")
            print(f"{'#'*80}")
            
            self.test_vector_search()
            self.test_graph_search()
            self.test_llm_intent()
            self.test_sql_generation()
            self.test_e2e_flow()
            
            if iteration < iterations:
                print(f"\nâ³ Waiting 3 seconds before next iteration...")
                time.sleep(3)
        
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ“Š TEST SUMMARY")
        print("="*80)
        
        print(f"\nTotal Tests: {self.total_tests}")
        print(f"âœ… Passed: {self.passed_tests}")
        print(f"âŒ Failed: {self.failed_tests}")
        print(f"Success Rate: {(self.passed_tests/self.total_tests*100):.1f}%")
        
        print("\n" + "="*80)
        
        if self.failed_tests == 0:
            print("ğŸ‰ ALL TESTS PASSED - PRODUCTION READY!")
        else:
            print("âš ï¸  SOME TESTS FAILED - REVIEW REQUIRED")
        
        print("="*80 + "\n")
        
        # Save results
        with open('test_results.json', 'w', encoding='utf-8') as f:
            json.dump({
                "summary": {
                    "total": self.total_tests,
                    "passed": self.passed_tests,
                    "failed": self.failed_tests,
                    "success_rate": self.passed_tests/self.total_tests*100
                },
                "details": self.results
            }, f, ensure_ascii=False, indent=2)
        
        print("ğŸ“„ Detailed results saved to: test_results.json\n")

if __name__ == "__main__":
    suite = ProductionTestSuite()
    suite.run_all_tests(iterations=2)
