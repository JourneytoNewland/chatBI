"""æ¼”ç¤ºæœåŠ¡å™¨ - ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•æ„å›¾è¯†åˆ«å’Œå‰ç«¯."""

from datetime import datetime, timedelta
import time
import uuid
import random
from typing import Any, Optional, List, Dict
import sys
import os

# Ensure project root is in python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# ä¿æŒåŸæœ‰çš„ç®€å• IntentRecognizer å¼•ç”¨ï¼Œåç»­å¯èƒ½ä¼šç”¨å®ƒä½œä¸º fallback æˆ–åŸºç¡€
# from src.inference.intent import IntentRecognizer 
# ä½†ä¸ºäº† Demo æ•ˆæœï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªæ›´å¼ºå¤§çš„ DemoHybridIntentRecognizer

from src.config.metric_loader import metric_loader
from src.inference.intent import QueryIntent, TimeGranularity, AggregationType
from src.recall.vector.qdrant_store import QdrantVectorStore
from src.recall.vector.vectorizer import MetricVectorizer
from src.recall.graph.graph_store import GraphStore
from src.inference.zhipu_intent import ZhipuIntentRecognizer
from src.mql.sql_generator_v2 import SQLGeneratorV2
from src.inference.intent import QueryIntent, TimeGranularity, AggregationType
from src.mql.intelligent_interpreter import IntelligentInterpreter
# from src.mql.mql_engine import MQLEngine # Removed to avoid error if not used or wrong name

# æ˜¯å¦å¯ç”¨çœŸå® LLM(å¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶)
import os
USE_REAL_LLM = os.getenv("USE_REAL_LLM", "true").lower() == "true"

# åŠ è½½æŒ‡æ ‡æ•°æ®
MOCK_METRICS = metric_loader.get_all_metrics()

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="æ™ºèƒ½é—®æ•°ç³»ç»Ÿ - æ¼”ç¤ºæ¨¡å¼",
    description="ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•æ„å›¾è¯†åˆ«å’Œå‰ç«¯ç•Œé¢",
    version="1.0.0-demo",
)

# æ·»åŠ  CORS æ”¯æŒ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """å…¨å±€å¼‚å¸¸å¤„ç†."""
    import traceback
    error_msg = str(exc)
    error_trace = traceback.format_exc()
    print(f"âŒ Unhandled Exception: {error_msg}")
    print(error_trace)
    
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "code": 500,
            "message": "Internal Server Error",
            "error": error_msg,
        }
    )

# --- æ•°æ®æ¨¡å‹å®šä¹‰ (åŒ¹é… frontend/index.html) ---

class LayerInfo(BaseModel):
    """å±‚çº§æ‰§è¡Œä¿¡æ¯."""
    layer_name: str
    confidence: float
    duration: float
    status: str = "success"
    success: bool = True
    metadata: Dict[str, Any] = Field(default_factory=dict)

class Interpretation(BaseModel):
    """æ™ºèƒ½è§£è¯»."""
    summary: str
    trend: str
    key_findings: List[str]
    error: Optional[str] = None

class IntentResult(BaseModel):
    """æ„å›¾è¯†åˆ«ç»“æœ."""
    core_query: str
    source_layer: str
    confidence: float
    time_range: Optional[List[str]] = None # [start, end]
    time_granularity: Optional[str] = None
    aggregation_type: Optional[str] = None
    dimensions: List[str] = Field(default_factory=list)
    filters: Dict[str, Any] = Field(default_factory=dict)
    comparison_type: Optional[str] = None

class RootCauseAnalysis(BaseModel):
    """æ ¹å› åˆ†æç»“æœ."""
    report: str
    anomalies: List[Dict[str, Any]]
    trends: Dict[str, Any]
    dimensions: List[Dict[str, Any]]

class QueryRequestV3(BaseModel):
    """V3 æŸ¥è¯¢è¯·æ±‚."""
    query: str
    conversation_id: Optional[str] = None

class QueryResponseV3(BaseModel):
    """V3 æŸ¥è¯¢å“åº”."""
    conversation_id: str
    query: str
    intent: IntentResult
    data: List[Dict[str, Any]]
    execution_time_ms: float
    all_layers: List[LayerInfo]
    mql: str
    sql: str
    interpretation: Interpretation
    root_cause_analysis: Optional[RootCauseAnalysis] = None


# --- Demo æ ¸å¿ƒé€»è¾‘ ---

class DemoHybridIntentRecognizer:
    """æ¼”ç¤ºç”¨æ··åˆæ„å›¾è¯†åˆ«å™¨."""
    
    def __init__(self, metrics: List[Dict[str, Any]]):
        self.metrics = metrics
        # å»ºç«‹æ›´ä¸°å¯Œçš„ç´¢å¼•ï¼šåç§°ã€åŒä¹‰è¯ -> æŒ‡æ ‡ä¿¡æ¯
        self.index = {}
        for m in self.metrics:
            self.index[m['name'].lower()] = m
            for syn in m.get('synonyms', []):
                self.index[syn.lower()] = m
        
        # æ‰©å±•çš„è¯­ä¹‰æ˜ å°„ (ä¿ç•™ä½œä¸ºé«˜ç½®ä¿¡åº¦è§„åˆ™)
        self.semantic_map = {
            "ç”¨æˆ·æ´»è·ƒåº¦": "DAU",
            "æ´»è·ƒç”¨æˆ·": "DAU",
            "user activity": "DAU",
            "active users": "DAU",
            "é”€å”®é¢": "GMV",
            "æˆäº¤é¢": "GMV",
            "sales": "GMV",
            "gmv": "GMV",
            "è¥æ”¶": "Revenue",
            "revenue": "Revenue"
        }
        
        # åˆå§‹åŒ–å‘é‡å’Œå›¾è°±ç»„ä»¶
        try:
            self.vector_store = QdrantVectorStore()
            self.vectorizer = MetricVectorizer()
            self.graph_store = GraphStore()
            print("ğŸš€ [DemoHybridIntentRecognizer] Vector Store, Vectorizer & Graph Store Initialized")
        except Exception as e:
            print(f"âš ï¸ [DemoHybridIntentRecognizer] Failed to initialize stores: {e}")
            self.vector_store = None
            self.vectorizer = None
            self.graph_store = None
        
        # åˆå§‹åŒ– LLM è¯†åˆ«å™¨
        if USE_REAL_LLM:
            try:
                self.llm_recognizer = ZhipuIntentRecognizer(model="glm-4-flash")
                print("ğŸ§  [DemoHybridIntentRecognizer] ZhipuAI LLM Recognizer Initialized")
            except Exception as e:
                print(f"âš ï¸ [DemoHybridIntentRecognizer] Failed to initialize LLM: {e}")
                self.llm_recognizer = None
        else:
            self.llm_recognizer = None
            print("ğŸ”‡ [DemoHybridIntentRecognizer] Real LLM disabled (USE_REAL_LLM=false)")

    def recognize(self, query: str) -> dict:
        """è¯†åˆ«æ„å›¾ï¼Œè¿”å›è¯¦ç»†çš„å±‚çº§ä¿¡æ¯."""
        start_time = time.time()
        layers = []
        best_metric = None
        confidence = 0.0
        
        # 1. L1 ç²¾ç¡®åŒ¹é…å±‚ (Exact + Synonym Matching - PRODUCTION with Scoring)
        l1_start = time.time()
        exact_match = None
        query_lower = query.lower()
        matched_by = "unknown"
        best_score = 0  # ç”¨äºé€‰æ‹©æœ€ä½³åŒ¹é…
        
        # 1.1 ç²¾ç¡®åç§°/ç¼–ç åŒ¹é… (æœ€é«˜ä¼˜å…ˆçº§,å¾—åˆ†100)
        for metric in self.metrics:
            if metric['name'].lower() == query_lower or metric['code'].lower() == query_lower:
                exact_match = metric
                matched_by = "exact_name"
                best_score = 100
                print(f"   âœ… L1 Exact Match: {metric['name']}")
                break
        
        # 1.2 åŒä¹‰è¯ç²¾ç¡®åŒ¹é… (æ¬¡é«˜ä¼˜å…ˆçº§,å¾—åˆ†90)
        if best_score < 90:
            for metric in self.metrics:
                synonyms = metric.get('synonyms', [])
                for syn in synonyms:
                    if syn.lower() == query_lower:
                        exact_match = metric
                        matched_by = f"synonym_exact:{syn}"
                        best_score = 90
                        print(f"   âœ… L1 Synonym Exact Match: {metric['name']} (via '{syn}')")
                        break
                if best_score >= 90:
                    break
        
        # 1.3 æŸ¥è¯¢è¯å®Œæ•´åŒ…å«æŒ‡æ ‡å (å¾—åˆ†80)
        if best_score < 80:
            for metric in self.metrics:
                metric_name_lower = metric['name'].lower()
                # æŸ¥è¯¢è¯åŒ…å«å®Œæ•´æŒ‡æ ‡å(ä½œä¸ºç‹¬ç«‹è¯)
                if metric_name_lower in query_lower:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç‹¬ç«‹è¯
                    # å¯¹äºè‹±æ–‡æŒ‡æ ‡(å¦‚DAU, GMV),åªéœ€æ£€æŸ¥å‰åä¸æ˜¯ASCIIå­—æ¯æ•°å­—
                    idx = query_lower.find(metric_name_lower)
                    
                    # æ£€æŸ¥å‰ä¸€ä¸ªå­—ç¬¦
                    char_before_ok = (idx == 0 or not query_lower[idx-1].isascii() or not query_lower[idx-1].isalnum())
                    # æ£€æŸ¥åä¸€ä¸ªå­—ç¬¦
                    char_after_ok = (idx + len(metric_name_lower) == len(query_lower) or \
                                    not query_lower[idx + len(metric_name_lower)].isascii() or \
                                    not query_lower[idx + len(metric_name_lower)].isalnum())
                    
                    if char_before_ok and char_after_ok:
                        exact_match = metric
                        matched_by = f"query_contains_metric:{metric_name_lower}"
                        best_score = 80
                        print(f"   âœ… L1 Query Contains Metric: {metric['name']}")
                        break

        
        # 1.4 åŒä¹‰è¯éƒ¨åˆ†åŒ¹é… (å¾—åˆ†60-70,æŒ‰åŒ¹é…é•¿åº¦)
        if best_score < 70:
            for metric in self.metrics:
                synonyms = metric.get('synonyms', [])
                for syn in synonyms:
                    syn_lower = syn.lower()
                    # åŒä¹‰è¯åŒ…å«åœ¨æŸ¥è¯¢ä¸­ æˆ– æŸ¥è¯¢åŒ…å«åŒä¹‰è¯
                    if syn_lower in query_lower:
                        score = 60 + min(10, len(syn_lower))  # è¶Šé•¿çš„åŒä¹‰è¯å¾—åˆ†è¶Šé«˜
                        if score > best_score:
                            exact_match = metric
                            matched_by = f"synonym_partial:{syn}"
                            best_score = score
                            print(f"   âœ… L1 Synonym Partial Match: {metric['name']} (via '{syn}', score={score})")
        
        l1_duration = (time.time() - l1_start) * 1000
        
        if exact_match and best_score >= 60:  # è‡³å°‘60åˆ†æ‰ç®—åŒ¹é…æˆåŠŸ
            layers.append(LayerInfo(
                layer_name="L1 ç²¾ç¡®åŒ¹é…",
                confidence=min(1.0, best_score / 100.0),
                duration=l1_duration,
                success=True,
                metadata={"match_type": matched_by, "metric": exact_match['name'], "score": best_score}
            ))
            best_metric = exact_match
            confidence = min(1.0, best_score / 100.0)
        else:
            layers.append(LayerInfo(
                layer_name="L1 ç²¾ç¡®åŒ¹é…",
                confidence=0.0,
                duration=l1_duration,
                success=False,
                metadata={"match_type": "none", "best_score": best_score}
            ))
        
        # 2. L2 å‘é‡/å›¾è°±å¬å›å±‚ (ä»…åœ¨L1æœªåŒ¹é…æ—¶æ‰§è¡Œ)
        l2_start = time.time()
        
        # 2.1 å‘é‡æ£€ç´¢ (Real Vector Search - ä»…åœ¨L1å¤±è´¥æ—¶)

        if not best_metric and self.vector_store:
            try:
                # å‘é‡åŒ–æŸ¥è¯¢
                query_vec = self.vectorizer.model.encode(query, normalize_embeddings=True)
                # æ£€ç´¢ Top-1
                results = self.vector_store.search(query_vec, top_k=1, score_threshold=0.15)
                
                if results:
                    top_result = results[0]
                    payload = top_result['payload']
                    target_metric = self._find_metric_by_name(payload['name'])
                    
                    if target_metric:
                        best_metric = target_metric
                        matched_by = "vector_search"
                        # å½’ä¸€åŒ–åˆ†æ•° (Qdrant Cosine is -1 to 1, usually 0-1 for text)
                        confidence = float(top_result['score'])
                        print(f"   vector search found: {target_metric['name']} with score: {confidence}")
                        # æå‡ä¸€ç‚¹ä¿¡å¿ƒ
                        if confidence > 0.15: 
                            confidence = min(0.9, confidence + 0.4) 
            except Exception as e:
                print(f"âš ï¸ Vector search error: {e}")

        # å¹¶è¡Œå°è¯•å›¾è°±å¬å› (Domain Search)
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå¦‚æœ query åŒ…å« domain å…³é”®è¯ (e.g. "ç”µå•†", "ç”¨æˆ·")ï¼Œ
        # åˆ™ä»å›¾è°±æ‰¾å›ç›¸å…³æŒ‡æ ‡ã€‚å¦‚æœ vector æ²¡æ‰¾åˆ°ï¼Œæˆ–è€…åˆ†æ•°ä½ï¼Œå¯ä»¥åˆ©ç”¨ graph ç»“æœå¢å¼ºã€‚
        # ç®€å• Demo: å¦‚æœåŒ¹é…åˆ° Domainï¼Œåˆ™çœ‹çœ‹ Domain ä¸‹æ˜¯å¦æœ‰æŒ‡æ ‡åŒ¹é… query çš„éƒ¨åˆ†ï¼Ÿ
        # æˆ–è€…ä»…ä»…ä½œä¸º candidates æä¾›ç»™ Debugã€‚
        graph_candidates = []
        if self.graph_store:
            try:
                # ç®€å•å…³é”®è¯æå– Domain
                target_domain = None
                if "ç”µå•†" in query: target_domain = "ç”µå•†"
                elif "ç”¨æˆ·" in query: target_domain = "ç”¨æˆ·"
                
                if target_domain:
                    # ä»å›¾è°±æŸ¥è¯¥ Domain ä¸‹çš„æ‰€æœ‰æŒ‡æ ‡
                    domain_metrics = self.graph_store.search_by_domain(target_domain)
                    for dm in domain_metrics:
                        graph_candidates.append(dm)
                    print(f"   graph search found {len(domain_metrics)} metrics in domain '{target_domain}'")
                    
                    # å¦‚æœè¿˜æ²¡æœ‰ best_metricï¼Œçœ‹çœ‹èƒ½å¦ä» graph ç»“æœé‡Œæ’ä¸Š?
                    if not best_metric and domain_metrics:
                        # ç®€å•çš„åŒ…å«åŒ¹é…
                        for dm in domain_metrics:
                            if dm['name'] in query:
                                best_metric = self._find_metric_by_name(dm['name'])
                                matched_by = "graph_domain_match"
                                confidence = 0.9
                                break
            except Exception as e:
                print(f"âš ï¸ Graph search error: {e}")

        # æœ€åå°è¯•æ¨¡ç³ŠåŒ¹é… (Fallback)
        if not best_metric:
            for metric in self.metrics:
                if metric['name'].lower() in query_lower:
                    best_metric = metric
                    matched_by = "fuzzy_match"
                    confidence = 0.85
                    break

        # é»˜è®¤ GMV (Failover)
        if not best_metric:
            best_metric = self._find_metric_by_name("GMV") if self._find_metric_by_name("GMV") else self.metrics[0]
            matched_by = "default"
            confidence = 0.6

        l2_duration = (time.time() - l2_start) * 1000
        
        # æ„é€  L2 å…ƒæ•°æ®
        candidates = []
        if best_metric:
            candidates.append({
                "rank": 1,
                "name": best_metric['name'],
                "source": matched_by,
                "final_score": confidence,
                "feature_scores": {
                    "VectorSimilarity": {"value": 0.9, "weight": 0.3, "score": 0.27},
                    "ExactMatch": {"value": 1.0 if matched_by == "keyword_match" else 0.0, "weight": 0.15, "score": 0.15},
                }
            })
            
        layers.append(LayerInfo(
            layer_name="L2 å‘é‡/å›¾è°±å¬å›",
            confidence=confidence,
            duration=l2_duration,
            success=True,
            metadata={
                "recall_type": "dual_recall",
                "candidates": candidates,
                "fusion_stats": {
                    "total_candidates": len(candidates) + len(graph_candidates),
                    "vector_avg_score": 0.8,
                    "graph_hit": len(graph_candidates) > 0
                },
                "graph_candidates": [c['name'] for c in graph_candidates[:5]] # Debug info
            }
        ))

        # 3. LLM å±‚ (L3) - è§£ææ—¶é—´èŒƒå›´/ç»´åº¦ (Real or Mock)
        l3_start = time.time()
        llm_result = None
        
        if self.llm_recognizer:
            try:
                # å°† candidates ä¼ é€’ç»™ LLM è®©å®ƒæ ¹æ®å®é™…å€™é€‰æŒ‡æ ‡è¿›è¡Œé€‰æ‹©
                candidate_list = [{'name': best_metric['name'], 'code': best_metric['code']}] if best_metric else []
                llm_result = self.llm_recognizer.recognize(query, candidates=candidate_list)
                
                if llm_result:
                    # ä½¿ç”¨ LLM è§£æçš„ç»´åº¦
                    dimensions = llm_result.dimensions if llm_result.dimensions else []
                    
                    # LLM è§£æçš„æ—¶é—´èŒƒå›´
                    if llm_result.time_range:
                        time_info = llm_result.time_range
                        now = datetime.now()
                        # ç®€åŒ–å¤„ç†: å‡è®¾ LLM è¿”å› "7d" æˆ– "this_month" ç­‰
                        time_value = time_info.get('value', '')
                        if time_value == '7d' or '7' in time_value:
                            start_date = now - timedelta(days=7)
                            end_date = now
                        elif 'this_month' in time_value or 'æœ¬æœˆ' in time_info.get('description', ''):
                            start_date = now.replace(day=1)
                            end_date = now
                        else:
                            start_date = now - timedelta(days=7)
                            end_date = now
                    else:
                        start_date = datetime.now() - timedelta(days=7)
                        end_date = datetime.now()
                    
                    print(f"   LLM parsed: dimensions={dimensions}, time_range={llm_result.time_range}")
                else:
                    # LLM è¿”å› Noneï¼Œå›é€€åˆ° Mock
                    now = datetime.now()
                    start_date = now - timedelta(days=7)
                    end_date = now
                    dimensions = []
                    if "åœ°åŒº" in query: dimensions.append("åœ°åŒº")
                    if "æ¸ é“" in query: dimensions.append("æ¸ é“")
            except Exception as e:
                print(f"âš ï¸ LLM recognition error: {e}")
                now = datetime.now()
                start_date = now - timedelta(days=7)
                end_date = now
                dimensions = []
                if "åœ°åŒº" in query: dimensions.append("åœ°åŒº")
                if "æ¸ é“" in query: dimensions.append("æ¸ é“")
        else:
            # Mock time range logic (LLM disabled)
            now = datetime.now()
            start_date = now - timedelta(days=7)
            end_date = now
            dimensions = []
            if "åœ°åŒº" in query:
                dimensions.append("åœ°åŒº")
            if "æ¸ é“" in query:
                dimensions.append("æ¸ é“")
        
        l3_duration = (time.time() - l3_start) * 1000
        layers.append(LayerInfo(
            layer_name="L3 LLMå¢å¼º",
            confidence=llm_result.confidence if llm_result else 0.95,
            duration=l3_duration,
            success=True,
            metadata={
                "llm_model": "glm-4-flash" if self.llm_recognizer else "mock",
                "tokens": llm_result.tokens_used.get('total_tokens', 0) if llm_result else 0,
                "real_llm": self.llm_recognizer is not None
            }
        ))

        return {
            "metric": best_metric,
            "layers": layers,
            "dimensions": dimensions,
            "time_range": (start_date, end_date),
            "confidence": confidence
        }

    def _find_metric_by_name(self, name: str):
        for m in self.metrics:
            if m['name'].upper() == name.upper():
                return m
            if m['code'].upper() == name.upper():
                return m
        return None

demo_recognizer = DemoHybridIntentRecognizer(MOCK_METRICS)
intelligent_interpreter = IntelligentInterpreter()


def _generate_intelligent_interpretation(query: str, metric: Dict, data: List[Dict], sql: str, start_time: float) -> Interpretation:
    """ç”Ÿæˆæ™ºèƒ½è§£è¯»(ä½¿ç”¨LLM)."""
    try:
        # è§„èŒƒåŒ–æ•°æ®å­—æ®µå(intelligent_interpreteræœŸæœ›"value"å­—æ®µ)
        normalized_data = []
        for row in data:
            normalized_row = row.copy()
            if "metric_value" in normalized_row and "value" not in normalized_row:
                normalized_row["value"] = normalized_row["metric_value"]
            normalized_data.append(normalized_row)
        
        # æ„å»ºmql_resultä¾›interpretæ–¹æ³•ä½¿ç”¨
        mql_result_for_interpret = {
            "result": normalized_data,
            "row_count": len(normalized_data),
            "sql": sql,
            "execution_time_ms": (time.time() - start_time) * 1000
        }
        
        # è·å–metric_def
        metric_def = {
            "name": metric['name'],
            "code": metric['code'],
            "unit": metric.get('unit', 'æœªçŸ¥'),
            "description": metric.get('description', '')
        }
        
        # è°ƒç”¨æ™ºèƒ½è§£è¯»å™¨
        interpretation_result = intelligent_interpreter.interpret(
            query=query,
            mql_result=mql_result_for_interpret,
            metric_def=metric_def
        )
        
        return Interpretation(
            summary=interpretation_result.summary,
            trend=interpretation_result.trend,
            key_findings=interpretation_result.key_findings,
            error=None
        )
    except Exception as e:
        import traceback
        print(f"âŒ Intelligent Interpretation failed: {str(e)}")
        traceback.print_exc()
        
        # é™çº§åˆ°é»˜è®¤æ¨¡æ¿
        return Interpretation(
            summary=f"æŸ¥è¯¢ {metric['name']} çš„æ•°æ®",
            trend="stable",
            key_findings=[f"å…± {len(data)} æ¡è®°å½•"],
            error=str(e)
        )


@app.post("/api/v3/query", response_model=QueryResponseV3)
async def query_v3(request: QueryRequestV3):
    """å…¨åŠŸèƒ½æŸ¥è¯¢æ¥å£ (æ¨¡æ‹Ÿ)."""
    start_time = time.time()
    
    # 1. æ„å›¾è¯†åˆ«
    recognition_result = demo_recognizer.recognize(request.query)
    metric = recognition_result['metric']
    start_date, end_date = recognition_result['time_range']
    dimensions = recognition_result['dimensions']
    
    # 2. ç”Ÿæˆ SQL (Real SQL Generation)
    generated_sql = None
    sql_params = {}
    try:
        # æ„é€  QueryIntent å¯¹è±¡
        query_intent = QueryIntent(
            query=request.query,
            core_query=metric['name'],
            time_range=(start_date, end_date),
            time_granularity=TimeGranularity.DAY,
            aggregation_type=AggregationType.SUM,
            dimensions=dimensions,
            comparison_type=None,
            filters={}
        )
        
        # ç”Ÿæˆ SQL
        sql_generator = SQLGeneratorV2()
        generated_sql, sql_params = sql_generator.generate(query_intent)
        print(f"   âœ… Generated SQL ({len(generated_sql)} chars)")
    except Exception as e:
        print(f"   âš ï¸ SQL generation error: {e}")
        generated_sql = None
    
    # 3. æ„é€ æ„å›¾ç»“æœ
    intent_result = IntentResult(
        core_query=metric['name'],
        source_layer="L2 å‘é‡/å›¾è°±å¬å›" if recognition_result['confidence'] > 0.8 else "L3 LLMå¢å¼º",
        confidence=recognition_result['confidence'],
        time_range=[start_date.strftime("%Y-%m-%d"), end_date.strftime("%Y-%m-%d")],
        time_granularity="day",
        aggregation_type="SUM",
        dimensions=dimensions
    )
    
    # 4. ç”Ÿæˆ Mock æ•°æ® (TODO: æ›¿æ¢ä¸ºçœŸå®æ•°æ®åº“æŸ¥è¯¢)
    data = []
    current = start_date
    while current <= end_date:
        date_str = current.strftime("%Y-%m-%d")
        base_value = random.randint(1000, 5000)
        
        if dimensions:
            for dim in ["åä¸œ", "åå—", "ååŒ—"]: # Mock ç»´åº¦å€¼
                data.append({
                    "date": date_str,
                    dimensions[0]: dim,
                    "metric_value": base_value * random.uniform(0.8, 1.2),
                    "metric": metric['name']
                })
        else:
            data.append({
                "date": date_str,
                "metric_value": base_value * random.uniform(0.8, 1.2),
                "metric": metric['name']
            })
        current += timedelta(days=1)

    # 4. ç”Ÿæˆ Interpretation
    interpretation = Interpretation(
        summary=f"{metric['name']} åœ¨è¿‡å»7å¤©è¡¨ç°å¹³ç¨³ã€‚",
        trend="stable",
        key_findings=[
            f"{metric['name']} å‡å€¼ä¸º {sum(d['metric_value'] for d in data)/len(data):.2f}",
            "æœªå‘ç°æ˜æ˜¾å¼‚å¸¸æ³¢åŠ¨"
        ]
    )
    
    # 5. ç”Ÿæˆ MQL/SQL (Mock)
    mql_str = f"SELECT {metric['name']} BY {','.join(dimensions) if dimensions else 'overall'} FROM {start_date.strftime('%Y-%m-%d')} TO {end_date.strftime('%Y-%m-%d')}"
    sql_str = f"SELECT dd.date, {', '.join([d+'.name' for d in dimensions] + ['']) if dimensions else ''} SUM(f.{metric['column']}) \nFROM {metric.get('table', 'fact_table')} f \nJOIN dim_date dd ON f.date_key = dd.date_key \nWHERE dd.date BETWEEN '{start_date.strftime('%Y-%m-%d')}' AND '{end_date.strftime('%Y-%m-%d')}' \nGROUP BY dd.date {', ' + ','.join([d+'.name' for d in dimensions]) if dimensions else ''}"

    # 6. æ ¹å› åˆ†æ (å¦‚æœæŸ¥è¯¢åŒ…å«å…³é”®è¯)
    rca = None
    if any(k in request.query for k in ["ä¸ºä»€ä¹ˆ", "åˆ†æ", "åŸå› "]):
        rca = RootCauseAnalysis(
            report=f"{metric['name']} çš„å˜åŒ–ä¸»è¦å—å­£èŠ‚æ€§å› ç´ å½±å“ã€‚",
            anomalies=[
                {"timestamp": start_date.strftime("%Y-%m-%d"), "value": 1200, "expected": 1500, "severity": "medium", "type": "dip", "deviation_pct": -20.0}
            ],
            trends={"trend_type": "stable", "trend_strength": 0.8, "slope": 0.1, "r_squared": 0.95},
            dimensions=[
                {"dimension_name": "åœ°åŒº", "analysis": "åä¸œåœ°åŒºè´¡çŒ®æœ€å¤§", "top_contributors": [{"name": "åä¸œ", "contribution_pct": 45}]}
            ]
        )

    execution_time = (time.time() - start_time) * 1000

    # 5. è¿”å›å“åº” (åŒ…å«ç”Ÿæˆçš„ SQL)
    return QueryResponseV3(
        conversation_id=request.conversation_id or str(uuid.uuid4()),
        query=request.query,
        intent=intent_result,
        data=data,
        execution_time_ms=int((time.time() - start_time) * 1000),
        all_layers=recognition_result['layers'],
        mql=f"Query(metric='{metric['name']}', dimensions={dimensions})",
        sql=generated_sql if generated_sql else "-- SQL generation failed",
        interpretation=_generate_intelligent_interpretation(request.query, metric, data, generated_sql if generated_sql else sql_str, start_time),
        root_cause_analysis=None
    )


# ä¿æŒ /api/v1/search ä»¥å…¼å®¹æ—§è„šæœ¬ (Optional)
# ... code omitted for brevity but keeping it simple ...
# ä¸ºäº†é¿å…å†²çªï¼Œæˆ‘ä»¬ä¸å†å®šä¹‰æ—§çš„ search_request/response class, 
# ä½†å¦‚æœæ—§è„šæœ¬æ¨¡æ‹Ÿçš„æ˜¯ vector searchï¼Œæˆ‘ä»¬å¯ä»¥ä¿ç•™ä¸€ä¸ªç®€åŒ–ç‰ˆ

class SearchRequestV1(BaseModel):
    query: str
    top_k: int = 10

class SearchResponseV1(BaseModel):
    query: str
    candidates: List[Dict[str, Any]]

@app.post("/api/v1/search", response_model=SearchResponseV1)
async def search_v1(request: SearchRequestV1):
    """å…¼å®¹æ—§ç‰ˆæ£€ç´¢æ¥å£."""
    # å¤ç”¨ DemoHybridIntentRecognizer çš„ L2 é€»è¾‘
    recog = demo_recognizer.recognize(request.query)
    metric = recog['metric']
    
    return SearchResponseV1(
        query=request.query,
        candidates=[{
            "id": metric['id'],
            "metric_id": metric['id'], # Compat
            "name": metric['name'],
            "code": metric['code'],
            "description": metric['description'],
            "domain": metric['domain'],
            "score": recog['confidence'],
            "synonyms": metric.get('synonyms', []),
            "formula": metric.get('formula')
        }]
    )

@app.get("/")
async def root():
    return {
        "message": "æ™ºèƒ½é—®æ•°ç³»ç»Ÿ - æ¼”ç¤ºæ¨¡å¼ (V3 API Enabled)",
        "version": "1.0.0-demo-v3",
        "docs": "/docs",
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "mode": "demo"}

if __name__ == "__main__":
    import uvicorn
    print("""
    ğŸš€ æ™ºèƒ½é—®æ•°ç³»ç»Ÿ - æ¼”ç¤ºæ¨¡å¼ (V3 API Enabled)
    =====================================
    æœåŠ¡åœ°å€: http://localhost:8000
    API æ–‡æ¡£: http://localhost:8000/docs
    å‰ç«¯ç•Œé¢: åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ frontend/index.html
    =====================================
    """)
    uvicorn.run("scripts.run_demo_server:app", host="0.0.0.0", port=8000, reload=True)
